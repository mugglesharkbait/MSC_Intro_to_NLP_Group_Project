{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmdXVBAWGpsM"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeY28k58DG8T",
        "outputId": "6b693515-340c-40c1-d26b-a879e9d16beb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/drive/MyDrive/MSC_Intro_to_NLP_Group_Project/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-kDQGv4DMWY",
        "outputId": "0dcabac8-5b12-440d-8111-a0b0a9661361"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/MSC_Intro_to_NLP_Group_Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jf8V8EEnGpsO",
        "outputId": "b9bb50a3-24d5-4add-d9de-23f7308a3ea4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.20.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu121)\n",
            "Requirement already satisfied: accelerate>=0.20.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.26.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.16.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "!pip install gensim\n",
        "!pip install 'transformers[torch]'\n",
        "!pip install datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dsTI9k5EGpsP"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "za2IufRcGpsQ"
      },
      "outputs": [],
      "source": [
        "# Load the preprocessed data from the JSON file\n",
        "data_files={\n",
        "    \"train\":\"data_train_rule_based_preprocess.json\",\n",
        "}\n",
        "\n",
        "dataset = load_dataset(\"json\", data_files=data_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZVuX7tQGpsQ",
        "outputId": "a411c81f-aa47-45a4-f982-3dc450e1ea95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['transformation', 'topic', 'id'],\n",
            "        num_rows: 104562\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VHVZE9FqGpsQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57cf8ca2-d1c3-455e-c388-f36f83446f75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UmUg7stiGpsQ"
      },
      "outputs": [],
      "source": [
        "# Tokenize the informal sentences\n",
        "def preprocess_function(examples, input_field=\"informal\", target_field=\"formal.ref0\"):\n",
        "    inputs = [ex[input_field] for ex in examples[\"transformation\"]]\n",
        "    targets = [ex[target_field] for ex in examples[\"transformation\"]]\n",
        "\n",
        "    new_examples = tokenizer(\n",
        "        inputs, text_target=targets, max_length=64, truncation=True, padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "    return new_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uvuG0IrXGpsR"
      },
      "outputs": [],
      "source": [
        "def create_multi_ref_dataset(dataset):\n",
        "  for i, target_field in enumerate(['formal.ref0', 'formal.ref1', 'formal.ref2', 'formal.ref3']):\n",
        "    new_dataset = preprocess_function(dataset, 'informal', target_field)\n",
        "    dataset = dataset.add_column(f'labels_{i}', new_dataset['labels'])\n",
        "    if i == 0:\n",
        "      dataset = dataset.add_column('input_ids', new_dataset['input_ids'])\n",
        "      dataset = dataset.add_column('token_type_ids', new_dataset['token_type_ids'])\n",
        "      dataset = dataset.add_column('attention_mask', new_dataset['attention_mask'])\n",
        "\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Ty2c8kBNGpsR"
      },
      "outputs": [],
      "source": [
        "train_dataset = dataset['train'].map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvaqW1B5GpsR",
        "outputId": "b4421225-ae38-4e2c-a238-f1c410517e36"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'Sure',\n",
              " ',',\n",
              " 'it',\n",
              " \"'\",\n",
              " 's',\n",
              " 'ok',\n",
              " ',',\n",
              " 'but',\n",
              " 'I',\n",
              " 'always',\n",
              " 'have',\n",
              " 'let',\n",
              " 'the',\n",
              " 'guy',\n",
              " 'ask',\n",
              " 'me',\n",
              " '.',\n",
              " '[SEP]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "tokenizer.convert_ids_to_tokens(train_dataset['input_ids'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xB25faOAGpsR"
      },
      "source": [
        "# RNN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaP22qU2GpsR"
      },
      "source": [
        "# RNN Try 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "sPAeeNqbGpsR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.rnn = nn.RNN(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        output, _ = self.rnn(x)\n",
        "        output = self.fc(output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozIs738NGpsS"
      },
      "source": [
        "RNN try 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # splitting the train dataset to use only 10% of it\n",
        "# train_dataset = train_dataset.train_test_split(test_size=0.9, shuffle=True, seed=42)\n",
        "# train_dataset = train_dataset['train']"
      ],
      "metadata": {
        "id": "Kudj3691Z7Gb"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(train_dataset)"
      ],
      "metadata": {
        "id": "u_sSfRsYZ84N"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "q2_8RWpJGpsS"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "\n",
        "# class Seq2SeqRNN(nn.Module):\n",
        "#     def __init__(self, input_size, embedding_size, hidden_size, output_size):\n",
        "#         super(Seq2SeqRNN, self).__init__()\n",
        "#         self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "#         self.rnn = nn.RNN(embedding_size, hidden_size)\n",
        "#         self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "#     def forward(self, input_seq, hidden=None):\n",
        "#         embedded = self.embedding(input_seq)\n",
        "#         output, hidden = self.rnn(embedded, hidden)\n",
        "#         output = self.fc(output)\n",
        "#         return output, hidden\n",
        "\n",
        "# # Define hyperparameters\n",
        "# input_size = len(tokenizer.get_vocab())\n",
        "# embedding_size = 256\n",
        "# hidden_size = 512\n",
        "# output_size = len(tokenizer.get_vocab())\n",
        "\n",
        "# # Instantiate the model\n",
        "# model = Seq2SeqRNN(input_size, embedding_size, hidden_size, output_size)\n",
        "\n",
        "# # Define loss function and optimizer\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# # Training loop\n",
        "# num_epochs = 1\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# model.to(device)\n",
        "\n",
        "# for epoch in range(num_epochs):\n",
        "#     for batch in train_dataset:\n",
        "#         inputs = torch.tensor(batch[\"input_ids\"]).to(device)\n",
        "#         targets = torch.tensor(batch[\"labels\"]).to(device)\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         output, _ = model(inputs)\n",
        "\n",
        "#         # Reshape the output to be 2D (batch_size * sequence_length, vocab_size)\n",
        "#         output = output.view(-1, output_size)\n",
        "\n",
        "#         loss = criterion(output, targets.view(-1))\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         # Print the loss for monitoring training progress\n",
        "#         print(f\"Epoch: {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "# # Save the trained model\n",
        "# torch.save(model.state_dict(), \"seq2seq_rnn_model.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load the model\n",
        "# model = Seq2SeqRNN(input_size, embedding_size, hidden_size, output_size)\n",
        "# model.load_state_dict(torch.load(\"seq2seq_rnn_model.pth\"))\n",
        "# model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# # Define the input sequence\n",
        "# input_seq = torch.tensor(tokenizer.encode(\"Yo! Sidd, what's up!!!\")).unsqueeze(0)\n",
        "\n",
        "# # Predict the output\n",
        "# output, _ = model(input_seq)\n",
        "# output = torch.argmax(output, dim=2)  # Get the most probable next token\n",
        "\n",
        "# # Decode the output\n",
        "# decoded_output = tokenizer.decode(output[0])\n",
        "\n",
        "# print(decoded_output)"
      ],
      "metadata": {
        "id": "Xdzv3JhxXn6i"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN Try 2"
      ],
      "metadata": {
        "id": "kNVqWd6wlkUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, _ = self.lstm(embedded)\n",
        "        output = self.fc(output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "e9aLRTQ6XrKz"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# choosing GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Loading preprocessed data in train_dataset\n",
        "input_ids = torch.tensor(train_dataset['input_ids']).to(device)\n",
        "labels_0 = torch.tensor(train_dataset['labels']).to(device)\n",
        "\n",
        "# Define your model\n",
        "input_size = len(tokenizer.get_vocab())\n",
        "print(\"||\" * 10)\n",
        "print(input_size)\n",
        "print(\"||\" * 10)\n",
        "hidden_size = 512\n",
        "output_size = len(tokenizer.get_vocab())\n",
        "# output_size = 512\n",
        "\n",
        "model = RNNModel(input_size, hidden_size, output_size)\n",
        "model.to(device)\n",
        "\n",
        "# Define your loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 5\n",
        "batch_size = 32\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i in range(0, len(input_ids), batch_size):\n",
        "        inputs = input_ids[i:i+batch_size]\n",
        "        targets = labels_0[i:i+batch_size]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.view(-1, output_size), targets.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i // batch_size) % 10 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i // batch_size}/{len(input_ids) // batch_size}], Loss: {loss.item()}')\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), \"seq2seq_rnn_model.pth\")\n",
        "print('Training finished!')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lucLENGaon4",
        "outputId": "4e1c3e85-8631-460e-9eba-4789b0a0ae7b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "||||||||||||||||||||\n",
            "28996\n",
            "||||||||||||||||||||\n",
            "Epoch [1/5], Step [0/3267], Loss: 10.308734893798828\n",
            "Epoch [1/5], Step [10/3267], Loss: 2.4906623363494873\n",
            "Epoch [1/5], Step [20/3267], Loss: 1.713649034500122\n",
            "Epoch [1/5], Step [30/3267], Loss: 1.515468716621399\n",
            "Epoch [1/5], Step [40/3267], Loss: 1.5696322917938232\n",
            "Epoch [1/5], Step [50/3267], Loss: 1.4720643758773804\n",
            "Epoch [1/5], Step [60/3267], Loss: 1.6231441497802734\n",
            "Epoch [1/5], Step [70/3267], Loss: 1.5128427743911743\n",
            "Epoch [1/5], Step [80/3267], Loss: 1.5815153121948242\n",
            "Epoch [1/5], Step [90/3267], Loss: 1.6748604774475098\n",
            "Epoch [1/5], Step [100/3267], Loss: 1.724317193031311\n",
            "Epoch [1/5], Step [110/3267], Loss: 1.4342445135116577\n",
            "Epoch [1/5], Step [120/3267], Loss: 1.440632700920105\n",
            "Epoch [1/5], Step [130/3267], Loss: 1.6602866649627686\n",
            "Epoch [1/5], Step [140/3267], Loss: 1.3173856735229492\n",
            "Epoch [1/5], Step [150/3267], Loss: 1.5122061967849731\n",
            "Epoch [1/5], Step [160/3267], Loss: 1.6246155500411987\n",
            "Epoch [1/5], Step [170/3267], Loss: 1.3762065172195435\n",
            "Epoch [1/5], Step [180/3267], Loss: 1.3120176792144775\n",
            "Epoch [1/5], Step [190/3267], Loss: 1.2497977018356323\n",
            "Epoch [1/5], Step [200/3267], Loss: 1.3162273168563843\n",
            "Epoch [1/5], Step [210/3267], Loss: 1.316988468170166\n",
            "Epoch [1/5], Step [220/3267], Loss: 1.4228652715682983\n",
            "Epoch [1/5], Step [230/3267], Loss: 1.318847417831421\n",
            "Epoch [1/5], Step [240/3267], Loss: 1.3326069116592407\n",
            "Epoch [1/5], Step [250/3267], Loss: 1.5518107414245605\n",
            "Epoch [1/5], Step [260/3267], Loss: 1.5049965381622314\n",
            "Epoch [1/5], Step [270/3267], Loss: 1.5807380676269531\n",
            "Epoch [1/5], Step [280/3267], Loss: 1.56112802028656\n",
            "Epoch [1/5], Step [290/3267], Loss: 1.8771350383758545\n",
            "Epoch [1/5], Step [300/3267], Loss: 1.6035979986190796\n",
            "Epoch [1/5], Step [310/3267], Loss: 1.3670754432678223\n",
            "Epoch [1/5], Step [320/3267], Loss: 1.7115392684936523\n",
            "Epoch [1/5], Step [330/3267], Loss: 1.3539618253707886\n",
            "Epoch [1/5], Step [340/3267], Loss: 1.1363635063171387\n",
            "Epoch [1/5], Step [350/3267], Loss: 1.498807668685913\n",
            "Epoch [1/5], Step [360/3267], Loss: 1.5916392803192139\n",
            "Epoch [1/5], Step [370/3267], Loss: 1.3038434982299805\n",
            "Epoch [1/5], Step [380/3267], Loss: 1.439164400100708\n",
            "Epoch [1/5], Step [390/3267], Loss: 1.1944303512573242\n",
            "Epoch [1/5], Step [400/3267], Loss: 1.470226764678955\n",
            "Epoch [1/5], Step [410/3267], Loss: 1.3195010423660278\n",
            "Epoch [1/5], Step [420/3267], Loss: 1.3581633567810059\n",
            "Epoch [1/5], Step [430/3267], Loss: 1.16092848777771\n",
            "Epoch [1/5], Step [440/3267], Loss: 1.2385343313217163\n",
            "Epoch [1/5], Step [450/3267], Loss: 1.1576167345046997\n",
            "Epoch [1/5], Step [460/3267], Loss: 1.3252789974212646\n",
            "Epoch [1/5], Step [470/3267], Loss: 1.5531468391418457\n",
            "Epoch [1/5], Step [480/3267], Loss: 1.2505199909210205\n",
            "Epoch [1/5], Step [490/3267], Loss: 1.17230224609375\n",
            "Epoch [1/5], Step [500/3267], Loss: 1.390014410018921\n",
            "Epoch [1/5], Step [510/3267], Loss: 1.136397123336792\n",
            "Epoch [1/5], Step [520/3267], Loss: 1.0998798608779907\n",
            "Epoch [1/5], Step [530/3267], Loss: 1.1737034320831299\n",
            "Epoch [1/5], Step [540/3267], Loss: 1.2027831077575684\n",
            "Epoch [1/5], Step [550/3267], Loss: 1.48941969871521\n",
            "Epoch [1/5], Step [560/3267], Loss: 1.1329331398010254\n",
            "Epoch [1/5], Step [570/3267], Loss: 1.1991872787475586\n",
            "Epoch [1/5], Step [580/3267], Loss: 1.2603482007980347\n",
            "Epoch [1/5], Step [590/3267], Loss: 1.4764928817749023\n",
            "Epoch [1/5], Step [600/3267], Loss: 1.2571589946746826\n",
            "Epoch [1/5], Step [610/3267], Loss: 1.4261482954025269\n",
            "Epoch [1/5], Step [620/3267], Loss: 1.3092797994613647\n",
            "Epoch [1/5], Step [630/3267], Loss: 1.2380025386810303\n",
            "Epoch [1/5], Step [640/3267], Loss: 1.443576693534851\n",
            "Epoch [1/5], Step [650/3267], Loss: 1.357580304145813\n",
            "Epoch [1/5], Step [660/3267], Loss: 1.3183783292770386\n",
            "Epoch [1/5], Step [670/3267], Loss: 1.4272774457931519\n",
            "Epoch [1/5], Step [680/3267], Loss: 1.4002470970153809\n",
            "Epoch [1/5], Step [690/3267], Loss: 1.186447262763977\n",
            "Epoch [1/5], Step [700/3267], Loss: 1.2194993495941162\n",
            "Epoch [1/5], Step [710/3267], Loss: 1.3692809343338013\n",
            "Epoch [1/5], Step [720/3267], Loss: 1.3896666765213013\n",
            "Epoch [1/5], Step [730/3267], Loss: 1.207695722579956\n",
            "Epoch [1/5], Step [740/3267], Loss: 1.292624831199646\n",
            "Epoch [1/5], Step [750/3267], Loss: 1.430884599685669\n",
            "Epoch [1/5], Step [760/3267], Loss: 1.0652252435684204\n",
            "Epoch [1/5], Step [770/3267], Loss: 1.3182088136672974\n",
            "Epoch [1/5], Step [780/3267], Loss: 1.2994486093521118\n",
            "Epoch [1/5], Step [790/3267], Loss: 1.1214892864227295\n",
            "Epoch [1/5], Step [800/3267], Loss: 1.0399696826934814\n",
            "Epoch [1/5], Step [810/3267], Loss: 1.1777358055114746\n",
            "Epoch [1/5], Step [820/3267], Loss: 1.1681137084960938\n",
            "Epoch [1/5], Step [830/3267], Loss: 1.1578181982040405\n",
            "Epoch [1/5], Step [840/3267], Loss: 1.2956576347351074\n",
            "Epoch [1/5], Step [850/3267], Loss: 1.0797337293624878\n",
            "Epoch [1/5], Step [860/3267], Loss: 1.077309489250183\n",
            "Epoch [1/5], Step [870/3267], Loss: 1.2407708168029785\n",
            "Epoch [1/5], Step [880/3267], Loss: 1.0461699962615967\n",
            "Epoch [1/5], Step [890/3267], Loss: 1.2084457874298096\n",
            "Epoch [1/5], Step [900/3267], Loss: 1.3705295324325562\n",
            "Epoch [1/5], Step [910/3267], Loss: 1.176100254058838\n",
            "Epoch [1/5], Step [920/3267], Loss: 1.2405246496200562\n",
            "Epoch [1/5], Step [930/3267], Loss: 1.4084526300430298\n",
            "Epoch [1/5], Step [940/3267], Loss: 1.2066013813018799\n",
            "Epoch [1/5], Step [950/3267], Loss: 1.423454999923706\n",
            "Epoch [1/5], Step [960/3267], Loss: 1.2976651191711426\n",
            "Epoch [1/5], Step [970/3267], Loss: 1.3706661462783813\n",
            "Epoch [1/5], Step [980/3267], Loss: 1.0015366077423096\n",
            "Epoch [1/5], Step [990/3267], Loss: 0.9296266436576843\n",
            "Epoch [1/5], Step [1000/3267], Loss: 1.4214348793029785\n",
            "Epoch [1/5], Step [1010/3267], Loss: 1.13616144657135\n",
            "Epoch [1/5], Step [1020/3267], Loss: 1.2460805177688599\n",
            "Epoch [1/5], Step [1030/3267], Loss: 0.9906209111213684\n",
            "Epoch [1/5], Step [1040/3267], Loss: 1.2667316198349\n",
            "Epoch [1/5], Step [1050/3267], Loss: 1.0952059030532837\n",
            "Epoch [1/5], Step [1060/3267], Loss: 1.2514995336532593\n",
            "Epoch [1/5], Step [1070/3267], Loss: 1.4391096830368042\n",
            "Epoch [1/5], Step [1080/3267], Loss: 1.0880004167556763\n",
            "Epoch [1/5], Step [1090/3267], Loss: 1.1997089385986328\n",
            "Epoch [1/5], Step [1100/3267], Loss: 1.1021288633346558\n",
            "Epoch [1/5], Step [1110/3267], Loss: 1.164097785949707\n",
            "Epoch [1/5], Step [1120/3267], Loss: 1.481175422668457\n",
            "Epoch [1/5], Step [1130/3267], Loss: 1.0593070983886719\n",
            "Epoch [1/5], Step [1140/3267], Loss: 1.1158121824264526\n",
            "Epoch [1/5], Step [1150/3267], Loss: 1.2078200578689575\n",
            "Epoch [1/5], Step [1160/3267], Loss: 1.301796317100525\n",
            "Epoch [1/5], Step [1170/3267], Loss: 1.198695182800293\n",
            "Epoch [1/5], Step [1180/3267], Loss: 1.6013481616973877\n",
            "Epoch [1/5], Step [1190/3267], Loss: 1.1381319761276245\n",
            "Epoch [1/5], Step [1200/3267], Loss: 1.0734398365020752\n",
            "Epoch [1/5], Step [1210/3267], Loss: 1.2141547203063965\n",
            "Epoch [1/5], Step [1220/3267], Loss: 1.2956098318099976\n",
            "Epoch [1/5], Step [1230/3267], Loss: 1.1809688806533813\n",
            "Epoch [1/5], Step [1240/3267], Loss: 1.1226681470870972\n",
            "Epoch [1/5], Step [1250/3267], Loss: 1.167946219444275\n",
            "Epoch [1/5], Step [1260/3267], Loss: 1.3271971940994263\n",
            "Epoch [1/5], Step [1270/3267], Loss: 1.0003911256790161\n",
            "Epoch [1/5], Step [1280/3267], Loss: 1.1636821031570435\n",
            "Epoch [1/5], Step [1290/3267], Loss: 1.3502386808395386\n",
            "Epoch [1/5], Step [1300/3267], Loss: 1.151193618774414\n",
            "Epoch [1/5], Step [1310/3267], Loss: 1.1515413522720337\n",
            "Epoch [1/5], Step [1320/3267], Loss: 1.2255135774612427\n",
            "Epoch [1/5], Step [1330/3267], Loss: 1.1563725471496582\n",
            "Epoch [1/5], Step [1340/3267], Loss: 1.2218986749649048\n",
            "Epoch [1/5], Step [1350/3267], Loss: 1.2298628091812134\n",
            "Epoch [1/5], Step [1360/3267], Loss: 1.0561548471450806\n",
            "Epoch [1/5], Step [1370/3267], Loss: 0.9751195907592773\n",
            "Epoch [1/5], Step [1380/3267], Loss: 1.3568835258483887\n",
            "Epoch [1/5], Step [1390/3267], Loss: 1.4600399732589722\n",
            "Epoch [1/5], Step [1400/3267], Loss: 1.1921457052230835\n",
            "Epoch [1/5], Step [1410/3267], Loss: 1.2959163188934326\n",
            "Epoch [1/5], Step [1420/3267], Loss: 1.1091593503952026\n",
            "Epoch [1/5], Step [1430/3267], Loss: 1.1158119440078735\n",
            "Epoch [1/5], Step [1440/3267], Loss: 1.1022263765335083\n",
            "Epoch [1/5], Step [1450/3267], Loss: 1.4349373579025269\n",
            "Epoch [1/5], Step [1460/3267], Loss: 1.2667311429977417\n",
            "Epoch [1/5], Step [1470/3267], Loss: 0.9777084589004517\n",
            "Epoch [1/5], Step [1480/3267], Loss: 1.0758717060089111\n",
            "Epoch [1/5], Step [1490/3267], Loss: 1.0035967826843262\n",
            "Epoch [1/5], Step [1500/3267], Loss: 1.0285634994506836\n",
            "Epoch [1/5], Step [1510/3267], Loss: 1.160836935043335\n",
            "Epoch [1/5], Step [1520/3267], Loss: 1.0120407342910767\n",
            "Epoch [1/5], Step [1530/3267], Loss: 1.1767728328704834\n",
            "Epoch [1/5], Step [1540/3267], Loss: 1.0875773429870605\n",
            "Epoch [1/5], Step [1550/3267], Loss: 0.9946388602256775\n",
            "Epoch [1/5], Step [1560/3267], Loss: 1.233264446258545\n",
            "Epoch [1/5], Step [1570/3267], Loss: 1.0684990882873535\n",
            "Epoch [1/5], Step [1580/3267], Loss: 1.042280912399292\n",
            "Epoch [1/5], Step [1590/3267], Loss: 0.9582000374794006\n",
            "Epoch [1/5], Step [1600/3267], Loss: 1.198190689086914\n",
            "Epoch [1/5], Step [1610/3267], Loss: 1.0799096822738647\n",
            "Epoch [1/5], Step [1620/3267], Loss: 1.3496004343032837\n",
            "Epoch [1/5], Step [1630/3267], Loss: 1.320432186126709\n",
            "Epoch [1/5], Step [1640/3267], Loss: 1.3776531219482422\n",
            "Epoch [1/5], Step [1650/3267], Loss: 1.8203587532043457\n",
            "Epoch [1/5], Step [1660/3267], Loss: 1.3577449321746826\n",
            "Epoch [1/5], Step [1670/3267], Loss: 1.6557607650756836\n",
            "Epoch [1/5], Step [1680/3267], Loss: 1.291894793510437\n",
            "Epoch [1/5], Step [1690/3267], Loss: 1.5477800369262695\n",
            "Epoch [1/5], Step [1700/3267], Loss: 1.2311272621154785\n",
            "Epoch [1/5], Step [1710/3267], Loss: 1.4971076250076294\n",
            "Epoch [1/5], Step [1720/3267], Loss: 1.237551212310791\n",
            "Epoch [1/5], Step [1730/3267], Loss: 1.3731186389923096\n",
            "Epoch [1/5], Step [1740/3267], Loss: 1.34097421169281\n",
            "Epoch [1/5], Step [1750/3267], Loss: 1.0869559049606323\n",
            "Epoch [1/5], Step [1760/3267], Loss: 1.871559500694275\n",
            "Epoch [1/5], Step [1770/3267], Loss: 1.2327585220336914\n",
            "Epoch [1/5], Step [1780/3267], Loss: 1.5246561765670776\n",
            "Epoch [1/5], Step [1790/3267], Loss: 1.3108285665512085\n",
            "Epoch [1/5], Step [1800/3267], Loss: 1.2929545640945435\n",
            "Epoch [1/5], Step [1810/3267], Loss: 1.2154632806777954\n",
            "Epoch [1/5], Step [1820/3267], Loss: 1.3198440074920654\n",
            "Epoch [1/5], Step [1830/3267], Loss: 1.4117830991744995\n",
            "Epoch [1/5], Step [1840/3267], Loss: 1.3990789651870728\n",
            "Epoch [1/5], Step [1850/3267], Loss: 1.6840310096740723\n",
            "Epoch [1/5], Step [1860/3267], Loss: 1.3016698360443115\n",
            "Epoch [1/5], Step [1870/3267], Loss: 1.4228802919387817\n",
            "Epoch [1/5], Step [1880/3267], Loss: 1.4091864824295044\n",
            "Epoch [1/5], Step [1890/3267], Loss: 1.4094361066818237\n",
            "Epoch [1/5], Step [1900/3267], Loss: 1.3217220306396484\n",
            "Epoch [1/5], Step [1910/3267], Loss: 1.300614595413208\n",
            "Epoch [1/5], Step [1920/3267], Loss: 1.1667441129684448\n",
            "Epoch [1/5], Step [1930/3267], Loss: 1.5059679746627808\n",
            "Epoch [1/5], Step [1940/3267], Loss: 1.2800439596176147\n",
            "Epoch [1/5], Step [1950/3267], Loss: 1.378617525100708\n",
            "Epoch [1/5], Step [1960/3267], Loss: 1.1946592330932617\n",
            "Epoch [1/5], Step [1970/3267], Loss: 1.2741014957427979\n",
            "Epoch [1/5], Step [1980/3267], Loss: 1.2276962995529175\n",
            "Epoch [1/5], Step [1990/3267], Loss: 1.528685212135315\n",
            "Epoch [1/5], Step [2000/3267], Loss: 1.608588457107544\n",
            "Epoch [1/5], Step [2010/3267], Loss: 1.2421952486038208\n",
            "Epoch [1/5], Step [2020/3267], Loss: 1.6221853494644165\n",
            "Epoch [1/5], Step [2030/3267], Loss: 1.254941463470459\n",
            "Epoch [1/5], Step [2040/3267], Loss: 1.595650553703308\n",
            "Epoch [1/5], Step [2050/3267], Loss: 1.2482649087905884\n",
            "Epoch [1/5], Step [2060/3267], Loss: 1.1089791059494019\n",
            "Epoch [1/5], Step [2070/3267], Loss: 1.1786243915557861\n",
            "Epoch [1/5], Step [2080/3267], Loss: 1.2698097229003906\n",
            "Epoch [1/5], Step [2090/3267], Loss: 1.4200636148452759\n",
            "Epoch [1/5], Step [2100/3267], Loss: 1.2744240760803223\n",
            "Epoch [1/5], Step [2110/3267], Loss: 1.4401196241378784\n",
            "Epoch [1/5], Step [2120/3267], Loss: 1.5540673732757568\n",
            "Epoch [1/5], Step [2130/3267], Loss: 1.2354635000228882\n",
            "Epoch [1/5], Step [2140/3267], Loss: 1.4466956853866577\n",
            "Epoch [1/5], Step [2150/3267], Loss: 1.37886381149292\n",
            "Epoch [1/5], Step [2160/3267], Loss: 1.3349168300628662\n",
            "Epoch [1/5], Step [2170/3267], Loss: 1.1611241102218628\n",
            "Epoch [1/5], Step [2180/3267], Loss: 0.9795925617218018\n",
            "Epoch [1/5], Step [2190/3267], Loss: 1.2595851421356201\n",
            "Epoch [1/5], Step [2200/3267], Loss: 1.189871072769165\n",
            "Epoch [1/5], Step [2210/3267], Loss: 1.4032466411590576\n",
            "Epoch [1/5], Step [2220/3267], Loss: 1.3259464502334595\n",
            "Epoch [1/5], Step [2230/3267], Loss: 1.1343181133270264\n",
            "Epoch [1/5], Step [2240/3267], Loss: 1.070520281791687\n",
            "Epoch [1/5], Step [2250/3267], Loss: 1.1237261295318604\n",
            "Epoch [1/5], Step [2260/3267], Loss: 0.9674201011657715\n",
            "Epoch [1/5], Step [2270/3267], Loss: 1.4232326745986938\n",
            "Epoch [1/5], Step [2280/3267], Loss: 1.264729380607605\n",
            "Epoch [1/5], Step [2290/3267], Loss: 1.1571757793426514\n",
            "Epoch [1/5], Step [2300/3267], Loss: 1.1384392976760864\n",
            "Epoch [1/5], Step [2310/3267], Loss: 1.4504518508911133\n",
            "Epoch [1/5], Step [2320/3267], Loss: 1.2852637767791748\n",
            "Epoch [1/5], Step [2330/3267], Loss: 1.243588924407959\n",
            "Epoch [1/5], Step [2340/3267], Loss: 1.3481245040893555\n",
            "Epoch [1/5], Step [2350/3267], Loss: 1.2517822980880737\n",
            "Epoch [1/5], Step [2360/3267], Loss: 1.2826566696166992\n",
            "Epoch [1/5], Step [2370/3267], Loss: 1.4391754865646362\n",
            "Epoch [1/5], Step [2380/3267], Loss: 1.2320218086242676\n",
            "Epoch [1/5], Step [2390/3267], Loss: 1.1122636795043945\n",
            "Epoch [1/5], Step [2400/3267], Loss: 1.1446430683135986\n",
            "Epoch [1/5], Step [2410/3267], Loss: 1.1583983898162842\n",
            "Epoch [1/5], Step [2420/3267], Loss: 1.2233402729034424\n",
            "Epoch [1/5], Step [2430/3267], Loss: 1.2034906148910522\n",
            "Epoch [1/5], Step [2440/3267], Loss: 1.2921136617660522\n",
            "Epoch [1/5], Step [2450/3267], Loss: 1.4771549701690674\n",
            "Epoch [1/5], Step [2460/3267], Loss: 1.4305200576782227\n",
            "Epoch [1/5], Step [2470/3267], Loss: 1.2433527708053589\n",
            "Epoch [1/5], Step [2480/3267], Loss: 1.3487343788146973\n",
            "Epoch [1/5], Step [2490/3267], Loss: 1.5708788633346558\n",
            "Epoch [1/5], Step [2500/3267], Loss: 1.2607897520065308\n",
            "Epoch [1/5], Step [2510/3267], Loss: 1.511706829071045\n",
            "Epoch [1/5], Step [2520/3267], Loss: 1.1562895774841309\n",
            "Epoch [1/5], Step [2530/3267], Loss: 1.3375166654586792\n",
            "Epoch [1/5], Step [2540/3267], Loss: 1.2127312421798706\n",
            "Epoch [1/5], Step [2550/3267], Loss: 1.3099850416183472\n",
            "Epoch [1/5], Step [2560/3267], Loss: 1.1382157802581787\n",
            "Epoch [1/5], Step [2570/3267], Loss: 1.2722328901290894\n",
            "Epoch [1/5], Step [2580/3267], Loss: 1.418148159980774\n",
            "Epoch [1/5], Step [2590/3267], Loss: 1.3731690645217896\n",
            "Epoch [1/5], Step [2600/3267], Loss: 1.5368844270706177\n",
            "Epoch [1/5], Step [2610/3267], Loss: 1.2367799282073975\n",
            "Epoch [1/5], Step [2620/3267], Loss: 1.1756973266601562\n",
            "Epoch [1/5], Step [2630/3267], Loss: 1.2345006465911865\n",
            "Epoch [1/5], Step [2640/3267], Loss: 1.205680012702942\n",
            "Epoch [1/5], Step [2650/3267], Loss: 1.3469765186309814\n",
            "Epoch [1/5], Step [2660/3267], Loss: 1.2707595825195312\n",
            "Epoch [1/5], Step [2670/3267], Loss: 1.4934989213943481\n",
            "Epoch [1/5], Step [2680/3267], Loss: 1.2409977912902832\n",
            "Epoch [1/5], Step [2690/3267], Loss: 1.2713979482650757\n",
            "Epoch [1/5], Step [2700/3267], Loss: 1.213931918144226\n",
            "Epoch [1/5], Step [2710/3267], Loss: 1.0948413610458374\n",
            "Epoch [1/5], Step [2720/3267], Loss: 1.3653992414474487\n",
            "Epoch [1/5], Step [2730/3267], Loss: 1.0372897386550903\n",
            "Epoch [1/5], Step [2740/3267], Loss: 1.546129584312439\n",
            "Epoch [1/5], Step [2750/3267], Loss: 1.1022640466690063\n",
            "Epoch [1/5], Step [2760/3267], Loss: 1.2890620231628418\n",
            "Epoch [1/5], Step [2770/3267], Loss: 1.3984547853469849\n",
            "Epoch [1/5], Step [2780/3267], Loss: 1.1567137241363525\n",
            "Epoch [1/5], Step [2790/3267], Loss: 1.2166576385498047\n",
            "Epoch [1/5], Step [2800/3267], Loss: 1.1728922128677368\n",
            "Epoch [1/5], Step [2810/3267], Loss: 1.2825876474380493\n",
            "Epoch [1/5], Step [2820/3267], Loss: 1.312225580215454\n",
            "Epoch [1/5], Step [2830/3267], Loss: 1.0832659006118774\n",
            "Epoch [1/5], Step [2840/3267], Loss: 1.1660574674606323\n",
            "Epoch [1/5], Step [2850/3267], Loss: 1.5572187900543213\n",
            "Epoch [1/5], Step [2860/3267], Loss: 1.2361446619033813\n",
            "Epoch [1/5], Step [2870/3267], Loss: 1.2603050470352173\n",
            "Epoch [1/5], Step [2880/3267], Loss: 1.287717342376709\n",
            "Epoch [1/5], Step [2890/3267], Loss: 1.4011855125427246\n",
            "Epoch [1/5], Step [2900/3267], Loss: 1.330688238143921\n",
            "Epoch [1/5], Step [2910/3267], Loss: 1.1828688383102417\n",
            "Epoch [1/5], Step [2920/3267], Loss: 1.5091636180877686\n",
            "Epoch [1/5], Step [2930/3267], Loss: 1.2936311960220337\n",
            "Epoch [1/5], Step [2940/3267], Loss: 1.4355661869049072\n",
            "Epoch [1/5], Step [2950/3267], Loss: 1.4885127544403076\n",
            "Epoch [1/5], Step [2960/3267], Loss: 1.4234944581985474\n",
            "Epoch [1/5], Step [2970/3267], Loss: 1.444899082183838\n",
            "Epoch [1/5], Step [2980/3267], Loss: 1.5099505186080933\n",
            "Epoch [1/5], Step [2990/3267], Loss: 1.2318387031555176\n",
            "Epoch [1/5], Step [3000/3267], Loss: 1.262900710105896\n",
            "Epoch [1/5], Step [3010/3267], Loss: 1.2745308876037598\n",
            "Epoch [1/5], Step [3020/3267], Loss: 1.2096457481384277\n",
            "Epoch [1/5], Step [3030/3267], Loss: 1.2047598361968994\n",
            "Epoch [1/5], Step [3040/3267], Loss: 1.3537484407424927\n",
            "Epoch [1/5], Step [3050/3267], Loss: 1.4029035568237305\n",
            "Epoch [1/5], Step [3060/3267], Loss: 1.3915503025054932\n",
            "Epoch [1/5], Step [3070/3267], Loss: 1.3025474548339844\n",
            "Epoch [1/5], Step [3080/3267], Loss: 0.9630300402641296\n",
            "Epoch [1/5], Step [3090/3267], Loss: 1.1384981870651245\n",
            "Epoch [1/5], Step [3100/3267], Loss: 1.5096714496612549\n",
            "Epoch [1/5], Step [3110/3267], Loss: 1.2989277839660645\n",
            "Epoch [1/5], Step [3120/3267], Loss: 1.4602398872375488\n",
            "Epoch [1/5], Step [3130/3267], Loss: 1.1720225811004639\n",
            "Epoch [1/5], Step [3140/3267], Loss: 1.0637775659561157\n",
            "Epoch [1/5], Step [3150/3267], Loss: 1.1803051233291626\n",
            "Epoch [1/5], Step [3160/3267], Loss: 1.1059478521347046\n",
            "Epoch [1/5], Step [3170/3267], Loss: 1.2484440803527832\n",
            "Epoch [1/5], Step [3180/3267], Loss: 1.272855281829834\n",
            "Epoch [1/5], Step [3190/3267], Loss: 1.208471417427063\n",
            "Epoch [1/5], Step [3200/3267], Loss: 1.3594416379928589\n",
            "Epoch [1/5], Step [3210/3267], Loss: 1.2186201810836792\n",
            "Epoch [1/5], Step [3220/3267], Loss: 1.2481948137283325\n",
            "Epoch [1/5], Step [3230/3267], Loss: 1.3102654218673706\n",
            "Epoch [1/5], Step [3240/3267], Loss: 1.2586733102798462\n",
            "Epoch [1/5], Step [3250/3267], Loss: 1.3387415409088135\n",
            "Epoch [1/5], Step [3260/3267], Loss: 1.5973920822143555\n",
            "Epoch [2/5], Step [0/3267], Loss: 1.0283020734786987\n",
            "Epoch [2/5], Step [10/3267], Loss: 1.2066311836242676\n",
            "Epoch [2/5], Step [20/3267], Loss: 1.004743218421936\n",
            "Epoch [2/5], Step [30/3267], Loss: 1.0802712440490723\n",
            "Epoch [2/5], Step [40/3267], Loss: 1.195063591003418\n",
            "Epoch [2/5], Step [50/3267], Loss: 1.1456375122070312\n",
            "Epoch [2/5], Step [60/3267], Loss: 1.2354276180267334\n",
            "Epoch [2/5], Step [70/3267], Loss: 1.219115138053894\n",
            "Epoch [2/5], Step [80/3267], Loss: 1.2935338020324707\n",
            "Epoch [2/5], Step [90/3267], Loss: 1.3630366325378418\n",
            "Epoch [2/5], Step [100/3267], Loss: 1.3600023984909058\n",
            "Epoch [2/5], Step [110/3267], Loss: 1.2096669673919678\n",
            "Epoch [2/5], Step [120/3267], Loss: 1.185926914215088\n",
            "Epoch [2/5], Step [130/3267], Loss: 1.4035544395446777\n",
            "Epoch [2/5], Step [140/3267], Loss: 1.1322208642959595\n",
            "Epoch [2/5], Step [150/3267], Loss: 1.2915319204330444\n",
            "Epoch [2/5], Step [160/3267], Loss: 1.3816379308700562\n",
            "Epoch [2/5], Step [170/3267], Loss: 1.2151378393173218\n",
            "Epoch [2/5], Step [180/3267], Loss: 1.0411052703857422\n",
            "Epoch [2/5], Step [190/3267], Loss: 0.9981796145439148\n",
            "Epoch [2/5], Step [200/3267], Loss: 1.075915813446045\n",
            "Epoch [2/5], Step [210/3267], Loss: 1.1153424978256226\n",
            "Epoch [2/5], Step [220/3267], Loss: 1.188873052597046\n",
            "Epoch [2/5], Step [230/3267], Loss: 1.195412516593933\n",
            "Epoch [2/5], Step [240/3267], Loss: 1.1397639513015747\n",
            "Epoch [2/5], Step [250/3267], Loss: 1.4380943775177002\n",
            "Epoch [2/5], Step [260/3267], Loss: 1.2810475826263428\n",
            "Epoch [2/5], Step [270/3267], Loss: 1.422325849533081\n",
            "Epoch [2/5], Step [280/3267], Loss: 1.4063102006912231\n",
            "Epoch [2/5], Step [290/3267], Loss: 1.7241381406784058\n",
            "Epoch [2/5], Step [300/3267], Loss: 1.4855118989944458\n",
            "Epoch [2/5], Step [310/3267], Loss: 1.1962954998016357\n",
            "Epoch [2/5], Step [320/3267], Loss: 1.558835506439209\n",
            "Epoch [2/5], Step [330/3267], Loss: 1.165845513343811\n",
            "Epoch [2/5], Step [340/3267], Loss: 0.9167790412902832\n",
            "Epoch [2/5], Step [350/3267], Loss: 1.3324363231658936\n",
            "Epoch [2/5], Step [360/3267], Loss: 1.4709142446517944\n",
            "Epoch [2/5], Step [370/3267], Loss: 1.0880126953125\n",
            "Epoch [2/5], Step [380/3267], Loss: 1.3751449584960938\n",
            "Epoch [2/5], Step [390/3267], Loss: 1.0423855781555176\n",
            "Epoch [2/5], Step [400/3267], Loss: 1.3058183193206787\n",
            "Epoch [2/5], Step [410/3267], Loss: 1.1161423921585083\n",
            "Epoch [2/5], Step [420/3267], Loss: 1.2569648027420044\n",
            "Epoch [2/5], Step [430/3267], Loss: 1.064907431602478\n",
            "Epoch [2/5], Step [440/3267], Loss: 1.1177129745483398\n",
            "Epoch [2/5], Step [450/3267], Loss: 1.0180151462554932\n",
            "Epoch [2/5], Step [460/3267], Loss: 1.1841392517089844\n",
            "Epoch [2/5], Step [470/3267], Loss: 1.361758828163147\n",
            "Epoch [2/5], Step [480/3267], Loss: 1.101050615310669\n",
            "Epoch [2/5], Step [490/3267], Loss: 1.0547667741775513\n",
            "Epoch [2/5], Step [500/3267], Loss: 1.2856417894363403\n",
            "Epoch [2/5], Step [510/3267], Loss: 0.963513195514679\n",
            "Epoch [2/5], Step [520/3267], Loss: 0.9576084017753601\n",
            "Epoch [2/5], Step [530/3267], Loss: 0.9828875660896301\n",
            "Epoch [2/5], Step [540/3267], Loss: 1.087778925895691\n",
            "Epoch [2/5], Step [550/3267], Loss: 1.328783631324768\n",
            "Epoch [2/5], Step [560/3267], Loss: 0.9875890016555786\n",
            "Epoch [2/5], Step [570/3267], Loss: 1.0606138706207275\n",
            "Epoch [2/5], Step [580/3267], Loss: 1.119834065437317\n",
            "Epoch [2/5], Step [590/3267], Loss: 1.2965854406356812\n",
            "Epoch [2/5], Step [600/3267], Loss: 1.1241575479507446\n",
            "Epoch [2/5], Step [610/3267], Loss: 1.2312716245651245\n",
            "Epoch [2/5], Step [620/3267], Loss: 1.1658294200897217\n",
            "Epoch [2/5], Step [630/3267], Loss: 1.087753415107727\n",
            "Epoch [2/5], Step [640/3267], Loss: 1.3132625818252563\n",
            "Epoch [2/5], Step [650/3267], Loss: 1.239506721496582\n",
            "Epoch [2/5], Step [660/3267], Loss: 1.1872382164001465\n",
            "Epoch [2/5], Step [670/3267], Loss: 1.2862132787704468\n",
            "Epoch [2/5], Step [680/3267], Loss: 1.274249792098999\n",
            "Epoch [2/5], Step [690/3267], Loss: 1.0644142627716064\n",
            "Epoch [2/5], Step [700/3267], Loss: 1.0913609266281128\n",
            "Epoch [2/5], Step [710/3267], Loss: 1.257596731185913\n",
            "Epoch [2/5], Step [720/3267], Loss: 1.2297872304916382\n",
            "Epoch [2/5], Step [730/3267], Loss: 1.0673810243606567\n",
            "Epoch [2/5], Step [740/3267], Loss: 1.1193058490753174\n",
            "Epoch [2/5], Step [750/3267], Loss: 1.2413753271102905\n",
            "Epoch [2/5], Step [760/3267], Loss: 0.9721395373344421\n",
            "Epoch [2/5], Step [770/3267], Loss: 1.1959121227264404\n",
            "Epoch [2/5], Step [780/3267], Loss: 1.156973123550415\n",
            "Epoch [2/5], Step [790/3267], Loss: 0.9844231605529785\n",
            "Epoch [2/5], Step [800/3267], Loss: 0.9580680131912231\n",
            "Epoch [2/5], Step [810/3267], Loss: 1.06973397731781\n",
            "Epoch [2/5], Step [820/3267], Loss: 1.0589687824249268\n",
            "Epoch [2/5], Step [830/3267], Loss: 1.0503127574920654\n",
            "Epoch [2/5], Step [840/3267], Loss: 1.1689859628677368\n",
            "Epoch [2/5], Step [850/3267], Loss: 0.9886391758918762\n",
            "Epoch [2/5], Step [860/3267], Loss: 0.9878086447715759\n",
            "Epoch [2/5], Step [870/3267], Loss: 1.1009987592697144\n",
            "Epoch [2/5], Step [880/3267], Loss: 0.9627865552902222\n",
            "Epoch [2/5], Step [890/3267], Loss: 1.0998562574386597\n",
            "Epoch [2/5], Step [900/3267], Loss: 1.2160744667053223\n",
            "Epoch [2/5], Step [910/3267], Loss: 1.0410100221633911\n",
            "Epoch [2/5], Step [920/3267], Loss: 1.1123100519180298\n",
            "Epoch [2/5], Step [930/3267], Loss: 1.3054585456848145\n",
            "Epoch [2/5], Step [940/3267], Loss: 1.078362226486206\n",
            "Epoch [2/5], Step [950/3267], Loss: 1.323708176612854\n",
            "Epoch [2/5], Step [960/3267], Loss: 1.1610685586929321\n",
            "Epoch [2/5], Step [970/3267], Loss: 1.2556986808776855\n",
            "Epoch [2/5], Step [980/3267], Loss: 0.9111492037773132\n",
            "Epoch [2/5], Step [990/3267], Loss: 0.8254929184913635\n",
            "Epoch [2/5], Step [1000/3267], Loss: 1.3275154829025269\n",
            "Epoch [2/5], Step [1010/3267], Loss: 1.0150054693222046\n",
            "Epoch [2/5], Step [1020/3267], Loss: 1.1255580186843872\n",
            "Epoch [2/5], Step [1030/3267], Loss: 0.897555947303772\n",
            "Epoch [2/5], Step [1040/3267], Loss: 1.1290252208709717\n",
            "Epoch [2/5], Step [1050/3267], Loss: 0.9808981418609619\n",
            "Epoch [2/5], Step [1060/3267], Loss: 1.1165752410888672\n",
            "Epoch [2/5], Step [1070/3267], Loss: 1.2737252712249756\n",
            "Epoch [2/5], Step [1080/3267], Loss: 1.0001400709152222\n",
            "Epoch [2/5], Step [1090/3267], Loss: 1.1190990209579468\n",
            "Epoch [2/5], Step [1100/3267], Loss: 1.0216741561889648\n",
            "Epoch [2/5], Step [1110/3267], Loss: 1.0704528093338013\n",
            "Epoch [2/5], Step [1120/3267], Loss: 1.3133668899536133\n",
            "Epoch [2/5], Step [1130/3267], Loss: 0.9611127376556396\n",
            "Epoch [2/5], Step [1140/3267], Loss: 1.0280131101608276\n",
            "Epoch [2/5], Step [1150/3267], Loss: 1.1149990558624268\n",
            "Epoch [2/5], Step [1160/3267], Loss: 1.169217586517334\n",
            "Epoch [2/5], Step [1170/3267], Loss: 1.099076509475708\n",
            "Epoch [2/5], Step [1180/3267], Loss: 1.4861394166946411\n",
            "Epoch [2/5], Step [1190/3267], Loss: 1.0488282442092896\n",
            "Epoch [2/5], Step [1200/3267], Loss: 0.9827705025672913\n",
            "Epoch [2/5], Step [1210/3267], Loss: 1.09164559841156\n",
            "Epoch [2/5], Step [1220/3267], Loss: 1.1915836334228516\n",
            "Epoch [2/5], Step [1230/3267], Loss: 1.0693001747131348\n",
            "Epoch [2/5], Step [1240/3267], Loss: 1.040537714958191\n",
            "Epoch [2/5], Step [1250/3267], Loss: 1.0781079530715942\n",
            "Epoch [2/5], Step [1260/3267], Loss: 1.2217589616775513\n",
            "Epoch [2/5], Step [1270/3267], Loss: 0.8814117312431335\n",
            "Epoch [2/5], Step [1280/3267], Loss: 1.0346784591674805\n",
            "Epoch [2/5], Step [1290/3267], Loss: 1.2266017198562622\n",
            "Epoch [2/5], Step [1300/3267], Loss: 1.0392630100250244\n",
            "Epoch [2/5], Step [1310/3267], Loss: 1.0012202262878418\n",
            "Epoch [2/5], Step [1320/3267], Loss: 1.1268656253814697\n",
            "Epoch [2/5], Step [1330/3267], Loss: 1.0510615110397339\n",
            "Epoch [2/5], Step [1340/3267], Loss: 1.1464402675628662\n",
            "Epoch [2/5], Step [1350/3267], Loss: 1.1319315433502197\n",
            "Epoch [2/5], Step [1360/3267], Loss: 0.9507460594177246\n",
            "Epoch [2/5], Step [1370/3267], Loss: 0.893388569355011\n",
            "Epoch [2/5], Step [1380/3267], Loss: 1.2535676956176758\n",
            "Epoch [2/5], Step [1390/3267], Loss: 1.370322585105896\n",
            "Epoch [2/5], Step [1400/3267], Loss: 1.0696146488189697\n",
            "Epoch [2/5], Step [1410/3267], Loss: 1.1720062494277954\n",
            "Epoch [2/5], Step [1420/3267], Loss: 1.0340795516967773\n",
            "Epoch [2/5], Step [1430/3267], Loss: 1.0056183338165283\n",
            "Epoch [2/5], Step [1440/3267], Loss: 1.0155688524246216\n",
            "Epoch [2/5], Step [1450/3267], Loss: 1.3546031713485718\n",
            "Epoch [2/5], Step [1460/3267], Loss: 1.1558187007904053\n",
            "Epoch [2/5], Step [1470/3267], Loss: 0.9171525239944458\n",
            "Epoch [2/5], Step [1480/3267], Loss: 0.9841118454933167\n",
            "Epoch [2/5], Step [1490/3267], Loss: 0.9135175943374634\n",
            "Epoch [2/5], Step [1500/3267], Loss: 0.941906213760376\n",
            "Epoch [2/5], Step [1510/3267], Loss: 1.069643259048462\n",
            "Epoch [2/5], Step [1520/3267], Loss: 0.9172873497009277\n",
            "Epoch [2/5], Step [1530/3267], Loss: 1.0847749710083008\n",
            "Epoch [2/5], Step [1540/3267], Loss: 0.980160653591156\n",
            "Epoch [2/5], Step [1550/3267], Loss: 0.9180147051811218\n",
            "Epoch [2/5], Step [1560/3267], Loss: 1.1210767030715942\n",
            "Epoch [2/5], Step [1570/3267], Loss: 0.9704406261444092\n",
            "Epoch [2/5], Step [1580/3267], Loss: 0.973704993724823\n",
            "Epoch [2/5], Step [1590/3267], Loss: 0.8979990482330322\n",
            "Epoch [2/5], Step [1600/3267], Loss: 1.0746146440505981\n",
            "Epoch [2/5], Step [1610/3267], Loss: 0.9709650278091431\n",
            "Epoch [2/5], Step [1620/3267], Loss: 1.2005980014801025\n",
            "Epoch [2/5], Step [1630/3267], Loss: 1.1305252313613892\n",
            "Epoch [2/5], Step [1640/3267], Loss: 1.1399129629135132\n",
            "Epoch [2/5], Step [1650/3267], Loss: 1.4853566884994507\n",
            "Epoch [2/5], Step [1660/3267], Loss: 1.0467109680175781\n",
            "Epoch [2/5], Step [1670/3267], Loss: 1.44562828540802\n",
            "Epoch [2/5], Step [1680/3267], Loss: 1.1033942699432373\n",
            "Epoch [2/5], Step [1690/3267], Loss: 1.3113458156585693\n",
            "Epoch [2/5], Step [1700/3267], Loss: 1.0483992099761963\n",
            "Epoch [2/5], Step [1710/3267], Loss: 1.3010809421539307\n",
            "Epoch [2/5], Step [1720/3267], Loss: 1.02585768699646\n",
            "Epoch [2/5], Step [1730/3267], Loss: 1.1292479038238525\n",
            "Epoch [2/5], Step [1740/3267], Loss: 1.1352006196975708\n",
            "Epoch [2/5], Step [1750/3267], Loss: 0.9204495549201965\n",
            "Epoch [2/5], Step [1760/3267], Loss: 1.667462944984436\n",
            "Epoch [2/5], Step [1770/3267], Loss: 1.0312877893447876\n",
            "Epoch [2/5], Step [1780/3267], Loss: 1.3379534482955933\n",
            "Epoch [2/5], Step [1790/3267], Loss: 1.1559348106384277\n",
            "Epoch [2/5], Step [1800/3267], Loss: 1.1456547975540161\n",
            "Epoch [2/5], Step [1810/3267], Loss: 1.0375181436538696\n",
            "Epoch [2/5], Step [1820/3267], Loss: 1.138882040977478\n",
            "Epoch [2/5], Step [1830/3267], Loss: 1.2076622247695923\n",
            "Epoch [2/5], Step [1840/3267], Loss: 1.2356265783309937\n",
            "Epoch [2/5], Step [1850/3267], Loss: 1.4619313478469849\n",
            "Epoch [2/5], Step [1860/3267], Loss: 1.136136770248413\n",
            "Epoch [2/5], Step [1870/3267], Loss: 1.2685202360153198\n",
            "Epoch [2/5], Step [1880/3267], Loss: 1.2624729871749878\n",
            "Epoch [2/5], Step [1890/3267], Loss: 1.260029911994934\n",
            "Epoch [2/5], Step [1900/3267], Loss: 1.2257215976715088\n",
            "Epoch [2/5], Step [1910/3267], Loss: 1.138245940208435\n",
            "Epoch [2/5], Step [1920/3267], Loss: 1.035264253616333\n",
            "Epoch [2/5], Step [1930/3267], Loss: 1.3256679773330688\n",
            "Epoch [2/5], Step [1940/3267], Loss: 1.1047489643096924\n",
            "Epoch [2/5], Step [1950/3267], Loss: 1.2148958444595337\n",
            "Epoch [2/5], Step [1960/3267], Loss: 1.0577597618103027\n",
            "Epoch [2/5], Step [1970/3267], Loss: 1.1421078443527222\n",
            "Epoch [2/5], Step [1980/3267], Loss: 1.092936635017395\n",
            "Epoch [2/5], Step [1990/3267], Loss: 1.3458251953125\n",
            "Epoch [2/5], Step [2000/3267], Loss: 1.429732322692871\n",
            "Epoch [2/5], Step [2010/3267], Loss: 1.1112029552459717\n",
            "Epoch [2/5], Step [2020/3267], Loss: 1.4562628269195557\n",
            "Epoch [2/5], Step [2030/3267], Loss: 1.1504567861557007\n",
            "Epoch [2/5], Step [2040/3267], Loss: 1.4025332927703857\n",
            "Epoch [2/5], Step [2050/3267], Loss: 1.0959974527359009\n",
            "Epoch [2/5], Step [2060/3267], Loss: 0.9760860800743103\n",
            "Epoch [2/5], Step [2070/3267], Loss: 1.0539506673812866\n",
            "Epoch [2/5], Step [2080/3267], Loss: 1.0831944942474365\n",
            "Epoch [2/5], Step [2090/3267], Loss: 1.2827444076538086\n",
            "Epoch [2/5], Step [2100/3267], Loss: 1.1247459650039673\n",
            "Epoch [2/5], Step [2110/3267], Loss: 1.3051093816757202\n",
            "Epoch [2/5], Step [2120/3267], Loss: 1.3949071168899536\n",
            "Epoch [2/5], Step [2130/3267], Loss: 1.0760533809661865\n",
            "Epoch [2/5], Step [2140/3267], Loss: 1.3076623678207397\n",
            "Epoch [2/5], Step [2150/3267], Loss: 1.2190417051315308\n",
            "Epoch [2/5], Step [2160/3267], Loss: 1.2012107372283936\n",
            "Epoch [2/5], Step [2170/3267], Loss: 1.0337246656417847\n",
            "Epoch [2/5], Step [2180/3267], Loss: 0.8824699521064758\n",
            "Epoch [2/5], Step [2190/3267], Loss: 1.132818341255188\n",
            "Epoch [2/5], Step [2200/3267], Loss: 0.9962769150733948\n",
            "Epoch [2/5], Step [2210/3267], Loss: 1.250849723815918\n",
            "Epoch [2/5], Step [2220/3267], Loss: 1.1954206228256226\n",
            "Epoch [2/5], Step [2230/3267], Loss: 1.003070592880249\n",
            "Epoch [2/5], Step [2240/3267], Loss: 0.9710450172424316\n",
            "Epoch [2/5], Step [2250/3267], Loss: 0.9808192253112793\n",
            "Epoch [2/5], Step [2260/3267], Loss: 0.8319261074066162\n",
            "Epoch [2/5], Step [2270/3267], Loss: 1.2300316095352173\n",
            "Epoch [2/5], Step [2280/3267], Loss: 1.1307792663574219\n",
            "Epoch [2/5], Step [2290/3267], Loss: 1.0403648614883423\n",
            "Epoch [2/5], Step [2300/3267], Loss: 1.0078283548355103\n",
            "Epoch [2/5], Step [2310/3267], Loss: 1.2941776514053345\n",
            "Epoch [2/5], Step [2320/3267], Loss: 1.1406822204589844\n",
            "Epoch [2/5], Step [2330/3267], Loss: 1.1297550201416016\n",
            "Epoch [2/5], Step [2340/3267], Loss: 1.190659999847412\n",
            "Epoch [2/5], Step [2350/3267], Loss: 1.124934196472168\n",
            "Epoch [2/5], Step [2360/3267], Loss: 1.142601728439331\n",
            "Epoch [2/5], Step [2370/3267], Loss: 1.277411699295044\n",
            "Epoch [2/5], Step [2380/3267], Loss: 1.0926991701126099\n",
            "Epoch [2/5], Step [2390/3267], Loss: 0.990742564201355\n",
            "Epoch [2/5], Step [2400/3267], Loss: 1.0319348573684692\n",
            "Epoch [2/5], Step [2410/3267], Loss: 1.0061538219451904\n",
            "Epoch [2/5], Step [2420/3267], Loss: 1.0810168981552124\n",
            "Epoch [2/5], Step [2430/3267], Loss: 1.0700219869613647\n",
            "Epoch [2/5], Step [2440/3267], Loss: 1.1298737525939941\n",
            "Epoch [2/5], Step [2450/3267], Loss: 1.33011794090271\n",
            "Epoch [2/5], Step [2460/3267], Loss: 1.2868918180465698\n",
            "Epoch [2/5], Step [2470/3267], Loss: 1.1222378015518188\n",
            "Epoch [2/5], Step [2480/3267], Loss: 1.224212646484375\n",
            "Epoch [2/5], Step [2490/3267], Loss: 1.4043726921081543\n",
            "Epoch [2/5], Step [2500/3267], Loss: 1.1220349073410034\n",
            "Epoch [2/5], Step [2510/3267], Loss: 1.3719888925552368\n",
            "Epoch [2/5], Step [2520/3267], Loss: 1.0416285991668701\n",
            "Epoch [2/5], Step [2530/3267], Loss: 1.199185848236084\n",
            "Epoch [2/5], Step [2540/3267], Loss: 1.0587238073349\n",
            "Epoch [2/5], Step [2550/3267], Loss: 1.1941394805908203\n",
            "Epoch [2/5], Step [2560/3267], Loss: 1.0144542455673218\n",
            "Epoch [2/5], Step [2570/3267], Loss: 1.169950008392334\n",
            "Epoch [2/5], Step [2580/3267], Loss: 1.2751564979553223\n",
            "Epoch [2/5], Step [2590/3267], Loss: 1.2579957246780396\n",
            "Epoch [2/5], Step [2600/3267], Loss: 1.3783634901046753\n",
            "Epoch [2/5], Step [2610/3267], Loss: 1.1033227443695068\n",
            "Epoch [2/5], Step [2620/3267], Loss: 1.0140436887741089\n",
            "Epoch [2/5], Step [2630/3267], Loss: 1.0883334875106812\n",
            "Epoch [2/5], Step [2640/3267], Loss: 1.0517085790634155\n",
            "Epoch [2/5], Step [2650/3267], Loss: 1.2376993894577026\n",
            "Epoch [2/5], Step [2660/3267], Loss: 1.1374396085739136\n",
            "Epoch [2/5], Step [2670/3267], Loss: 1.3042155504226685\n",
            "Epoch [2/5], Step [2680/3267], Loss: 1.1351033449172974\n",
            "Epoch [2/5], Step [2690/3267], Loss: 1.1238914728164673\n",
            "Epoch [2/5], Step [2700/3267], Loss: 1.081386685371399\n",
            "Epoch [2/5], Step [2710/3267], Loss: 0.9794024229049683\n",
            "Epoch [2/5], Step [2720/3267], Loss: 1.2269248962402344\n",
            "Epoch [2/5], Step [2730/3267], Loss: 0.9490898847579956\n",
            "Epoch [2/5], Step [2740/3267], Loss: 1.3931100368499756\n",
            "Epoch [2/5], Step [2750/3267], Loss: 0.9819789528846741\n",
            "Epoch [2/5], Step [2760/3267], Loss: 1.1874051094055176\n",
            "Epoch [2/5], Step [2770/3267], Loss: 1.282631278038025\n",
            "Epoch [2/5], Step [2780/3267], Loss: 1.0255292654037476\n",
            "Epoch [2/5], Step [2790/3267], Loss: 1.0975699424743652\n",
            "Epoch [2/5], Step [2800/3267], Loss: 1.009527325630188\n",
            "Epoch [2/5], Step [2810/3267], Loss: 1.1552599668502808\n",
            "Epoch [2/5], Step [2820/3267], Loss: 1.2092074155807495\n",
            "Epoch [2/5], Step [2830/3267], Loss: 0.9829440116882324\n",
            "Epoch [2/5], Step [2840/3267], Loss: 1.0547560453414917\n",
            "Epoch [2/5], Step [2850/3267], Loss: 1.4001972675323486\n",
            "Epoch [2/5], Step [2860/3267], Loss: 1.1234632730484009\n",
            "Epoch [2/5], Step [2870/3267], Loss: 1.1464450359344482\n",
            "Epoch [2/5], Step [2880/3267], Loss: 1.1814477443695068\n",
            "Epoch [2/5], Step [2890/3267], Loss: 1.2415192127227783\n",
            "Epoch [2/5], Step [2900/3267], Loss: 1.2143869400024414\n",
            "Epoch [2/5], Step [2910/3267], Loss: 1.0653430223464966\n",
            "Epoch [2/5], Step [2920/3267], Loss: 1.3387047052383423\n",
            "Epoch [2/5], Step [2930/3267], Loss: 1.141302227973938\n",
            "Epoch [2/5], Step [2940/3267], Loss: 1.3100292682647705\n",
            "Epoch [2/5], Step [2950/3267], Loss: 1.3195459842681885\n",
            "Epoch [2/5], Step [2960/3267], Loss: 1.298203468322754\n",
            "Epoch [2/5], Step [2970/3267], Loss: 1.343818187713623\n",
            "Epoch [2/5], Step [2980/3267], Loss: 1.3347257375717163\n",
            "Epoch [2/5], Step [2990/3267], Loss: 1.0723814964294434\n",
            "Epoch [2/5], Step [3000/3267], Loss: 1.163107991218567\n",
            "Epoch [2/5], Step [3010/3267], Loss: 1.1476070880889893\n",
            "Epoch [2/5], Step [3020/3267], Loss: 1.0852627754211426\n",
            "Epoch [2/5], Step [3030/3267], Loss: 1.1013767719268799\n",
            "Epoch [2/5], Step [3040/3267], Loss: 1.2390320301055908\n",
            "Epoch [2/5], Step [3050/3267], Loss: 1.2172727584838867\n",
            "Epoch [2/5], Step [3060/3267], Loss: 1.254696249961853\n",
            "Epoch [2/5], Step [3070/3267], Loss: 1.191110372543335\n",
            "Epoch [2/5], Step [3080/3267], Loss: 0.8742470145225525\n",
            "Epoch [2/5], Step [3090/3267], Loss: 1.0356025695800781\n",
            "Epoch [2/5], Step [3100/3267], Loss: 1.3437343835830688\n",
            "Epoch [2/5], Step [3110/3267], Loss: 1.17073392868042\n",
            "Epoch [2/5], Step [3120/3267], Loss: 1.2988629341125488\n",
            "Epoch [2/5], Step [3130/3267], Loss: 1.0433522462844849\n",
            "Epoch [2/5], Step [3140/3267], Loss: 0.977167546749115\n",
            "Epoch [2/5], Step [3150/3267], Loss: 1.094927430152893\n",
            "Epoch [2/5], Step [3160/3267], Loss: 1.0255845785140991\n",
            "Epoch [2/5], Step [3170/3267], Loss: 1.1511625051498413\n",
            "Epoch [2/5], Step [3180/3267], Loss: 1.1482057571411133\n",
            "Epoch [2/5], Step [3190/3267], Loss: 1.073992133140564\n",
            "Epoch [2/5], Step [3200/3267], Loss: 1.2159522771835327\n",
            "Epoch [2/5], Step [3210/3267], Loss: 1.1037240028381348\n",
            "Epoch [2/5], Step [3220/3267], Loss: 1.1356618404388428\n",
            "Epoch [2/5], Step [3230/3267], Loss: 1.1670811176300049\n",
            "Epoch [2/5], Step [3240/3267], Loss: 1.1420015096664429\n",
            "Epoch [2/5], Step [3250/3267], Loss: 1.227658748626709\n",
            "Epoch [2/5], Step [3260/3267], Loss: 1.3954027891159058\n",
            "Epoch [3/5], Step [0/3267], Loss: 0.9624871611595154\n",
            "Epoch [3/5], Step [10/3267], Loss: 1.1308742761611938\n",
            "Epoch [3/5], Step [20/3267], Loss: 0.9569286704063416\n",
            "Epoch [3/5], Step [30/3267], Loss: 1.0089422464370728\n",
            "Epoch [3/5], Step [40/3267], Loss: 1.1358046531677246\n",
            "Epoch [3/5], Step [50/3267], Loss: 1.0732916593551636\n",
            "Epoch [3/5], Step [60/3267], Loss: 1.135864019393921\n",
            "Epoch [3/5], Step [70/3267], Loss: 1.132959246635437\n",
            "Epoch [3/5], Step [80/3267], Loss: 1.2185235023498535\n",
            "Epoch [3/5], Step [90/3267], Loss: 1.2705721855163574\n",
            "Epoch [3/5], Step [100/3267], Loss: 1.2490441799163818\n",
            "Epoch [3/5], Step [110/3267], Loss: 1.1352742910385132\n",
            "Epoch [3/5], Step [120/3267], Loss: 1.1300876140594482\n",
            "Epoch [3/5], Step [130/3267], Loss: 1.323781967163086\n",
            "Epoch [3/5], Step [140/3267], Loss: 1.0818265676498413\n",
            "Epoch [3/5], Step [150/3267], Loss: 1.1960628032684326\n",
            "Epoch [3/5], Step [160/3267], Loss: 1.2789859771728516\n",
            "Epoch [3/5], Step [170/3267], Loss: 1.1641227006912231\n",
            "Epoch [3/5], Step [180/3267], Loss: 0.9870596528053284\n",
            "Epoch [3/5], Step [190/3267], Loss: 0.9189518094062805\n",
            "Epoch [3/5], Step [200/3267], Loss: 0.9776434302330017\n",
            "Epoch [3/5], Step [210/3267], Loss: 1.0508087873458862\n",
            "Epoch [3/5], Step [220/3267], Loss: 1.1010609865188599\n",
            "Epoch [3/5], Step [230/3267], Loss: 1.1237577199935913\n",
            "Epoch [3/5], Step [240/3267], Loss: 1.0839624404907227\n",
            "Epoch [3/5], Step [250/3267], Loss: 1.3509193658828735\n",
            "Epoch [3/5], Step [260/3267], Loss: 1.2107959985733032\n",
            "Epoch [3/5], Step [270/3267], Loss: 1.3347127437591553\n",
            "Epoch [3/5], Step [280/3267], Loss: 1.3049758672714233\n",
            "Epoch [3/5], Step [290/3267], Loss: 1.583382487297058\n",
            "Epoch [3/5], Step [300/3267], Loss: 1.3986427783966064\n",
            "Epoch [3/5], Step [310/3267], Loss: 1.1267585754394531\n",
            "Epoch [3/5], Step [320/3267], Loss: 1.4758918285369873\n",
            "Epoch [3/5], Step [330/3267], Loss: 1.0971105098724365\n",
            "Epoch [3/5], Step [340/3267], Loss: 0.8610442876815796\n",
            "Epoch [3/5], Step [350/3267], Loss: 1.2737157344818115\n",
            "Epoch [3/5], Step [360/3267], Loss: 1.397678017616272\n",
            "Epoch [3/5], Step [370/3267], Loss: 1.0011645555496216\n",
            "Epoch [3/5], Step [380/3267], Loss: 1.2976819276809692\n",
            "Epoch [3/5], Step [390/3267], Loss: 0.9872620701789856\n",
            "Epoch [3/5], Step [400/3267], Loss: 1.2269469499588013\n",
            "Epoch [3/5], Step [410/3267], Loss: 1.0287784337997437\n",
            "Epoch [3/5], Step [420/3267], Loss: 1.2074389457702637\n",
            "Epoch [3/5], Step [430/3267], Loss: 1.0217339992523193\n",
            "Epoch [3/5], Step [440/3267], Loss: 1.060943603515625\n",
            "Epoch [3/5], Step [450/3267], Loss: 0.9639622569084167\n",
            "Epoch [3/5], Step [460/3267], Loss: 1.131872534751892\n",
            "Epoch [3/5], Step [470/3267], Loss: 1.2820035219192505\n",
            "Epoch [3/5], Step [480/3267], Loss: 1.0346826314926147\n",
            "Epoch [3/5], Step [490/3267], Loss: 1.0010194778442383\n",
            "Epoch [3/5], Step [500/3267], Loss: 1.2321631908416748\n",
            "Epoch [3/5], Step [510/3267], Loss: 0.893798291683197\n",
            "Epoch [3/5], Step [520/3267], Loss: 0.9037392139434814\n",
            "Epoch [3/5], Step [530/3267], Loss: 0.9086459279060364\n",
            "Epoch [3/5], Step [540/3267], Loss: 1.021383285522461\n",
            "Epoch [3/5], Step [550/3267], Loss: 1.246189832687378\n",
            "Epoch [3/5], Step [560/3267], Loss: 0.929473340511322\n",
            "Epoch [3/5], Step [570/3267], Loss: 1.0064901113510132\n",
            "Epoch [3/5], Step [580/3267], Loss: 1.046939492225647\n",
            "Epoch [3/5], Step [590/3267], Loss: 1.220551609992981\n",
            "Epoch [3/5], Step [600/3267], Loss: 1.0637067556381226\n",
            "Epoch [3/5], Step [610/3267], Loss: 1.145427942276001\n",
            "Epoch [3/5], Step [620/3267], Loss: 1.0959696769714355\n",
            "Epoch [3/5], Step [630/3267], Loss: 1.0165739059448242\n",
            "Epoch [3/5], Step [640/3267], Loss: 1.2401559352874756\n",
            "Epoch [3/5], Step [650/3267], Loss: 1.1818848848342896\n",
            "Epoch [3/5], Step [660/3267], Loss: 1.1141077280044556\n",
            "Epoch [3/5], Step [670/3267], Loss: 1.2055233716964722\n",
            "Epoch [3/5], Step [680/3267], Loss: 1.173980951309204\n",
            "Epoch [3/5], Step [690/3267], Loss: 0.9981831908226013\n",
            "Epoch [3/5], Step [700/3267], Loss: 1.0297783613204956\n",
            "Epoch [3/5], Step [710/3267], Loss: 1.192685604095459\n",
            "Epoch [3/5], Step [720/3267], Loss: 1.1575496196746826\n",
            "Epoch [3/5], Step [730/3267], Loss: 0.9803483486175537\n",
            "Epoch [3/5], Step [740/3267], Loss: 1.0538393259048462\n",
            "Epoch [3/5], Step [750/3267], Loss: 1.1473506689071655\n",
            "Epoch [3/5], Step [760/3267], Loss: 0.9185360074043274\n",
            "Epoch [3/5], Step [770/3267], Loss: 1.1254889965057373\n",
            "Epoch [3/5], Step [780/3267], Loss: 1.077131986618042\n",
            "Epoch [3/5], Step [790/3267], Loss: 0.925495445728302\n",
            "Epoch [3/5], Step [800/3267], Loss: 0.9151257872581482\n",
            "Epoch [3/5], Step [810/3267], Loss: 1.0203406810760498\n",
            "Epoch [3/5], Step [820/3267], Loss: 1.0047821998596191\n",
            "Epoch [3/5], Step [830/3267], Loss: 0.9678261876106262\n",
            "Epoch [3/5], Step [840/3267], Loss: 1.1118605136871338\n",
            "Epoch [3/5], Step [850/3267], Loss: 0.9372653365135193\n",
            "Epoch [3/5], Step [860/3267], Loss: 0.9376695156097412\n",
            "Epoch [3/5], Step [870/3267], Loss: 1.0433628559112549\n",
            "Epoch [3/5], Step [880/3267], Loss: 0.9174164533615112\n",
            "Epoch [3/5], Step [890/3267], Loss: 1.0399636030197144\n",
            "Epoch [3/5], Step [900/3267], Loss: 1.1423778533935547\n",
            "Epoch [3/5], Step [910/3267], Loss: 0.9768663644790649\n",
            "Epoch [3/5], Step [920/3267], Loss: 1.0388498306274414\n",
            "Epoch [3/5], Step [930/3267], Loss: 1.240695595741272\n",
            "Epoch [3/5], Step [940/3267], Loss: 1.0207135677337646\n",
            "Epoch [3/5], Step [950/3267], Loss: 1.2616114616394043\n",
            "Epoch [3/5], Step [960/3267], Loss: 1.0879875421524048\n",
            "Epoch [3/5], Step [970/3267], Loss: 1.1765931844711304\n",
            "Epoch [3/5], Step [980/3267], Loss: 0.8566490411758423\n",
            "Epoch [3/5], Step [990/3267], Loss: 0.7850286960601807\n",
            "Epoch [3/5], Step [1000/3267], Loss: 1.2680193185806274\n",
            "Epoch [3/5], Step [1010/3267], Loss: 0.9613912105560303\n",
            "Epoch [3/5], Step [1020/3267], Loss: 1.0428510904312134\n",
            "Epoch [3/5], Step [1030/3267], Loss: 0.8500799536705017\n",
            "Epoch [3/5], Step [1040/3267], Loss: 1.0613945722579956\n",
            "Epoch [3/5], Step [1050/3267], Loss: 0.9084301590919495\n",
            "Epoch [3/5], Step [1060/3267], Loss: 1.034179449081421\n",
            "Epoch [3/5], Step [1070/3267], Loss: 1.1873464584350586\n",
            "Epoch [3/5], Step [1080/3267], Loss: 0.9431800246238708\n",
            "Epoch [3/5], Step [1090/3267], Loss: 1.0707865953445435\n",
            "Epoch [3/5], Step [1100/3267], Loss: 0.9728280901908875\n",
            "Epoch [3/5], Step [1110/3267], Loss: 1.0106812715530396\n",
            "Epoch [3/5], Step [1120/3267], Loss: 1.2157195806503296\n",
            "Epoch [3/5], Step [1130/3267], Loss: 0.8989048600196838\n",
            "Epoch [3/5], Step [1140/3267], Loss: 0.9841567873954773\n",
            "Epoch [3/5], Step [1150/3267], Loss: 1.0588828325271606\n",
            "Epoch [3/5], Step [1160/3267], Loss: 1.096359372138977\n",
            "Epoch [3/5], Step [1170/3267], Loss: 1.031481146812439\n",
            "Epoch [3/5], Step [1180/3267], Loss: 1.3975746631622314\n",
            "Epoch [3/5], Step [1190/3267], Loss: 0.9947425127029419\n",
            "Epoch [3/5], Step [1200/3267], Loss: 0.9402663707733154\n",
            "Epoch [3/5], Step [1210/3267], Loss: 1.0230194330215454\n",
            "Epoch [3/5], Step [1220/3267], Loss: 1.1276501417160034\n",
            "Epoch [3/5], Step [1230/3267], Loss: 1.002973198890686\n",
            "Epoch [3/5], Step [1240/3267], Loss: 0.9851781129837036\n",
            "Epoch [3/5], Step [1250/3267], Loss: 1.0278725624084473\n",
            "Epoch [3/5], Step [1260/3267], Loss: 1.150147557258606\n",
            "Epoch [3/5], Step [1270/3267], Loss: 0.8121757507324219\n",
            "Epoch [3/5], Step [1280/3267], Loss: 0.9682076573371887\n",
            "Epoch [3/5], Step [1290/3267], Loss: 1.1473796367645264\n",
            "Epoch [3/5], Step [1300/3267], Loss: 0.9736455678939819\n",
            "Epoch [3/5], Step [1310/3267], Loss: 0.9334291219711304\n",
            "Epoch [3/5], Step [1320/3267], Loss: 1.0602738857269287\n",
            "Epoch [3/5], Step [1330/3267], Loss: 0.9900261163711548\n",
            "Epoch [3/5], Step [1340/3267], Loss: 1.0913419723510742\n",
            "Epoch [3/5], Step [1350/3267], Loss: 1.0687131881713867\n",
            "Epoch [3/5], Step [1360/3267], Loss: 0.8925178647041321\n",
            "Epoch [3/5], Step [1370/3267], Loss: 0.848671019077301\n",
            "Epoch [3/5], Step [1380/3267], Loss: 1.1820045709609985\n",
            "Epoch [3/5], Step [1390/3267], Loss: 1.2961444854736328\n",
            "Epoch [3/5], Step [1400/3267], Loss: 0.9964324235916138\n",
            "Epoch [3/5], Step [1410/3267], Loss: 1.0989680290222168\n",
            "Epoch [3/5], Step [1420/3267], Loss: 0.9884674549102783\n",
            "Epoch [3/5], Step [1430/3267], Loss: 0.9302417635917664\n",
            "Epoch [3/5], Step [1440/3267], Loss: 0.9637671113014221\n",
            "Epoch [3/5], Step [1450/3267], Loss: 1.2883013486862183\n",
            "Epoch [3/5], Step [1460/3267], Loss: 1.0888274908065796\n",
            "Epoch [3/5], Step [1470/3267], Loss: 0.8785616159439087\n",
            "Epoch [3/5], Step [1480/3267], Loss: 0.9218634963035583\n",
            "Epoch [3/5], Step [1490/3267], Loss: 0.8611057996749878\n",
            "Epoch [3/5], Step [1500/3267], Loss: 0.8686864376068115\n",
            "Epoch [3/5], Step [1510/3267], Loss: 1.0034726858139038\n",
            "Epoch [3/5], Step [1520/3267], Loss: 0.8751962184906006\n",
            "Epoch [3/5], Step [1530/3267], Loss: 1.0220633745193481\n",
            "Epoch [3/5], Step [1540/3267], Loss: 0.9008914232254028\n",
            "Epoch [3/5], Step [1550/3267], Loss: 0.8723375797271729\n",
            "Epoch [3/5], Step [1560/3267], Loss: 1.0518805980682373\n",
            "Epoch [3/5], Step [1570/3267], Loss: 0.9130827784538269\n",
            "Epoch [3/5], Step [1580/3267], Loss: 0.9243218302726746\n",
            "Epoch [3/5], Step [1590/3267], Loss: 0.8560022115707397\n",
            "Epoch [3/5], Step [1600/3267], Loss: 0.9969609379768372\n",
            "Epoch [3/5], Step [1610/3267], Loss: 0.9072422981262207\n",
            "Epoch [3/5], Step [1620/3267], Loss: 1.1164032220840454\n",
            "Epoch [3/5], Step [1630/3267], Loss: 0.9986890554428101\n",
            "Epoch [3/5], Step [1640/3267], Loss: 1.011767029762268\n",
            "Epoch [3/5], Step [1650/3267], Loss: 1.2834733724594116\n",
            "Epoch [3/5], Step [1660/3267], Loss: 0.9321784973144531\n",
            "Epoch [3/5], Step [1670/3267], Loss: 1.2934168577194214\n",
            "Epoch [3/5], Step [1680/3267], Loss: 0.9965939521789551\n",
            "Epoch [3/5], Step [1690/3267], Loss: 1.1623786687850952\n",
            "Epoch [3/5], Step [1700/3267], Loss: 0.9409077763557434\n",
            "Epoch [3/5], Step [1710/3267], Loss: 1.179058313369751\n",
            "Epoch [3/5], Step [1720/3267], Loss: 0.897566556930542\n",
            "Epoch [3/5], Step [1730/3267], Loss: 0.9825918674468994\n",
            "Epoch [3/5], Step [1740/3267], Loss: 1.0076963901519775\n",
            "Epoch [3/5], Step [1750/3267], Loss: 0.8367364406585693\n",
            "Epoch [3/5], Step [1760/3267], Loss: 1.4967045783996582\n",
            "Epoch [3/5], Step [1770/3267], Loss: 0.9210243225097656\n",
            "Epoch [3/5], Step [1780/3267], Loss: 1.202340006828308\n",
            "Epoch [3/5], Step [1790/3267], Loss: 1.0460195541381836\n",
            "Epoch [3/5], Step [1800/3267], Loss: 1.0440046787261963\n",
            "Epoch [3/5], Step [1810/3267], Loss: 0.9258241057395935\n",
            "Epoch [3/5], Step [1820/3267], Loss: 1.0069841146469116\n",
            "Epoch [3/5], Step [1830/3267], Loss: 1.080551266670227\n",
            "Epoch [3/5], Step [1840/3267], Loss: 1.1146951913833618\n",
            "Epoch [3/5], Step [1850/3267], Loss: 1.3039281368255615\n",
            "Epoch [3/5], Step [1860/3267], Loss: 1.0321611166000366\n",
            "Epoch [3/5], Step [1870/3267], Loss: 1.1541577577590942\n",
            "Epoch [3/5], Step [1880/3267], Loss: 1.1516143083572388\n",
            "Epoch [3/5], Step [1890/3267], Loss: 1.1614322662353516\n",
            "Epoch [3/5], Step [1900/3267], Loss: 1.1384429931640625\n",
            "Epoch [3/5], Step [1910/3267], Loss: 1.019561529159546\n",
            "Epoch [3/5], Step [1920/3267], Loss: 0.8976982831954956\n",
            "Epoch [3/5], Step [1930/3267], Loss: 1.2179056406021118\n",
            "Epoch [3/5], Step [1940/3267], Loss: 0.9947265386581421\n",
            "Epoch [3/5], Step [1950/3267], Loss: 1.1197603940963745\n",
            "Epoch [3/5], Step [1960/3267], Loss: 0.9573563933372498\n",
            "Epoch [3/5], Step [1970/3267], Loss: 1.0506808757781982\n",
            "Epoch [3/5], Step [1980/3267], Loss: 1.01352059841156\n",
            "Epoch [3/5], Step [1990/3267], Loss: 1.205081820487976\n",
            "Epoch [3/5], Step [2000/3267], Loss: 1.298438549041748\n",
            "Epoch [3/5], Step [2010/3267], Loss: 1.0034146308898926\n",
            "Epoch [3/5], Step [2020/3267], Loss: 1.313563585281372\n",
            "Epoch [3/5], Step [2030/3267], Loss: 1.0686957836151123\n",
            "Epoch [3/5], Step [2040/3267], Loss: 1.2694813013076782\n",
            "Epoch [3/5], Step [2050/3267], Loss: 0.9832932353019714\n",
            "Epoch [3/5], Step [2060/3267], Loss: 0.863070011138916\n",
            "Epoch [3/5], Step [2070/3267], Loss: 0.9654493927955627\n",
            "Epoch [3/5], Step [2080/3267], Loss: 0.9478428959846497\n",
            "Epoch [3/5], Step [2090/3267], Loss: 1.1856809854507446\n",
            "Epoch [3/5], Step [2100/3267], Loss: 1.015007734298706\n",
            "Epoch [3/5], Step [2110/3267], Loss: 1.1899080276489258\n",
            "Epoch [3/5], Step [2120/3267], Loss: 1.2490484714508057\n",
            "Epoch [3/5], Step [2130/3267], Loss: 0.9556931853294373\n",
            "Epoch [3/5], Step [2140/3267], Loss: 1.217429280281067\n",
            "Epoch [3/5], Step [2150/3267], Loss: 1.0725739002227783\n",
            "Epoch [3/5], Step [2160/3267], Loss: 1.0988093614578247\n",
            "Epoch [3/5], Step [2170/3267], Loss: 0.9560003280639648\n",
            "Epoch [3/5], Step [2180/3267], Loss: 0.8127185702323914\n",
            "Epoch [3/5], Step [2190/3267], Loss: 1.0518049001693726\n",
            "Epoch [3/5], Step [2200/3267], Loss: 0.8779746890068054\n",
            "Epoch [3/5], Step [2210/3267], Loss: 1.1179858446121216\n",
            "Epoch [3/5], Step [2220/3267], Loss: 1.1001625061035156\n",
            "Epoch [3/5], Step [2230/3267], Loss: 0.8950613141059875\n",
            "Epoch [3/5], Step [2240/3267], Loss: 0.8924394249916077\n",
            "Epoch [3/5], Step [2250/3267], Loss: 0.8672827482223511\n",
            "Epoch [3/5], Step [2260/3267], Loss: 0.755355179309845\n",
            "Epoch [3/5], Step [2270/3267], Loss: 1.0723282098770142\n",
            "Epoch [3/5], Step [2280/3267], Loss: 1.0372350215911865\n",
            "Epoch [3/5], Step [2290/3267], Loss: 0.9406630396842957\n",
            "Epoch [3/5], Step [2300/3267], Loss: 0.9084410071372986\n",
            "Epoch [3/5], Step [2310/3267], Loss: 1.1725873947143555\n",
            "Epoch [3/5], Step [2320/3267], Loss: 1.024214267730713\n",
            "Epoch [3/5], Step [2330/3267], Loss: 1.0421510934829712\n",
            "Epoch [3/5], Step [2340/3267], Loss: 1.0903842449188232\n",
            "Epoch [3/5], Step [2350/3267], Loss: 1.0298629999160767\n",
            "Epoch [3/5], Step [2360/3267], Loss: 1.0439598560333252\n",
            "Epoch [3/5], Step [2370/3267], Loss: 1.1564921140670776\n",
            "Epoch [3/5], Step [2380/3267], Loss: 1.0089607238769531\n",
            "Epoch [3/5], Step [2390/3267], Loss: 0.919526219367981\n",
            "Epoch [3/5], Step [2400/3267], Loss: 0.9540811777114868\n",
            "Epoch [3/5], Step [2410/3267], Loss: 0.9090703129768372\n",
            "Epoch [3/5], Step [2420/3267], Loss: 0.9863197207450867\n",
            "Epoch [3/5], Step [2430/3267], Loss: 0.9581791758537292\n",
            "Epoch [3/5], Step [2440/3267], Loss: 0.990898072719574\n",
            "Epoch [3/5], Step [2450/3267], Loss: 1.2004746198654175\n",
            "Epoch [3/5], Step [2460/3267], Loss: 1.1525914669036865\n",
            "Epoch [3/5], Step [2470/3267], Loss: 1.0095441341400146\n",
            "Epoch [3/5], Step [2480/3267], Loss: 1.1334443092346191\n",
            "Epoch [3/5], Step [2490/3267], Loss: 1.2716376781463623\n",
            "Epoch [3/5], Step [2500/3267], Loss: 1.0231173038482666\n",
            "Epoch [3/5], Step [2510/3267], Loss: 1.2326499223709106\n",
            "Epoch [3/5], Step [2520/3267], Loss: 0.9465634822845459\n",
            "Epoch [3/5], Step [2530/3267], Loss: 1.1009849309921265\n",
            "Epoch [3/5], Step [2540/3267], Loss: 0.9575662016868591\n",
            "Epoch [3/5], Step [2550/3267], Loss: 1.1088957786560059\n",
            "Epoch [3/5], Step [2560/3267], Loss: 0.9400949478149414\n",
            "Epoch [3/5], Step [2570/3267], Loss: 1.0784202814102173\n",
            "Epoch [3/5], Step [2580/3267], Loss: 1.1568241119384766\n",
            "Epoch [3/5], Step [2590/3267], Loss: 1.1322987079620361\n",
            "Epoch [3/5], Step [2600/3267], Loss: 1.213842511177063\n",
            "Epoch [3/5], Step [2610/3267], Loss: 1.0091358423233032\n",
            "Epoch [3/5], Step [2620/3267], Loss: 0.9221535325050354\n",
            "Epoch [3/5], Step [2630/3267], Loss: 0.9762856364250183\n",
            "Epoch [3/5], Step [2640/3267], Loss: 0.9430956244468689\n",
            "Epoch [3/5], Step [2650/3267], Loss: 1.140687346458435\n",
            "Epoch [3/5], Step [2660/3267], Loss: 1.042082667350769\n",
            "Epoch [3/5], Step [2670/3267], Loss: 1.178065538406372\n",
            "Epoch [3/5], Step [2680/3267], Loss: 1.0528531074523926\n",
            "Epoch [3/5], Step [2690/3267], Loss: 1.0149449110031128\n",
            "Epoch [3/5], Step [2700/3267], Loss: 0.9872665405273438\n",
            "Epoch [3/5], Step [2710/3267], Loss: 0.9030447602272034\n",
            "Epoch [3/5], Step [2720/3267], Loss: 1.1252503395080566\n",
            "Epoch [3/5], Step [2730/3267], Loss: 0.886262059211731\n",
            "Epoch [3/5], Step [2740/3267], Loss: 1.239623785018921\n",
            "Epoch [3/5], Step [2750/3267], Loss: 0.8919429779052734\n",
            "Epoch [3/5], Step [2760/3267], Loss: 1.0992413759231567\n",
            "Epoch [3/5], Step [2770/3267], Loss: 1.178414225578308\n",
            "Epoch [3/5], Step [2780/3267], Loss: 0.9250963926315308\n",
            "Epoch [3/5], Step [2790/3267], Loss: 1.0038522481918335\n",
            "Epoch [3/5], Step [2800/3267], Loss: 0.9095032215118408\n",
            "Epoch [3/5], Step [2810/3267], Loss: 1.0636247396469116\n",
            "Epoch [3/5], Step [2820/3267], Loss: 1.1309150457382202\n",
            "Epoch [3/5], Step [2830/3267], Loss: 0.9036834239959717\n",
            "Epoch [3/5], Step [2840/3267], Loss: 0.9640807509422302\n",
            "Epoch [3/5], Step [2850/3267], Loss: 1.2681066989898682\n",
            "Epoch [3/5], Step [2860/3267], Loss: 1.031890630722046\n",
            "Epoch [3/5], Step [2870/3267], Loss: 1.0622631311416626\n",
            "Epoch [3/5], Step [2880/3267], Loss: 1.0932520627975464\n",
            "Epoch [3/5], Step [2890/3267], Loss: 1.117152214050293\n",
            "Epoch [3/5], Step [2900/3267], Loss: 1.1181641817092896\n",
            "Epoch [3/5], Step [2910/3267], Loss: 0.9815839529037476\n",
            "Epoch [3/5], Step [2920/3267], Loss: 1.2252110242843628\n",
            "Epoch [3/5], Step [2930/3267], Loss: 1.0152500867843628\n",
            "Epoch [3/5], Step [2940/3267], Loss: 1.2113268375396729\n",
            "Epoch [3/5], Step [2950/3267], Loss: 1.1887961626052856\n",
            "Epoch [3/5], Step [2960/3267], Loss: 1.199586033821106\n",
            "Epoch [3/5], Step [2970/3267], Loss: 1.2561947107315063\n",
            "Epoch [3/5], Step [2980/3267], Loss: 1.197036623954773\n",
            "Epoch [3/5], Step [2990/3267], Loss: 0.9739924669265747\n",
            "Epoch [3/5], Step [3000/3267], Loss: 1.0759233236312866\n",
            "Epoch [3/5], Step [3010/3267], Loss: 1.0553476810455322\n",
            "Epoch [3/5], Step [3020/3267], Loss: 0.9954190254211426\n",
            "Epoch [3/5], Step [3030/3267], Loss: 1.007994294166565\n",
            "Epoch [3/5], Step [3040/3267], Loss: 1.1320704221725464\n",
            "Epoch [3/5], Step [3050/3267], Loss: 1.0914244651794434\n",
            "Epoch [3/5], Step [3060/3267], Loss: 1.1353100538253784\n",
            "Epoch [3/5], Step [3070/3267], Loss: 1.1074415445327759\n",
            "Epoch [3/5], Step [3080/3267], Loss: 0.8108290433883667\n",
            "Epoch [3/5], Step [3090/3267], Loss: 0.9698653817176819\n",
            "Epoch [3/5], Step [3100/3267], Loss: 1.2243703603744507\n",
            "Epoch [3/5], Step [3110/3267], Loss: 1.0687627792358398\n",
            "Epoch [3/5], Step [3120/3267], Loss: 1.1618181467056274\n",
            "Epoch [3/5], Step [3130/3267], Loss: 0.9567723274230957\n",
            "Epoch [3/5], Step [3140/3267], Loss: 0.9042914509773254\n",
            "Epoch [3/5], Step [3150/3267], Loss: 1.0087547302246094\n",
            "Epoch [3/5], Step [3160/3267], Loss: 0.9516263604164124\n",
            "Epoch [3/5], Step [3170/3267], Loss: 1.0731303691864014\n",
            "Epoch [3/5], Step [3180/3267], Loss: 1.0597379207611084\n",
            "Epoch [3/5], Step [3190/3267], Loss: 0.970687210559845\n",
            "Epoch [3/5], Step [3200/3267], Loss: 1.1046570539474487\n",
            "Epoch [3/5], Step [3210/3267], Loss: 1.043591856956482\n",
            "Epoch [3/5], Step [3220/3267], Loss: 1.0312880277633667\n",
            "Epoch [3/5], Step [3230/3267], Loss: 1.0467674732208252\n",
            "Epoch [3/5], Step [3240/3267], Loss: 1.053566813468933\n",
            "Epoch [3/5], Step [3250/3267], Loss: 1.1382476091384888\n",
            "Epoch [3/5], Step [3260/3267], Loss: 1.2375229597091675\n",
            "Epoch [4/5], Step [0/3267], Loss: 0.9109930992126465\n",
            "Epoch [4/5], Step [10/3267], Loss: 1.068747639656067\n",
            "Epoch [4/5], Step [20/3267], Loss: 0.9165539741516113\n",
            "Epoch [4/5], Step [30/3267], Loss: 0.9629690647125244\n",
            "Epoch [4/5], Step [40/3267], Loss: 1.0883976221084595\n",
            "Epoch [4/5], Step [50/3267], Loss: 1.0133036375045776\n",
            "Epoch [4/5], Step [60/3267], Loss: 1.0571086406707764\n",
            "Epoch [4/5], Step [70/3267], Loss: 1.0612199306488037\n",
            "Epoch [4/5], Step [80/3267], Loss: 1.147088646888733\n",
            "Epoch [4/5], Step [90/3267], Loss: 1.1923614740371704\n",
            "Epoch [4/5], Step [100/3267], Loss: 1.160182237625122\n",
            "Epoch [4/5], Step [110/3267], Loss: 1.0734702348709106\n",
            "Epoch [4/5], Step [120/3267], Loss: 1.0742294788360596\n",
            "Epoch [4/5], Step [130/3267], Loss: 1.244712471961975\n",
            "Epoch [4/5], Step [140/3267], Loss: 1.0285983085632324\n",
            "Epoch [4/5], Step [150/3267], Loss: 1.1198959350585938\n",
            "Epoch [4/5], Step [160/3267], Loss: 1.185916781425476\n",
            "Epoch [4/5], Step [170/3267], Loss: 1.1205213069915771\n",
            "Epoch [4/5], Step [180/3267], Loss: 0.9509700536727905\n",
            "Epoch [4/5], Step [190/3267], Loss: 0.8728592395782471\n",
            "Epoch [4/5], Step [200/3267], Loss: 0.9126296639442444\n",
            "Epoch [4/5], Step [210/3267], Loss: 0.983523964881897\n",
            "Epoch [4/5], Step [220/3267], Loss: 1.028016209602356\n",
            "Epoch [4/5], Step [230/3267], Loss: 1.0689672231674194\n",
            "Epoch [4/5], Step [240/3267], Loss: 1.0402523279190063\n",
            "Epoch [4/5], Step [250/3267], Loss: 1.271623134613037\n",
            "Epoch [4/5], Step [260/3267], Loss: 1.1502485275268555\n",
            "Epoch [4/5], Step [270/3267], Loss: 1.237701416015625\n",
            "Epoch [4/5], Step [280/3267], Loss: 1.2389754056930542\n",
            "Epoch [4/5], Step [290/3267], Loss: 1.4785122871398926\n",
            "Epoch [4/5], Step [300/3267], Loss: 1.3298540115356445\n",
            "Epoch [4/5], Step [310/3267], Loss: 1.0634022951126099\n",
            "Epoch [4/5], Step [320/3267], Loss: 1.3801158666610718\n",
            "Epoch [4/5], Step [330/3267], Loss: 1.0420018434524536\n",
            "Epoch [4/5], Step [340/3267], Loss: 0.8151692152023315\n",
            "Epoch [4/5], Step [350/3267], Loss: 1.2198477983474731\n",
            "Epoch [4/5], Step [360/3267], Loss: 1.3284716606140137\n",
            "Epoch [4/5], Step [370/3267], Loss: 0.9267577528953552\n",
            "Epoch [4/5], Step [380/3267], Loss: 1.2202136516571045\n",
            "Epoch [4/5], Step [390/3267], Loss: 0.9561662673950195\n",
            "Epoch [4/5], Step [400/3267], Loss: 1.1560688018798828\n",
            "Epoch [4/5], Step [410/3267], Loss: 0.9744872450828552\n",
            "Epoch [4/5], Step [420/3267], Loss: 1.1606053113937378\n",
            "Epoch [4/5], Step [430/3267], Loss: 0.9791839122772217\n",
            "Epoch [4/5], Step [440/3267], Loss: 1.0172208547592163\n",
            "Epoch [4/5], Step [450/3267], Loss: 0.9205584526062012\n",
            "Epoch [4/5], Step [460/3267], Loss: 1.0829859972000122\n",
            "Epoch [4/5], Step [470/3267], Loss: 1.2027995586395264\n",
            "Epoch [4/5], Step [480/3267], Loss: 0.9873992204666138\n",
            "Epoch [4/5], Step [490/3267], Loss: 0.9672622680664062\n",
            "Epoch [4/5], Step [500/3267], Loss: 1.194455862045288\n",
            "Epoch [4/5], Step [510/3267], Loss: 0.8483242988586426\n",
            "Epoch [4/5], Step [520/3267], Loss: 0.8672810196876526\n",
            "Epoch [4/5], Step [530/3267], Loss: 0.8590036034584045\n",
            "Epoch [4/5], Step [540/3267], Loss: 0.9673514366149902\n",
            "Epoch [4/5], Step [550/3267], Loss: 1.1732248067855835\n",
            "Epoch [4/5], Step [560/3267], Loss: 0.8723399639129639\n",
            "Epoch [4/5], Step [570/3267], Loss: 0.9554169178009033\n",
            "Epoch [4/5], Step [580/3267], Loss: 0.9891561269760132\n",
            "Epoch [4/5], Step [590/3267], Loss: 1.1616841554641724\n",
            "Epoch [4/5], Step [600/3267], Loss: 1.0182870626449585\n",
            "Epoch [4/5], Step [610/3267], Loss: 1.092982292175293\n",
            "Epoch [4/5], Step [620/3267], Loss: 1.0275253057479858\n",
            "Epoch [4/5], Step [630/3267], Loss: 0.959315299987793\n",
            "Epoch [4/5], Step [640/3267], Loss: 1.1586506366729736\n",
            "Epoch [4/5], Step [650/3267], Loss: 1.129584789276123\n",
            "Epoch [4/5], Step [660/3267], Loss: 1.0620172023773193\n",
            "Epoch [4/5], Step [670/3267], Loss: 1.1443274021148682\n",
            "Epoch [4/5], Step [680/3267], Loss: 1.098559021949768\n",
            "Epoch [4/5], Step [690/3267], Loss: 0.9394755363464355\n",
            "Epoch [4/5], Step [700/3267], Loss: 0.9847231507301331\n",
            "Epoch [4/5], Step [710/3267], Loss: 1.1417378187179565\n",
            "Epoch [4/5], Step [720/3267], Loss: 1.099186658859253\n",
            "Epoch [4/5], Step [730/3267], Loss: 0.9064196348190308\n",
            "Epoch [4/5], Step [740/3267], Loss: 1.0017446279525757\n",
            "Epoch [4/5], Step [750/3267], Loss: 1.06830894947052\n",
            "Epoch [4/5], Step [760/3267], Loss: 0.8677535057067871\n",
            "Epoch [4/5], Step [770/3267], Loss: 1.0444889068603516\n",
            "Epoch [4/5], Step [780/3267], Loss: 1.0019166469573975\n",
            "Epoch [4/5], Step [790/3267], Loss: 0.8755943775177002\n",
            "Epoch [4/5], Step [800/3267], Loss: 0.8754391074180603\n",
            "Epoch [4/5], Step [810/3267], Loss: 0.9690724015235901\n",
            "Epoch [4/5], Step [820/3267], Loss: 0.9419004917144775\n",
            "Epoch [4/5], Step [830/3267], Loss: 0.911689281463623\n",
            "Epoch [4/5], Step [840/3267], Loss: 1.054226279258728\n",
            "Epoch [4/5], Step [850/3267], Loss: 0.8888459205627441\n",
            "Epoch [4/5], Step [860/3267], Loss: 0.893833577632904\n",
            "Epoch [4/5], Step [870/3267], Loss: 0.9919303059577942\n",
            "Epoch [4/5], Step [880/3267], Loss: 0.873821496963501\n",
            "Epoch [4/5], Step [890/3267], Loss: 0.9880226850509644\n",
            "Epoch [4/5], Step [900/3267], Loss: 1.0765736103057861\n",
            "Epoch [4/5], Step [910/3267], Loss: 0.9216161966323853\n",
            "Epoch [4/5], Step [920/3267], Loss: 0.9887431263923645\n",
            "Epoch [4/5], Step [930/3267], Loss: 1.1894161701202393\n",
            "Epoch [4/5], Step [940/3267], Loss: 0.962844967842102\n",
            "Epoch [4/5], Step [950/3267], Loss: 1.207136631011963\n",
            "Epoch [4/5], Step [960/3267], Loss: 1.0222513675689697\n",
            "Epoch [4/5], Step [970/3267], Loss: 1.1114798784255981\n",
            "Epoch [4/5], Step [980/3267], Loss: 0.8181784152984619\n",
            "Epoch [4/5], Step [990/3267], Loss: 0.7466414570808411\n",
            "Epoch [4/5], Step [1000/3267], Loss: 1.187779426574707\n",
            "Epoch [4/5], Step [1010/3267], Loss: 0.906414806842804\n",
            "Epoch [4/5], Step [1020/3267], Loss: 0.9787101745605469\n",
            "Epoch [4/5], Step [1030/3267], Loss: 0.8179054260253906\n",
            "Epoch [4/5], Step [1040/3267], Loss: 1.0125435590744019\n",
            "Epoch [4/5], Step [1050/3267], Loss: 0.8533833026885986\n",
            "Epoch [4/5], Step [1060/3267], Loss: 0.9704380035400391\n",
            "Epoch [4/5], Step [1070/3267], Loss: 1.1057631969451904\n",
            "Epoch [4/5], Step [1080/3267], Loss: 0.8993648886680603\n",
            "Epoch [4/5], Step [1090/3267], Loss: 1.016961932182312\n",
            "Epoch [4/5], Step [1100/3267], Loss: 0.9371786117553711\n",
            "Epoch [4/5], Step [1110/3267], Loss: 0.9629133343696594\n",
            "Epoch [4/5], Step [1120/3267], Loss: 1.1298366785049438\n",
            "Epoch [4/5], Step [1130/3267], Loss: 0.848808765411377\n",
            "Epoch [4/5], Step [1140/3267], Loss: 0.9334741234779358\n",
            "Epoch [4/5], Step [1150/3267], Loss: 1.0175448656082153\n",
            "Epoch [4/5], Step [1160/3267], Loss: 1.0315136909484863\n",
            "Epoch [4/5], Step [1170/3267], Loss: 0.9819047451019287\n",
            "Epoch [4/5], Step [1180/3267], Loss: 1.306191325187683\n",
            "Epoch [4/5], Step [1190/3267], Loss: 0.9575331211090088\n",
            "Epoch [4/5], Step [1200/3267], Loss: 0.896106481552124\n",
            "Epoch [4/5], Step [1210/3267], Loss: 0.9632068276405334\n",
            "Epoch [4/5], Step [1220/3267], Loss: 1.0745741128921509\n",
            "Epoch [4/5], Step [1230/3267], Loss: 0.9543229341506958\n",
            "Epoch [4/5], Step [1240/3267], Loss: 0.9373454451560974\n",
            "Epoch [4/5], Step [1250/3267], Loss: 0.9860602021217346\n",
            "Epoch [4/5], Step [1260/3267], Loss: 1.08627450466156\n",
            "Epoch [4/5], Step [1270/3267], Loss: 0.7633432745933533\n",
            "Epoch [4/5], Step [1280/3267], Loss: 0.9211317896842957\n",
            "Epoch [4/5], Step [1290/3267], Loss: 1.0786606073379517\n",
            "Epoch [4/5], Step [1300/3267], Loss: 0.9268993735313416\n",
            "Epoch [4/5], Step [1310/3267], Loss: 0.8675956726074219\n",
            "Epoch [4/5], Step [1320/3267], Loss: 0.9991698265075684\n",
            "Epoch [4/5], Step [1330/3267], Loss: 0.9489333629608154\n",
            "Epoch [4/5], Step [1340/3267], Loss: 1.0373034477233887\n",
            "Epoch [4/5], Step [1350/3267], Loss: 1.015915036201477\n",
            "Epoch [4/5], Step [1360/3267], Loss: 0.8553914427757263\n",
            "Epoch [4/5], Step [1370/3267], Loss: 0.8234360814094543\n",
            "Epoch [4/5], Step [1380/3267], Loss: 1.1155952215194702\n",
            "Epoch [4/5], Step [1390/3267], Loss: 1.221215009689331\n",
            "Epoch [4/5], Step [1400/3267], Loss: 0.9279387593269348\n",
            "Epoch [4/5], Step [1410/3267], Loss: 1.0381405353546143\n",
            "Epoch [4/5], Step [1420/3267], Loss: 0.9409212470054626\n",
            "Epoch [4/5], Step [1430/3267], Loss: 0.8702174425125122\n",
            "Epoch [4/5], Step [1440/3267], Loss: 0.924461305141449\n",
            "Epoch [4/5], Step [1450/3267], Loss: 1.211681604385376\n",
            "Epoch [4/5], Step [1460/3267], Loss: 1.0383069515228271\n",
            "Epoch [4/5], Step [1470/3267], Loss: 0.8418456315994263\n",
            "Epoch [4/5], Step [1480/3267], Loss: 0.8770979046821594\n",
            "Epoch [4/5], Step [1490/3267], Loss: 0.8099974393844604\n",
            "Epoch [4/5], Step [1500/3267], Loss: 0.8194147944450378\n",
            "Epoch [4/5], Step [1510/3267], Loss: 0.9530977010726929\n",
            "Epoch [4/5], Step [1520/3267], Loss: 0.8246788382530212\n",
            "Epoch [4/5], Step [1530/3267], Loss: 0.9666431546211243\n",
            "Epoch [4/5], Step [1540/3267], Loss: 0.851615309715271\n",
            "Epoch [4/5], Step [1550/3267], Loss: 0.8288296461105347\n",
            "Epoch [4/5], Step [1560/3267], Loss: 1.0007429122924805\n",
            "Epoch [4/5], Step [1570/3267], Loss: 0.869888424873352\n",
            "Epoch [4/5], Step [1580/3267], Loss: 0.8874397277832031\n",
            "Epoch [4/5], Step [1590/3267], Loss: 0.8182955384254456\n",
            "Epoch [4/5], Step [1600/3267], Loss: 0.936655580997467\n",
            "Epoch [4/5], Step [1610/3267], Loss: 0.8617131114006042\n",
            "Epoch [4/5], Step [1620/3267], Loss: 1.0362002849578857\n",
            "Epoch [4/5], Step [1630/3267], Loss: 0.906477153301239\n",
            "Epoch [4/5], Step [1640/3267], Loss: 0.8995770812034607\n",
            "Epoch [4/5], Step [1650/3267], Loss: 1.155867576599121\n",
            "Epoch [4/5], Step [1660/3267], Loss: 0.8574318885803223\n",
            "Epoch [4/5], Step [1670/3267], Loss: 1.1743135452270508\n",
            "Epoch [4/5], Step [1680/3267], Loss: 0.8946419358253479\n",
            "Epoch [4/5], Step [1690/3267], Loss: 1.0393102169036865\n",
            "Epoch [4/5], Step [1700/3267], Loss: 0.8764896392822266\n",
            "Epoch [4/5], Step [1710/3267], Loss: 1.076640248298645\n",
            "Epoch [4/5], Step [1720/3267], Loss: 0.8155653476715088\n",
            "Epoch [4/5], Step [1730/3267], Loss: 0.8842692971229553\n",
            "Epoch [4/5], Step [1740/3267], Loss: 0.9112766981124878\n",
            "Epoch [4/5], Step [1750/3267], Loss: 0.7785696387290955\n",
            "Epoch [4/5], Step [1760/3267], Loss: 1.3321858644485474\n",
            "Epoch [4/5], Step [1770/3267], Loss: 0.8383874893188477\n",
            "Epoch [4/5], Step [1780/3267], Loss: 1.100268006324768\n",
            "Epoch [4/5], Step [1790/3267], Loss: 0.9596609473228455\n",
            "Epoch [4/5], Step [1800/3267], Loss: 0.9852965474128723\n",
            "Epoch [4/5], Step [1810/3267], Loss: 0.8505005836486816\n",
            "Epoch [4/5], Step [1820/3267], Loss: 0.9034236669540405\n",
            "Epoch [4/5], Step [1830/3267], Loss: 0.9772195816040039\n",
            "Epoch [4/5], Step [1840/3267], Loss: 1.0175284147262573\n",
            "Epoch [4/5], Step [1850/3267], Loss: 1.1921815872192383\n",
            "Epoch [4/5], Step [1860/3267], Loss: 0.9471226930618286\n",
            "Epoch [4/5], Step [1870/3267], Loss: 1.063230037689209\n",
            "Epoch [4/5], Step [1880/3267], Loss: 1.0696074962615967\n",
            "Epoch [4/5], Step [1890/3267], Loss: 1.086629033088684\n",
            "Epoch [4/5], Step [1900/3267], Loss: 1.0756622552871704\n",
            "Epoch [4/5], Step [1910/3267], Loss: 0.9362773895263672\n",
            "Epoch [4/5], Step [1920/3267], Loss: 0.8067826628684998\n",
            "Epoch [4/5], Step [1930/3267], Loss: 1.1248797178268433\n",
            "Epoch [4/5], Step [1940/3267], Loss: 0.9135176539421082\n",
            "Epoch [4/5], Step [1950/3267], Loss: 1.0527243614196777\n",
            "Epoch [4/5], Step [1960/3267], Loss: 0.8793490529060364\n",
            "Epoch [4/5], Step [1970/3267], Loss: 0.9799886345863342\n",
            "Epoch [4/5], Step [1980/3267], Loss: 0.9433449506759644\n",
            "Epoch [4/5], Step [1990/3267], Loss: 1.1065593957901\n",
            "Epoch [4/5], Step [2000/3267], Loss: 1.1876542568206787\n",
            "Epoch [4/5], Step [2010/3267], Loss: 0.910212516784668\n",
            "Epoch [4/5], Step [2020/3267], Loss: 1.2261961698532104\n",
            "Epoch [4/5], Step [2030/3267], Loss: 0.9989418387413025\n",
            "Epoch [4/5], Step [2040/3267], Loss: 1.1367032527923584\n",
            "Epoch [4/5], Step [2050/3267], Loss: 0.9031729698181152\n",
            "Epoch [4/5], Step [2060/3267], Loss: 0.7862679958343506\n",
            "Epoch [4/5], Step [2070/3267], Loss: 0.9087298512458801\n",
            "Epoch [4/5], Step [2080/3267], Loss: 0.837502658367157\n",
            "Epoch [4/5], Step [2090/3267], Loss: 1.0941005945205688\n",
            "Epoch [4/5], Step [2100/3267], Loss: 0.9240741729736328\n",
            "Epoch [4/5], Step [2110/3267], Loss: 1.079813003540039\n",
            "Epoch [4/5], Step [2120/3267], Loss: 1.1298677921295166\n",
            "Epoch [4/5], Step [2130/3267], Loss: 0.8716394901275635\n",
            "Epoch [4/5], Step [2140/3267], Loss: 1.1435081958770752\n",
            "Epoch [4/5], Step [2150/3267], Loss: 0.9741096496582031\n",
            "Epoch [4/5], Step [2160/3267], Loss: 1.0043030977249146\n",
            "Epoch [4/5], Step [2170/3267], Loss: 0.8977741599082947\n",
            "Epoch [4/5], Step [2180/3267], Loss: 0.7573111653327942\n",
            "Epoch [4/5], Step [2190/3267], Loss: 0.9715167284011841\n",
            "Epoch [4/5], Step [2200/3267], Loss: 0.7963110208511353\n",
            "Epoch [4/5], Step [2210/3267], Loss: 1.0244742631912231\n",
            "Epoch [4/5], Step [2220/3267], Loss: 1.0255576372146606\n",
            "Epoch [4/5], Step [2230/3267], Loss: 0.8261315822601318\n",
            "Epoch [4/5], Step [2240/3267], Loss: 0.820219874382019\n",
            "Epoch [4/5], Step [2250/3267], Loss: 0.7961259484291077\n",
            "Epoch [4/5], Step [2260/3267], Loss: 0.6858248710632324\n",
            "Epoch [4/5], Step [2270/3267], Loss: 0.9639694094657898\n",
            "Epoch [4/5], Step [2280/3267], Loss: 0.9755123257637024\n",
            "Epoch [4/5], Step [2290/3267], Loss: 0.8759214878082275\n",
            "Epoch [4/5], Step [2300/3267], Loss: 0.8400653600692749\n",
            "Epoch [4/5], Step [2310/3267], Loss: 1.0769039392471313\n",
            "Epoch [4/5], Step [2320/3267], Loss: 0.9434188008308411\n",
            "Epoch [4/5], Step [2330/3267], Loss: 0.9493310451507568\n",
            "Epoch [4/5], Step [2340/3267], Loss: 1.0098456144332886\n",
            "Epoch [4/5], Step [2350/3267], Loss: 0.9469442963600159\n",
            "Epoch [4/5], Step [2360/3267], Loss: 0.9578095078468323\n",
            "Epoch [4/5], Step [2370/3267], Loss: 1.0713964700698853\n",
            "Epoch [4/5], Step [2380/3267], Loss: 0.9250984191894531\n",
            "Epoch [4/5], Step [2390/3267], Loss: 0.8626383543014526\n",
            "Epoch [4/5], Step [2400/3267], Loss: 0.8920659422874451\n",
            "Epoch [4/5], Step [2410/3267], Loss: 0.8479587435722351\n",
            "Epoch [4/5], Step [2420/3267], Loss: 0.9065224528312683\n",
            "Epoch [4/5], Step [2430/3267], Loss: 0.9014891386032104\n",
            "Epoch [4/5], Step [2440/3267], Loss: 0.9101871252059937\n",
            "Epoch [4/5], Step [2450/3267], Loss: 1.0893309116363525\n",
            "Epoch [4/5], Step [2460/3267], Loss: 1.0545191764831543\n",
            "Epoch [4/5], Step [2470/3267], Loss: 0.9214832186698914\n",
            "Epoch [4/5], Step [2480/3267], Loss: 1.0570191144943237\n",
            "Epoch [4/5], Step [2490/3267], Loss: 1.1590520143508911\n",
            "Epoch [4/5], Step [2500/3267], Loss: 0.9323246479034424\n",
            "Epoch [4/5], Step [2510/3267], Loss: 1.130319356918335\n",
            "Epoch [4/5], Step [2520/3267], Loss: 0.8716921806335449\n",
            "Epoch [4/5], Step [2530/3267], Loss: 1.002921223640442\n",
            "Epoch [4/5], Step [2540/3267], Loss: 0.8898775577545166\n",
            "Epoch [4/5], Step [2550/3267], Loss: 1.0339949131011963\n",
            "Epoch [4/5], Step [2560/3267], Loss: 0.8992873430252075\n",
            "Epoch [4/5], Step [2570/3267], Loss: 1.0031094551086426\n",
            "Epoch [4/5], Step [2580/3267], Loss: 1.0693392753601074\n",
            "Epoch [4/5], Step [2590/3267], Loss: 1.0447365045547485\n",
            "Epoch [4/5], Step [2600/3267], Loss: 1.0913892984390259\n",
            "Epoch [4/5], Step [2610/3267], Loss: 0.9505505561828613\n",
            "Epoch [4/5], Step [2620/3267], Loss: 0.8266536593437195\n",
            "Epoch [4/5], Step [2630/3267], Loss: 0.8878338932991028\n",
            "Epoch [4/5], Step [2640/3267], Loss: 0.8681291937828064\n",
            "Epoch [4/5], Step [2650/3267], Loss: 1.059653639793396\n",
            "Epoch [4/5], Step [2660/3267], Loss: 0.9523339867591858\n",
            "Epoch [4/5], Step [2670/3267], Loss: 1.079282283782959\n",
            "Epoch [4/5], Step [2680/3267], Loss: 0.9704030752182007\n",
            "Epoch [4/5], Step [2690/3267], Loss: 0.9318892955780029\n",
            "Epoch [4/5], Step [2700/3267], Loss: 0.9343048334121704\n",
            "Epoch [4/5], Step [2710/3267], Loss: 0.8497776985168457\n",
            "Epoch [4/5], Step [2720/3267], Loss: 1.045913577079773\n",
            "Epoch [4/5], Step [2730/3267], Loss: 0.827335000038147\n",
            "Epoch [4/5], Step [2740/3267], Loss: 1.140600562095642\n",
            "Epoch [4/5], Step [2750/3267], Loss: 0.8260812163352966\n",
            "Epoch [4/5], Step [2760/3267], Loss: 1.0248912572860718\n",
            "Epoch [4/5], Step [2770/3267], Loss: 1.0907886028289795\n",
            "Epoch [4/5], Step [2780/3267], Loss: 0.8637714385986328\n",
            "Epoch [4/5], Step [2790/3267], Loss: 0.9297298789024353\n",
            "Epoch [4/5], Step [2800/3267], Loss: 0.8530887365341187\n",
            "Epoch [4/5], Step [2810/3267], Loss: 0.9911006093025208\n",
            "Epoch [4/5], Step [2820/3267], Loss: 1.0564113855361938\n",
            "Epoch [4/5], Step [2830/3267], Loss: 0.8470020294189453\n",
            "Epoch [4/5], Step [2840/3267], Loss: 0.9026800394058228\n",
            "Epoch [4/5], Step [2850/3267], Loss: 1.1610385179519653\n",
            "Epoch [4/5], Step [2860/3267], Loss: 0.9609264731407166\n",
            "Epoch [4/5], Step [2870/3267], Loss: 0.9787073731422424\n",
            "Epoch [4/5], Step [2880/3267], Loss: 1.0120728015899658\n",
            "Epoch [4/5], Step [2890/3267], Loss: 1.009149193763733\n",
            "Epoch [4/5], Step [2900/3267], Loss: 1.0299146175384521\n",
            "Epoch [4/5], Step [2910/3267], Loss: 0.9076541662216187\n",
            "Epoch [4/5], Step [2920/3267], Loss: 1.1457149982452393\n",
            "Epoch [4/5], Step [2930/3267], Loss: 0.9238125085830688\n",
            "Epoch [4/5], Step [2940/3267], Loss: 1.1222617626190186\n",
            "Epoch [4/5], Step [2950/3267], Loss: 1.0950530767440796\n",
            "Epoch [4/5], Step [2960/3267], Loss: 1.128612995147705\n",
            "Epoch [4/5], Step [2970/3267], Loss: 1.1902039051055908\n",
            "Epoch [4/5], Step [2980/3267], Loss: 1.0847423076629639\n",
            "Epoch [4/5], Step [2990/3267], Loss: 0.883358895778656\n",
            "Epoch [4/5], Step [3000/3267], Loss: 0.9942355155944824\n",
            "Epoch [4/5], Step [3010/3267], Loss: 0.9756108522415161\n",
            "Epoch [4/5], Step [3020/3267], Loss: 0.9348223805427551\n",
            "Epoch [4/5], Step [3030/3267], Loss: 0.9308145046234131\n",
            "Epoch [4/5], Step [3040/3267], Loss: 1.046724796295166\n",
            "Epoch [4/5], Step [3050/3267], Loss: 0.9993439316749573\n",
            "Epoch [4/5], Step [3060/3267], Loss: 1.0374853610992432\n",
            "Epoch [4/5], Step [3070/3267], Loss: 1.0232378244400024\n",
            "Epoch [4/5], Step [3080/3267], Loss: 0.7518374919891357\n",
            "Epoch [4/5], Step [3090/3267], Loss: 0.9079382419586182\n",
            "Epoch [4/5], Step [3100/3267], Loss: 1.1285827159881592\n",
            "Epoch [4/5], Step [3110/3267], Loss: 1.012860655784607\n",
            "Epoch [4/5], Step [3120/3267], Loss: 1.054280161857605\n",
            "Epoch [4/5], Step [3130/3267], Loss: 0.8827764391899109\n",
            "Epoch [4/5], Step [3140/3267], Loss: 0.841686487197876\n",
            "Epoch [4/5], Step [3150/3267], Loss: 0.9550780653953552\n",
            "Epoch [4/5], Step [3160/3267], Loss: 0.8856006860733032\n",
            "Epoch [4/5], Step [3170/3267], Loss: 1.0028632879257202\n",
            "Epoch [4/5], Step [3180/3267], Loss: 0.9777573347091675\n",
            "Epoch [4/5], Step [3190/3267], Loss: 0.9097142815589905\n",
            "Epoch [4/5], Step [3200/3267], Loss: 1.0144633054733276\n",
            "Epoch [4/5], Step [3210/3267], Loss: 0.9865666627883911\n",
            "Epoch [4/5], Step [3220/3267], Loss: 0.9510359168052673\n",
            "Epoch [4/5], Step [3230/3267], Loss: 0.9496229290962219\n",
            "Epoch [4/5], Step [3240/3267], Loss: 0.9901280403137207\n",
            "Epoch [4/5], Step [3250/3267], Loss: 1.0620845556259155\n",
            "Epoch [4/5], Step [3260/3267], Loss: 1.112101674079895\n",
            "Epoch [5/5], Step [0/3267], Loss: 0.864406406879425\n",
            "Epoch [5/5], Step [10/3267], Loss: 1.0029124021530151\n",
            "Epoch [5/5], Step [20/3267], Loss: 0.8779119253158569\n",
            "Epoch [5/5], Step [30/3267], Loss: 0.9193629622459412\n",
            "Epoch [5/5], Step [40/3267], Loss: 1.040740728378296\n",
            "Epoch [5/5], Step [50/3267], Loss: 0.9547029733657837\n",
            "Epoch [5/5], Step [60/3267], Loss: 0.9833254814147949\n",
            "Epoch [5/5], Step [70/3267], Loss: 0.9944893717765808\n",
            "Epoch [5/5], Step [80/3267], Loss: 1.0864782333374023\n",
            "Epoch [5/5], Step [90/3267], Loss: 1.1193463802337646\n",
            "Epoch [5/5], Step [100/3267], Loss: 1.0836015939712524\n",
            "Epoch [5/5], Step [110/3267], Loss: 1.0203832387924194\n",
            "Epoch [5/5], Step [120/3267], Loss: 1.0264809131622314\n",
            "Epoch [5/5], Step [130/3267], Loss: 1.194752812385559\n",
            "Epoch [5/5], Step [140/3267], Loss: 0.9749161601066589\n",
            "Epoch [5/5], Step [150/3267], Loss: 1.054872751235962\n",
            "Epoch [5/5], Step [160/3267], Loss: 1.1089110374450684\n",
            "Epoch [5/5], Step [170/3267], Loss: 1.0784052610397339\n",
            "Epoch [5/5], Step [180/3267], Loss: 0.919741690158844\n",
            "Epoch [5/5], Step [190/3267], Loss: 0.8286296725273132\n",
            "Epoch [5/5], Step [200/3267], Loss: 0.8568863272666931\n",
            "Epoch [5/5], Step [210/3267], Loss: 0.9398812055587769\n",
            "Epoch [5/5], Step [220/3267], Loss: 0.9621692299842834\n",
            "Epoch [5/5], Step [230/3267], Loss: 1.0356239080429077\n",
            "Epoch [5/5], Step [240/3267], Loss: 1.0024174451828003\n",
            "Epoch [5/5], Step [250/3267], Loss: 1.186280608177185\n",
            "Epoch [5/5], Step [260/3267], Loss: 1.0993810892105103\n",
            "Epoch [5/5], Step [270/3267], Loss: 1.1512887477874756\n",
            "Epoch [5/5], Step [280/3267], Loss: 1.1889715194702148\n",
            "Epoch [5/5], Step [290/3267], Loss: 1.3818665742874146\n",
            "Epoch [5/5], Step [300/3267], Loss: 1.2602512836456299\n",
            "Epoch [5/5], Step [310/3267], Loss: 1.0064668655395508\n",
            "Epoch [5/5], Step [320/3267], Loss: 1.3023502826690674\n",
            "Epoch [5/5], Step [330/3267], Loss: 1.0053757429122925\n",
            "Epoch [5/5], Step [340/3267], Loss: 0.7875745892524719\n",
            "Epoch [5/5], Step [350/3267], Loss: 1.1460634469985962\n",
            "Epoch [5/5], Step [360/3267], Loss: 1.2696477174758911\n",
            "Epoch [5/5], Step [370/3267], Loss: 0.8903924822807312\n",
            "Epoch [5/5], Step [380/3267], Loss: 1.142477035522461\n",
            "Epoch [5/5], Step [390/3267], Loss: 0.9321197867393494\n",
            "Epoch [5/5], Step [400/3267], Loss: 1.1000207662582397\n",
            "Epoch [5/5], Step [410/3267], Loss: 0.940474271774292\n",
            "Epoch [5/5], Step [420/3267], Loss: 1.1120436191558838\n",
            "Epoch [5/5], Step [430/3267], Loss: 0.9476382732391357\n",
            "Epoch [5/5], Step [440/3267], Loss: 0.977595329284668\n",
            "Epoch [5/5], Step [450/3267], Loss: 0.8926911950111389\n",
            "Epoch [5/5], Step [460/3267], Loss: 1.0474662780761719\n",
            "Epoch [5/5], Step [470/3267], Loss: 1.1358071565628052\n",
            "Epoch [5/5], Step [480/3267], Loss: 0.9499536156654358\n",
            "Epoch [5/5], Step [490/3267], Loss: 0.927679181098938\n",
            "Epoch [5/5], Step [500/3267], Loss: 1.1686326265335083\n",
            "Epoch [5/5], Step [510/3267], Loss: 0.8110731244087219\n",
            "Epoch [5/5], Step [520/3267], Loss: 0.8377856612205505\n",
            "Epoch [5/5], Step [530/3267], Loss: 0.8233546614646912\n",
            "Epoch [5/5], Step [540/3267], Loss: 0.9175180196762085\n",
            "Epoch [5/5], Step [550/3267], Loss: 1.1143072843551636\n",
            "Epoch [5/5], Step [560/3267], Loss: 0.8411737084388733\n",
            "Epoch [5/5], Step [570/3267], Loss: 0.920902669429779\n",
            "Epoch [5/5], Step [580/3267], Loss: 0.9377774000167847\n",
            "Epoch [5/5], Step [590/3267], Loss: 1.1026533842086792\n",
            "Epoch [5/5], Step [600/3267], Loss: 0.9710721969604492\n",
            "Epoch [5/5], Step [610/3267], Loss: 1.0351948738098145\n",
            "Epoch [5/5], Step [620/3267], Loss: 0.9944326877593994\n",
            "Epoch [5/5], Step [630/3267], Loss: 0.9142252206802368\n",
            "Epoch [5/5], Step [640/3267], Loss: 1.106013298034668\n",
            "Epoch [5/5], Step [650/3267], Loss: 1.0836907625198364\n",
            "Epoch [5/5], Step [660/3267], Loss: 1.0194154977798462\n",
            "Epoch [5/5], Step [670/3267], Loss: 1.1027132272720337\n",
            "Epoch [5/5], Step [680/3267], Loss: 1.046827793121338\n",
            "Epoch [5/5], Step [690/3267], Loss: 0.8936336040496826\n",
            "Epoch [5/5], Step [700/3267], Loss: 0.9496660232543945\n",
            "Epoch [5/5], Step [710/3267], Loss: 1.108546495437622\n",
            "Epoch [5/5], Step [720/3267], Loss: 1.0538746118545532\n",
            "Epoch [5/5], Step [730/3267], Loss: 0.855189859867096\n",
            "Epoch [5/5], Step [740/3267], Loss: 0.9721976518630981\n",
            "Epoch [5/5], Step [750/3267], Loss: 1.0050373077392578\n",
            "Epoch [5/5], Step [760/3267], Loss: 0.8281517624855042\n",
            "Epoch [5/5], Step [770/3267], Loss: 0.9777606725692749\n",
            "Epoch [5/5], Step [780/3267], Loss: 0.9526699781417847\n",
            "Epoch [5/5], Step [790/3267], Loss: 0.8392120599746704\n",
            "Epoch [5/5], Step [800/3267], Loss: 0.8463743925094604\n",
            "Epoch [5/5], Step [810/3267], Loss: 0.9307674765586853\n",
            "Epoch [5/5], Step [820/3267], Loss: 0.907805323600769\n",
            "Epoch [5/5], Step [830/3267], Loss: 0.8701905608177185\n",
            "Epoch [5/5], Step [840/3267], Loss: 1.0054811239242554\n",
            "Epoch [5/5], Step [850/3267], Loss: 0.8501102328300476\n",
            "Epoch [5/5], Step [860/3267], Loss: 0.8613832592964172\n",
            "Epoch [5/5], Step [870/3267], Loss: 0.9583280086517334\n",
            "Epoch [5/5], Step [880/3267], Loss: 0.8452792763710022\n",
            "Epoch [5/5], Step [890/3267], Loss: 0.9437420964241028\n",
            "Epoch [5/5], Step [900/3267], Loss: 1.0202817916870117\n",
            "Epoch [5/5], Step [910/3267], Loss: 0.8737893104553223\n",
            "Epoch [5/5], Step [920/3267], Loss: 0.9337738156318665\n",
            "Epoch [5/5], Step [930/3267], Loss: 1.1223827600479126\n",
            "Epoch [5/5], Step [940/3267], Loss: 0.9365925788879395\n",
            "Epoch [5/5], Step [950/3267], Loss: 1.1441608667373657\n",
            "Epoch [5/5], Step [960/3267], Loss: 0.9661923050880432\n",
            "Epoch [5/5], Step [970/3267], Loss: 1.0612982511520386\n",
            "Epoch [5/5], Step [980/3267], Loss: 0.7889087796211243\n",
            "Epoch [5/5], Step [990/3267], Loss: 0.7165469527244568\n",
            "Epoch [5/5], Step [1000/3267], Loss: 1.131083369255066\n",
            "Epoch [5/5], Step [1010/3267], Loss: 0.8728206753730774\n",
            "Epoch [5/5], Step [1020/3267], Loss: 0.9428319334983826\n",
            "Epoch [5/5], Step [1030/3267], Loss: 0.7794795632362366\n",
            "Epoch [5/5], Step [1040/3267], Loss: 0.9610015153884888\n",
            "Epoch [5/5], Step [1050/3267], Loss: 0.8261445164680481\n",
            "Epoch [5/5], Step [1060/3267], Loss: 0.9119424223899841\n",
            "Epoch [5/5], Step [1070/3267], Loss: 1.050264835357666\n",
            "Epoch [5/5], Step [1080/3267], Loss: 0.8755383491516113\n",
            "Epoch [5/5], Step [1090/3267], Loss: 0.9774036407470703\n",
            "Epoch [5/5], Step [1100/3267], Loss: 0.9157044291496277\n",
            "Epoch [5/5], Step [1110/3267], Loss: 0.9319056272506714\n",
            "Epoch [5/5], Step [1120/3267], Loss: 1.0644110441207886\n",
            "Epoch [5/5], Step [1130/3267], Loss: 0.8154006004333496\n",
            "Epoch [5/5], Step [1140/3267], Loss: 0.8971399068832397\n",
            "Epoch [5/5], Step [1150/3267], Loss: 0.9800729751586914\n",
            "Epoch [5/5], Step [1160/3267], Loss: 0.9773976802825928\n",
            "Epoch [5/5], Step [1170/3267], Loss: 0.9406125545501709\n",
            "Epoch [5/5], Step [1180/3267], Loss: 1.2368443012237549\n",
            "Epoch [5/5], Step [1190/3267], Loss: 0.9266179800033569\n",
            "Epoch [5/5], Step [1200/3267], Loss: 0.8509694933891296\n",
            "Epoch [5/5], Step [1210/3267], Loss: 0.9167417287826538\n",
            "Epoch [5/5], Step [1220/3267], Loss: 1.0186598300933838\n",
            "Epoch [5/5], Step [1230/3267], Loss: 0.9201520681381226\n",
            "Epoch [5/5], Step [1240/3267], Loss: 0.8927332758903503\n",
            "Epoch [5/5], Step [1250/3267], Loss: 0.9304441213607788\n",
            "Epoch [5/5], Step [1260/3267], Loss: 1.0478836297988892\n",
            "Epoch [5/5], Step [1270/3267], Loss: 0.7261874079704285\n",
            "Epoch [5/5], Step [1280/3267], Loss: 0.8809013962745667\n",
            "Epoch [5/5], Step [1290/3267], Loss: 1.0350215435028076\n",
            "Epoch [5/5], Step [1300/3267], Loss: 0.8917779922485352\n",
            "Epoch [5/5], Step [1310/3267], Loss: 0.8259275555610657\n",
            "Epoch [5/5], Step [1320/3267], Loss: 0.9474651217460632\n",
            "Epoch [5/5], Step [1330/3267], Loss: 0.9058732390403748\n",
            "Epoch [5/5], Step [1340/3267], Loss: 0.9914417862892151\n",
            "Epoch [5/5], Step [1350/3267], Loss: 0.9746549129486084\n",
            "Epoch [5/5], Step [1360/3267], Loss: 0.8222001791000366\n",
            "Epoch [5/5], Step [1370/3267], Loss: 0.789544939994812\n",
            "Epoch [5/5], Step [1380/3267], Loss: 1.0563231706619263\n",
            "Epoch [5/5], Step [1390/3267], Loss: 1.1581734418869019\n",
            "Epoch [5/5], Step [1400/3267], Loss: 0.8692453503608704\n",
            "Epoch [5/5], Step [1410/3267], Loss: 0.985926628112793\n",
            "Epoch [5/5], Step [1420/3267], Loss: 0.8817815184593201\n",
            "Epoch [5/5], Step [1430/3267], Loss: 0.8181619644165039\n",
            "Epoch [5/5], Step [1440/3267], Loss: 0.8809854388237\n",
            "Epoch [5/5], Step [1450/3267], Loss: 1.16524338722229\n",
            "Epoch [5/5], Step [1460/3267], Loss: 0.9946774244308472\n",
            "Epoch [5/5], Step [1470/3267], Loss: 0.8116674423217773\n",
            "Epoch [5/5], Step [1480/3267], Loss: 0.8445698618888855\n",
            "Epoch [5/5], Step [1490/3267], Loss: 0.7747480869293213\n",
            "Epoch [5/5], Step [1500/3267], Loss: 0.7809553742408752\n",
            "Epoch [5/5], Step [1510/3267], Loss: 0.9079610109329224\n",
            "Epoch [5/5], Step [1520/3267], Loss: 0.7976728081703186\n",
            "Epoch [5/5], Step [1530/3267], Loss: 0.9240302443504333\n",
            "Epoch [5/5], Step [1540/3267], Loss: 0.8060311079025269\n",
            "Epoch [5/5], Step [1550/3267], Loss: 0.7915841341018677\n",
            "Epoch [5/5], Step [1560/3267], Loss: 0.9453790187835693\n",
            "Epoch [5/5], Step [1570/3267], Loss: 0.8454316258430481\n",
            "Epoch [5/5], Step [1580/3267], Loss: 0.8532248139381409\n",
            "Epoch [5/5], Step [1590/3267], Loss: 0.7831066846847534\n",
            "Epoch [5/5], Step [1600/3267], Loss: 0.887688934803009\n",
            "Epoch [5/5], Step [1610/3267], Loss: 0.8167943358421326\n",
            "Epoch [5/5], Step [1620/3267], Loss: 0.9828439354896545\n",
            "Epoch [5/5], Step [1630/3267], Loss: 0.8314971923828125\n",
            "Epoch [5/5], Step [1640/3267], Loss: 0.837102472782135\n",
            "Epoch [5/5], Step [1650/3267], Loss: 1.047216773033142\n",
            "Epoch [5/5], Step [1660/3267], Loss: 0.8143408894538879\n",
            "Epoch [5/5], Step [1670/3267], Loss: 1.0778001546859741\n",
            "Epoch [5/5], Step [1680/3267], Loss: 0.8112322092056274\n",
            "Epoch [5/5], Step [1690/3267], Loss: 0.9813874363899231\n",
            "Epoch [5/5], Step [1700/3267], Loss: 0.8313868045806885\n",
            "Epoch [5/5], Step [1710/3267], Loss: 1.0145212411880493\n",
            "Epoch [5/5], Step [1720/3267], Loss: 0.7476476430892944\n",
            "Epoch [5/5], Step [1730/3267], Loss: 0.8095101714134216\n",
            "Epoch [5/5], Step [1740/3267], Loss: 0.8353783488273621\n",
            "Epoch [5/5], Step [1750/3267], Loss: 0.7289382219314575\n",
            "Epoch [5/5], Step [1760/3267], Loss: 1.2480554580688477\n",
            "Epoch [5/5], Step [1770/3267], Loss: 0.8001681566238403\n",
            "Epoch [5/5], Step [1780/3267], Loss: 1.0310661792755127\n",
            "Epoch [5/5], Step [1790/3267], Loss: 0.894488513469696\n",
            "Epoch [5/5], Step [1800/3267], Loss: 0.9322630763053894\n",
            "Epoch [5/5], Step [1810/3267], Loss: 0.8129016160964966\n",
            "Epoch [5/5], Step [1820/3267], Loss: 0.8365719318389893\n",
            "Epoch [5/5], Step [1830/3267], Loss: 0.9146596789360046\n",
            "Epoch [5/5], Step [1840/3267], Loss: 0.9389545917510986\n",
            "Epoch [5/5], Step [1850/3267], Loss: 1.1358330249786377\n",
            "Epoch [5/5], Step [1860/3267], Loss: 0.9054326415061951\n",
            "Epoch [5/5], Step [1870/3267], Loss: 0.9900248646736145\n",
            "Epoch [5/5], Step [1880/3267], Loss: 0.9995719194412231\n",
            "Epoch [5/5], Step [1890/3267], Loss: 1.0150541067123413\n",
            "Epoch [5/5], Step [1900/3267], Loss: 1.017024278640747\n",
            "Epoch [5/5], Step [1910/3267], Loss: 0.885600209236145\n",
            "Epoch [5/5], Step [1920/3267], Loss: 0.766106128692627\n",
            "Epoch [5/5], Step [1930/3267], Loss: 1.0401999950408936\n",
            "Epoch [5/5], Step [1940/3267], Loss: 0.8645581603050232\n",
            "Epoch [5/5], Step [1950/3267], Loss: 1.0073353052139282\n",
            "Epoch [5/5], Step [1960/3267], Loss: 0.8396485447883606\n",
            "Epoch [5/5], Step [1970/3267], Loss: 0.943898618221283\n",
            "Epoch [5/5], Step [1980/3267], Loss: 0.8903064131736755\n",
            "Epoch [5/5], Step [1990/3267], Loss: 1.035121202468872\n",
            "Epoch [5/5], Step [2000/3267], Loss: 1.1270978450775146\n",
            "Epoch [5/5], Step [2010/3267], Loss: 0.8594832420349121\n",
            "Epoch [5/5], Step [2020/3267], Loss: 1.1388245820999146\n",
            "Epoch [5/5], Step [2030/3267], Loss: 0.9478921294212341\n",
            "Epoch [5/5], Step [2040/3267], Loss: 1.0644253492355347\n",
            "Epoch [5/5], Step [2050/3267], Loss: 0.8564808964729309\n",
            "Epoch [5/5], Step [2060/3267], Loss: 0.7316985726356506\n",
            "Epoch [5/5], Step [2070/3267], Loss: 0.8450878262519836\n",
            "Epoch [5/5], Step [2080/3267], Loss: 0.7919183969497681\n",
            "Epoch [5/5], Step [2090/3267], Loss: 1.042457103729248\n",
            "Epoch [5/5], Step [2100/3267], Loss: 0.876399040222168\n",
            "Epoch [5/5], Step [2110/3267], Loss: 1.0070358514785767\n",
            "Epoch [5/5], Step [2120/3267], Loss: 1.0505472421646118\n",
            "Epoch [5/5], Step [2130/3267], Loss: 0.8432920575141907\n",
            "Epoch [5/5], Step [2140/3267], Loss: 1.1030259132385254\n",
            "Epoch [5/5], Step [2150/3267], Loss: 0.90823894739151\n",
            "Epoch [5/5], Step [2160/3267], Loss: 0.947968065738678\n",
            "Epoch [5/5], Step [2170/3267], Loss: 0.8415994644165039\n",
            "Epoch [5/5], Step [2180/3267], Loss: 0.7235292792320251\n",
            "Epoch [5/5], Step [2190/3267], Loss: 0.9479247331619263\n",
            "Epoch [5/5], Step [2200/3267], Loss: 0.734775722026825\n",
            "Epoch [5/5], Step [2210/3267], Loss: 0.967428982257843\n",
            "Epoch [5/5], Step [2220/3267], Loss: 0.9802953004837036\n",
            "Epoch [5/5], Step [2230/3267], Loss: 0.7649365663528442\n",
            "Epoch [5/5], Step [2240/3267], Loss: 0.7672638297080994\n",
            "Epoch [5/5], Step [2250/3267], Loss: 0.7545564770698547\n",
            "Epoch [5/5], Step [2260/3267], Loss: 0.6379328370094299\n",
            "Epoch [5/5], Step [2270/3267], Loss: 0.9050493240356445\n",
            "Epoch [5/5], Step [2280/3267], Loss: 0.9224259853363037\n",
            "Epoch [5/5], Step [2290/3267], Loss: 0.8112742900848389\n",
            "Epoch [5/5], Step [2300/3267], Loss: 0.799924373626709\n",
            "Epoch [5/5], Step [2310/3267], Loss: 0.996626615524292\n",
            "Epoch [5/5], Step [2320/3267], Loss: 0.9020817279815674\n",
            "Epoch [5/5], Step [2330/3267], Loss: 0.8929982781410217\n",
            "Epoch [5/5], Step [2340/3267], Loss: 0.9601364135742188\n",
            "Epoch [5/5], Step [2350/3267], Loss: 0.8913633823394775\n",
            "Epoch [5/5], Step [2360/3267], Loss: 0.9046422839164734\n",
            "Epoch [5/5], Step [2370/3267], Loss: 1.0010592937469482\n",
            "Epoch [5/5], Step [2380/3267], Loss: 0.8593143224716187\n",
            "Epoch [5/5], Step [2390/3267], Loss: 0.8211814165115356\n",
            "Epoch [5/5], Step [2400/3267], Loss: 0.8413069248199463\n",
            "Epoch [5/5], Step [2410/3267], Loss: 0.8006950616836548\n",
            "Epoch [5/5], Step [2420/3267], Loss: 0.840872049331665\n",
            "Epoch [5/5], Step [2430/3267], Loss: 0.8484416604042053\n",
            "Epoch [5/5], Step [2440/3267], Loss: 0.8464177250862122\n",
            "Epoch [5/5], Step [2450/3267], Loss: 1.0179206132888794\n",
            "Epoch [5/5], Step [2460/3267], Loss: 1.003149390220642\n",
            "Epoch [5/5], Step [2470/3267], Loss: 0.8519330620765686\n",
            "Epoch [5/5], Step [2480/3267], Loss: 0.9844508171081543\n",
            "Epoch [5/5], Step [2490/3267], Loss: 1.0951863527297974\n",
            "Epoch [5/5], Step [2500/3267], Loss: 0.86870276927948\n",
            "Epoch [5/5], Step [2510/3267], Loss: 1.0676045417785645\n",
            "Epoch [5/5], Step [2520/3267], Loss: 0.8225234150886536\n",
            "Epoch [5/5], Step [2530/3267], Loss: 0.9292166829109192\n",
            "Epoch [5/5], Step [2540/3267], Loss: 0.8344212770462036\n",
            "Epoch [5/5], Step [2550/3267], Loss: 0.9632940888404846\n",
            "Epoch [5/5], Step [2560/3267], Loss: 0.8481906056404114\n",
            "Epoch [5/5], Step [2570/3267], Loss: 0.9525577425956726\n",
            "Epoch [5/5], Step [2580/3267], Loss: 0.996253490447998\n",
            "Epoch [5/5], Step [2590/3267], Loss: 1.0005972385406494\n",
            "Epoch [5/5], Step [2600/3267], Loss: 1.0195232629776\n",
            "Epoch [5/5], Step [2610/3267], Loss: 0.9106351137161255\n",
            "Epoch [5/5], Step [2620/3267], Loss: 0.7936620712280273\n",
            "Epoch [5/5], Step [2630/3267], Loss: 0.8244943618774414\n",
            "Epoch [5/5], Step [2640/3267], Loss: 0.8081181645393372\n",
            "Epoch [5/5], Step [2650/3267], Loss: 0.987677812576294\n",
            "Epoch [5/5], Step [2660/3267], Loss: 0.9004294872283936\n",
            "Epoch [5/5], Step [2670/3267], Loss: 1.0243992805480957\n",
            "Epoch [5/5], Step [2680/3267], Loss: 0.9309545755386353\n",
            "Epoch [5/5], Step [2690/3267], Loss: 0.889452338218689\n",
            "Epoch [5/5], Step [2700/3267], Loss: 0.8639542460441589\n",
            "Epoch [5/5], Step [2710/3267], Loss: 0.8080641627311707\n",
            "Epoch [5/5], Step [2720/3267], Loss: 0.9896409511566162\n",
            "Epoch [5/5], Step [2730/3267], Loss: 0.7776533365249634\n",
            "Epoch [5/5], Step [2740/3267], Loss: 1.059187889099121\n",
            "Epoch [5/5], Step [2750/3267], Loss: 0.7735772728919983\n",
            "Epoch [5/5], Step [2760/3267], Loss: 0.967425525188446\n",
            "Epoch [5/5], Step [2770/3267], Loss: 1.0101410150527954\n",
            "Epoch [5/5], Step [2780/3267], Loss: 0.8068950772285461\n",
            "Epoch [5/5], Step [2790/3267], Loss: 0.8844788670539856\n",
            "Epoch [5/5], Step [2800/3267], Loss: 0.7948530316352844\n",
            "Epoch [5/5], Step [2810/3267], Loss: 0.9309393167495728\n",
            "Epoch [5/5], Step [2820/3267], Loss: 1.016202688217163\n",
            "Epoch [5/5], Step [2830/3267], Loss: 0.7996891736984253\n",
            "Epoch [5/5], Step [2840/3267], Loss: 0.8428029417991638\n",
            "Epoch [5/5], Step [2850/3267], Loss: 1.0773526430130005\n",
            "Epoch [5/5], Step [2860/3267], Loss: 0.8986710906028748\n",
            "Epoch [5/5], Step [2870/3267], Loss: 0.9114054441452026\n",
            "Epoch [5/5], Step [2880/3267], Loss: 0.9373853206634521\n",
            "Epoch [5/5], Step [2890/3267], Loss: 0.9225097298622131\n",
            "Epoch [5/5], Step [2900/3267], Loss: 0.9694356322288513\n",
            "Epoch [5/5], Step [2910/3267], Loss: 0.851618230342865\n",
            "Epoch [5/5], Step [2920/3267], Loss: 1.0934404134750366\n",
            "Epoch [5/5], Step [2930/3267], Loss: 0.8612985610961914\n",
            "Epoch [5/5], Step [2940/3267], Loss: 1.0441406965255737\n",
            "Epoch [5/5], Step [2950/3267], Loss: 1.0129743814468384\n",
            "Epoch [5/5], Step [2960/3267], Loss: 1.0611050128936768\n",
            "Epoch [5/5], Step [2970/3267], Loss: 1.122989296913147\n",
            "Epoch [5/5], Step [2980/3267], Loss: 1.0300637483596802\n",
            "Epoch [5/5], Step [2990/3267], Loss: 0.8124590516090393\n",
            "Epoch [5/5], Step [3000/3267], Loss: 0.9300950765609741\n",
            "Epoch [5/5], Step [3010/3267], Loss: 0.9258705377578735\n",
            "Epoch [5/5], Step [3020/3267], Loss: 0.8766489624977112\n",
            "Epoch [5/5], Step [3030/3267], Loss: 0.8606370687484741\n",
            "Epoch [5/5], Step [3040/3267], Loss: 0.9820808172225952\n",
            "Epoch [5/5], Step [3050/3267], Loss: 0.9494724273681641\n",
            "Epoch [5/5], Step [3060/3267], Loss: 0.9729741811752319\n",
            "Epoch [5/5], Step [3070/3267], Loss: 0.9577845335006714\n",
            "Epoch [5/5], Step [3080/3267], Loss: 0.698628306388855\n",
            "Epoch [5/5], Step [3090/3267], Loss: 0.8580714464187622\n",
            "Epoch [5/5], Step [3100/3267], Loss: 1.057194709777832\n",
            "Epoch [5/5], Step [3110/3267], Loss: 0.9332462549209595\n",
            "Epoch [5/5], Step [3120/3267], Loss: 0.9749529361724854\n",
            "Epoch [5/5], Step [3130/3267], Loss: 0.8344898819923401\n",
            "Epoch [5/5], Step [3140/3267], Loss: 0.7820062041282654\n",
            "Epoch [5/5], Step [3150/3267], Loss: 0.885156512260437\n",
            "Epoch [5/5], Step [3160/3267], Loss: 0.8374587297439575\n",
            "Epoch [5/5], Step [3170/3267], Loss: 0.9514289498329163\n",
            "Epoch [5/5], Step [3180/3267], Loss: 0.9294134974479675\n",
            "Epoch [5/5], Step [3190/3267], Loss: 0.87320476770401\n",
            "Epoch [5/5], Step [3200/3267], Loss: 0.9461694955825806\n",
            "Epoch [5/5], Step [3210/3267], Loss: 0.9308412671089172\n",
            "Epoch [5/5], Step [3220/3267], Loss: 0.8928632140159607\n",
            "Epoch [5/5], Step [3230/3267], Loss: 0.9065402150154114\n",
            "Epoch [5/5], Step [3240/3267], Loss: 0.9385345578193665\n",
            "Epoch [5/5], Step [3250/3267], Loss: 0.9885078072547913\n",
            "Epoch [5/5], Step [3260/3267], Loss: 1.0530321598052979\n",
            "Training finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "loaded_model = RNNModel(input_size, hidden_size, output_size)\n",
        "loaded_model.load_state_dict(torch.load('seq2seq_rnn_model.pth'))\n",
        "loaded_model.eval()  # Set the model to evaluation mode\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-eJQxa7hD6P",
        "outputId": "01906701-8bdf-4786-b3b1-b3cfece045b8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNNModel(\n",
              "  (embedding): Embedding(28996, 512)\n",
              "  (lstm): LSTM(512, 512, batch_first=True)\n",
              "  (fc): Linear(in_features=512, out_features=28996, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_unseen_data(unseen_data, tokenizer):\n",
        "    inputs = tokenizer(unseen_data, return_tensors='pt', max_length=64, truncation=True, padding=\"max_length\")\n",
        "    return inputs\n",
        "\n",
        "def predict(model, input_ids):\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids)\n",
        "    return outputs\n",
        "\n",
        "def decode_predictions(outputs, tokenizer):\n",
        "    predicted_ids = torch.argmax(outputs, dim=-1)\n",
        "    predicted_text = tokenizer.batch_decode(predicted_ids, skip_special_tokens=True)\n",
        "    return predicted_text\n"
      ],
      "metadata": {
        "id": "RU07oV3nmPjS"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unseen_data = [\"Here's the data that needs to be checked!!!\"]\n",
        "\n",
        "# Preprocess unseen data\n",
        "unseen_inputs = preprocess_unseen_data(unseen_data, tokenizer)\n",
        "\n",
        "# Make predictions\n",
        "predictions = predict(loaded_model, unseen_inputs['input_ids'])\n",
        "\n",
        "# Decode predictions\n",
        "decoded_predictions = decode_predictions(predictions, tokenizer)\n",
        "\n",
        "# Print the results\n",
        "for input_text, output_text in zip(unseen_data, decoded_predictions):\n",
        "    print(f'Input: {input_text}')\n",
        "    print(f'Predicted Output: {output_text}\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_afZRcmnH-Y",
        "outputId": "38bc12be-64c9-4949-e43d-a7f689723025"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: Here's the data that needs to be checked!!!\n",
            "Predicted Output: Here is s the the that to to be be\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN Try 3 with encoder decoder arch"
      ],
      "metadata": {
        "id": "0J4oN5xzv76y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class Encoder(nn.Module):\n",
        "#     def __init__(self, input_size, embedding_size, hidden_size):\n",
        "#         super(Encoder, self).__init__()\n",
        "#         self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "#         self.rnn = nn.GRU(embedding_size, hidden_size, batch_first=True)\n",
        "\n",
        "#     def forward(self, input_seq):\n",
        "#         embedded = self.embedding(input_seq)\n",
        "#         outputs, hidden = self.rnn(embedded)\n",
        "#         return hidden\n",
        "\n",
        "# class Decoder(nn.Module):\n",
        "#     def __init__(self, output_size, embedding_size, hidden_size):\n",
        "#         super(Decoder, self).__init__()\n",
        "#         self.embedding = nn.Embedding(output_size, embedding_size)\n",
        "#         self.rnn = nn.GRU(embedding_size, hidden_size, batch_first=True)\n",
        "#         self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "#     def forward(self, input_token, hidden):\n",
        "#         embedded = self.embedding(input_token.unsqueeze(1))\n",
        "#         output, hidden = self.rnn(embedded, hidden)\n",
        "#         output = self.fc(output.squeeze(1))\n",
        "#         return output, hidden\n",
        "\n",
        "# class Seq2Seq(nn.Module):\n",
        "#     def __init__(self, encoder, decoder):\n",
        "#         super(Seq2Seq, self).__init__()\n",
        "#         self.encoder = encoder\n",
        "#         self.decoder = decoder\n",
        "\n",
        "#     def forward(self, src_seq, trg_seq):\n",
        "#         batch_size = src_seq.shape[0]\n",
        "#         trg_len = trg_seq.shape[1]\n",
        "#         trg_vocab_size = self.decoder.fc.out_features\n",
        "\n",
        "#         outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(src_seq.device)\n",
        "\n",
        "#         hidden = self.encoder(src_seq)\n",
        "\n",
        "#         input_token = trg_seq[:, 0]\n",
        "\n",
        "#         for t in range(1, trg_len):\n",
        "#             output, hidden = self.decoder(input_token, hidden)\n",
        "#             outputs[:, t] = output\n",
        "#             input_token = output.argmax(1)\n",
        "\n",
        "#         return outputs"
      ],
      "metadata": {
        "id": "ztCQjaoAnRLx"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Loading preprocessed data in train_dataset\n",
        "# input_ids = torch.tensor(train_dataset['input_ids']).to(device)\n",
        "# labels_0 = torch.tensor(train_dataset['labels']).to(device)\n",
        "\n",
        "# # Define your model\n",
        "# input_size = len(tokenizer.get_vocab())\n",
        "# hidden_size = 512\n",
        "# output_size = len(tokenizer.get_vocab())\n",
        "\n",
        "\n",
        "# encoder = Encoder(input_size, embedding_size, hidden_size)\n",
        "# decoder = Decoder(output_size, embedding_size, hidden_size)\n",
        "\n",
        "# model = Seq2Seq(encoder, decoder)\n",
        "\n",
        "# # Define your loss function and optimizer\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# # Train the model\n",
        "# num_epochs = 5\n",
        "# batch_size = 32\n",
        "# model.to(device)\n",
        "\n",
        "# for epoch in range(num_epochs):\n",
        "#     for i in range(0, len(input_ids), batch_size):\n",
        "#         inputs = input_ids[i:i+batch_size]\n",
        "#         targets = labels_0[i:i+batch_size]\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         outputs = model(inputs, targets)\n",
        "#         output = output.view(-1, output.shape[-1])\n",
        "#         targets = targets.view(-1)\n",
        "\n",
        "#         loss = criterion(outputs.view(-1, output_size), targets.view(-1))\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         if (i // batch_size) % 10 == 0:\n",
        "#             print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i // batch_size}/{len(input_ids) // batch_size}], Loss: {loss.item()}')\n",
        "\n",
        "# # Save the trained model\n",
        "# torch.save(model.state_dict(), \"seq2seq_rnn_model.pth\")\n",
        "# print('Training finished!')"
      ],
      "metadata": {
        "id": "rgaIaEn4sQwj"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XO10aFibuHcV"
      },
      "execution_count": 23,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}