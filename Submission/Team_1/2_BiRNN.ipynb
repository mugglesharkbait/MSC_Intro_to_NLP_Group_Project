{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RmdXVBAWGpsM"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-02-10T19:32:15.331967Z",
     "iopub.status.busy": "2024-02-10T19:32:15.331694Z",
     "iopub.status.idle": "2024-02-10T19:32:15.334936Z",
     "shell.execute_reply": "2024-02-10T19:32:15.334309Z",
     "shell.execute_reply.started": "2024-02-10T19:32:15.331939Z"
    },
    "id": "OeY28k58DG8T",
    "outputId": "289ac94e-d2a8-4260-8048-dbadc5637fb3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-02-10T19:32:15.336554Z",
     "iopub.status.busy": "2024-02-10T19:32:15.336264Z",
     "iopub.status.idle": "2024-02-10T19:32:15.342005Z",
     "shell.execute_reply": "2024-02-10T19:32:15.341193Z",
     "shell.execute_reply.started": "2024-02-10T19:32:15.336532Z"
    },
    "id": "L-kDQGv4DMWY",
    "outputId": "67d3b175-cb24-446c-cd44-839e76601de7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %cd /content/drive/MyDrive/MSC_Intro_to_NLP_Group_Project/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-02-10T19:32:15.343135Z",
     "iopub.status.busy": "2024-02-10T19:32:15.342782Z",
     "iopub.status.idle": "2024-02-10T19:33:05.736816Z",
     "shell.execute_reply": "2024-02-10T19:33:05.736066Z",
     "shell.execute_reply.started": "2024-02-10T19:32:15.343112Z"
    },
    "id": "Jf8V8EEnGpsO",
    "outputId": "df02b335-c881-46f9-f2dd-9ddbfb1f8346",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: click in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from nltk) (4.66.1)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.8.1\n",
      "Collecting gensim\n",
      "  Using cached gensim-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from gensim) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from gensim) (5.2.1)\n",
      "Using cached gensim-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n",
      "Installing collected packages: gensim\n",
      "Successfully installed gensim-4.3.2\n",
      "Collecting transformers[torch]\n",
      "  Using cached transformers-4.37.2-py3-none-any.whl.metadata (129 kB)\n",
      "Requirement already satisfied: filelock in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from transformers[torch]) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers[torch])\n",
      "  Using cached huggingface_hub-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from transformers[torch]) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from transformers[torch]) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from transformers[torch]) (2023.12.25)\n",
      "Requirement already satisfied: requests in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from transformers[torch]) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers[torch])\n",
      "  Using cached tokenizers-0.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers[torch])\n",
      "  Using cached safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from transformers[torch]) (4.66.1)\n",
      "Requirement already satisfied: torch!=1.12.0,>=1.11 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from transformers[torch]) (2.1.0)\n",
      "Collecting accelerate>=0.21.0 (from transformers[torch])\n",
      "  Using cached accelerate-0.27.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: psutil in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (3.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from requests->transformers[torch]) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from requests->transformers[torch]) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from requests->transformers[torch]) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from requests->transformers[torch]) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from jinja2->torch!=1.12.0,>=1.11->transformers[torch]) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from sympy->torch!=1.12.0,>=1.11->transformers[torch]) (1.3.0)\n",
      "Using cached accelerate-0.27.0-py3-none-any.whl (279 kB)\n",
      "Using cached huggingface_hub-0.20.3-py3-none-any.whl (330 kB)\n",
      "Using cached safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Using cached tokenizers-0.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Using cached transformers-4.37.2-py3-none-any.whl (8.4 MB)\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, accelerate, transformers\n",
      "Successfully installed accelerate-0.27.0 huggingface-hub-0.20.3 safetensors-0.4.2 tokenizers-0.15.1 transformers-4.37.2\n",
      "Collecting datasets\n",
      "  Using cached datasets-2.17.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from datasets) (1.26.3)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from datasets) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from datasets) (4.66.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets)\n",
      "  Using cached fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: aiohttp in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from datasets) (0.20.3)\n",
      "Requirement already satisfied: packaging in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Using cached datasets-2.17.0-py3-none-any.whl (536 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "Using cached multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Using cached xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Installing collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.12.2\n",
      "    Uninstalling fsspec-2023.12.2:\n",
      "      Successfully uninstalled fsspec-2023.12.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "s3fs 2023.12.2 requires fsspec==2023.12.2, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-2.17.0 dill-0.3.8 fsspec-2023.10.0 multiprocess-0.70.16 xxhash-3.4.1\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.15.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from tensorflow) (2.1.0)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow)\n",
      "  Using cached flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Using cached gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting h5py>=2.9.0 (from tensorflow)\n",
      "  Using cached h5py-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Using cached libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting ml-dtypes~=0.2.0 (from tensorflow)\n",
      "  Using cached ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from tensorflow) (1.26.3)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: packaging in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from tensorflow) (4.24.4)\n",
      "Requirement already satisfied: setuptools in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from tensorflow) (69.0.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Using cached termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from tensorflow) (4.9.0)\n",
      "Collecting wrapt<1.15,>=1.11.0 (from tensorflow)\n",
      "  Using cached wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.36.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from tensorflow) (1.59.3)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from tensorflow) (2.15.1)\n",
      "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow)\n",
      "  Using cached tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.16,>=2.15.0 (from tensorflow)\n",
      "  Using cached keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.4)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
      "Using cached tensorflow-2.15.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
      "Using cached flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Using cached h5py-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "Using cached keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "Using cached libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "Using cached ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "Using cached tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
      "Using cached tensorflow_io_gcs_filesystem-0.36.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "Using cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, opt-einsum, ml-dtypes, keras, h5py, google-pasta, gast, astunparse, tensorflow\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.16.0\n",
      "    Uninstalling wrapt-1.16.0:\n",
      "      Successfully uninstalled wrapt-1.16.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "s3fs 2023.12.2 requires fsspec==2023.12.2, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed astunparse-1.6.3 flatbuffers-23.5.26 gast-0.5.4 google-pasta-0.2.0 h5py-3.10.0 keras-2.15.0 libclang-16.0.6 ml-dtypes-0.2.0 opt-einsum-3.3.0 tensorflow-2.15.0.post1 tensorflow-estimator-2.15.0 tensorflow-io-gcs-filesystem-0.36.0 termcolor-2.4.0 wrapt-1.14.1\n",
      "Requirement already satisfied: torch in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (2.1.0)\n",
      "Requirement already satisfied: filelock in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from jinja2->torch) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Collecting contractions\n",
      "  Using cached contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting textsearch>=0.0.21 (from contractions)\n",
      "  Using cached textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
      "  Using cached anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
      "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
      "  Using cached pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
      "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install gensim\n",
    "!pip install 'transformers[torch]'\n",
    "!pip install datasets\n",
    "!pip install tensorflow\n",
    "!pip install torch\n",
    "!pip install contractions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-02-10T19:33:05.738333Z",
     "iopub.status.busy": "2024-02-10T19:33:05.737992Z",
     "iopub.status.idle": "2024-02-10T19:33:22.732499Z",
     "shell.execute_reply": "2024-02-10T19:33:22.731660Z",
     "shell.execute_reply.started": "2024-02-10T19:33:05.738305Z"
    },
    "id": "dsTI9k5EGpsP",
    "outputId": "29a59ae8-d5d8-440c-c69b-dc5c13cf7594"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m104.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.3)\n",
      "Requirement already satisfied: jinja2 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (69.0.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.14.6)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.11.17)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.10)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.4)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk import word_tokenize, pos_tag\n",
    "import contractions\n",
    "import pandas as pd\n",
    "import spacy\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-02-10T19:33:22.734790Z",
     "iopub.status.busy": "2024-02-10T19:33:22.733761Z",
     "iopub.status.idle": "2024-02-10T19:33:22.745618Z",
     "shell.execute_reply": "2024-02-10T19:33:22.744810Z",
     "shell.execute_reply.started": "2024-02-10T19:33:22.734761Z"
    },
    "id": "QS55Y89QZpgm",
    "outputId": "5e6b340b-1481-4d2a-8baf-5d5eb2f74906"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Tesla T4\n"
     ]
    }
   ],
   "source": [
    "# Using the GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "gpu_name = torch.cuda.get_device_name(device)\n",
    "print(gpu_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T19:33:22.747040Z",
     "iopub.status.busy": "2024-02-10T19:33:22.746801Z",
     "iopub.status.idle": "2024-02-10T19:33:23.308514Z",
     "shell.execute_reply": "2024-02-10T19:33:23.307852Z",
     "shell.execute_reply.started": "2024-02-10T19:33:22.747018Z"
    },
    "id": "dAoiwoKfZpgm"
   },
   "outputs": [],
   "source": [
    "# Load spaCy model outside of the function to avoid reloading it each time the function is called\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCypnUdGFJDB"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T19:33:23.311294Z",
     "iopub.status.busy": "2024-02-10T19:33:23.310888Z",
     "iopub.status.idle": "2024-02-10T19:33:23.318476Z",
     "shell.execute_reply": "2024-02-10T19:33:23.317633Z",
     "shell.execute_reply.started": "2024-02-10T19:33:23.311270Z"
    },
    "id": "TiyOveU2Zpgm"
   },
   "outputs": [],
   "source": [
    "# Preprocessing functions\n",
    "# Function to capitalize the first letter of each sentence and proper nouns\n",
    "def capitalize_first_and_proper_nouns(text):\n",
    "    # Process the text using spaCy to create a Doc object\n",
    "    doc = nlp(text)\n",
    "\n",
    "    result = []\n",
    "\n",
    "    # Iterate over the sentences in the Doc\n",
    "    for sent in doc.sents:\n",
    "        # Iterate over the tokens in the sentence\n",
    "        for token in sent:\n",
    "            # Capitalize the first letter of each sentence and proper nouns\n",
    "            if token.is_sent_start or token.pos_ == 'PROPN':\n",
    "                result.append(token.text.capitalize())\n",
    "            else:\n",
    "                result.append(token.text)\n",
    "\n",
    "    # Rejoin the tokens into a single string\n",
    "    return ' '.join(result)\n",
    "\n",
    "# Function to remove repeated punctuations\n",
    "def remove_repeated_punctuations(sentence):\n",
    "    # Use regular expression to remove consecutive repeated punctuations\n",
    "    cleaned_sentence = re.sub(r'(\\W)\\1+', r'\\1', sentence)\n",
    "    return cleaned_sentence\n",
    "\n",
    "# Function to expand contractions\n",
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "# Define a tokenization function\n",
    "def tokenize_sentences(sentences):\n",
    "    return [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "def fix_general_spacing(sentence):\n",
    "    # Fix space before punctuation (like ' ,' to ',')\n",
    "    sentence = re.sub(r'\\s([,.?!:;])', r'\\1', sentence)\n",
    "    # Fix space after punctuation (like ' . ' to '. ')\n",
    "    sentence = re.sub(r'([,.?!:;])\\s', r'\\1 ', sentence)\n",
    "    # Fix space in contractions (like \"don 't\" to \"don't\")\n",
    "    sentence = re.sub(r\"\\b(\\w+)\\s('t|'s|'m|'ll|'ve|'re|'d|n't)\\b\", r\"\\1\\2\", sentence)\n",
    "    # Reduce multiple spaces between words to a single space\n",
    "    sentence = re.sub(r'\\s{2,}', ' ', sentence)\n",
    "    return sentence\n",
    "\n",
    "def preprocess(text):\n",
    "    # text = text.lower()\n",
    "    text = expand_contractions(text)\n",
    "    text = remove_repeated_punctuations(text)\n",
    "    text = capitalize_first_and_proper_nouns(text)\n",
    "    text = fix_general_spacing(text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T19:33:23.319638Z",
     "iopub.status.busy": "2024-02-10T19:33:23.319395Z",
     "iopub.status.idle": "2024-02-10T19:33:23.934241Z",
     "shell.execute_reply": "2024-02-10T19:33:23.933599Z",
     "shell.execute_reply.started": "2024-02-10T19:33:23.319614Z"
    },
    "id": "249pORAGZpgm"
   },
   "outputs": [],
   "source": [
    "# Load the preprocessed data from the JSON file\n",
    "# data_files={\n",
    "#     \"train\":\"data_train.json\",\n",
    "# }\n",
    "\n",
    "# dataset = load_dataset(\"json\", data_files=data_files)\n",
    "# print(dataset)\n",
    "\n",
    "json_file_path = \"data_train.json\"\n",
    "\n",
    "with open(json_file_path, 'r') as file:\n",
    "    dataset = json.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3LhyQlHZpgn"
   },
   "source": [
    "# A dataset class to load and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T19:33:23.935525Z",
     "iopub.status.busy": "2024-02-10T19:33:23.935222Z",
     "iopub.status.idle": "2024-02-10T19:33:23.940233Z",
     "shell.execute_reply": "2024-02-10T19:33:23.939491Z",
     "shell.execute_reply.started": "2024-02-10T19:33:23.935502Z"
    },
    "id": "AY40VS7kZpgn"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        return {\n",
    "            \"id\": item[\"id\"],\n",
    "            \"topic\": item[\"topic\"],\n",
    "            \"input_seq\": torch.tensor(item[\"input_seq\"]),\n",
    "            \"output_seq\": torch.tensor(item[\"output_seq\"]),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDzIXXWAZpgn"
   },
   "source": [
    "# Creating a Bi-Directional RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T19:33:23.942197Z",
     "iopub.status.busy": "2024-02-10T19:33:23.941453Z",
     "iopub.status.idle": "2024-02-10T19:33:23.946467Z",
     "shell.execute_reply": "2024-02-10T19:33:23.945644Z",
     "shell.execute_reply.started": "2024-02-10T19:33:23.942160Z"
    },
    "id": "EoFBCnZVZpgn"
   },
   "outputs": [],
   "source": [
    "# Preprocessing the whole dataset's informal sentences\n",
    "def preprocess_data(data):\n",
    "    for item in data:\n",
    "        item[\"transformation\"][\"informal\"] = preprocess(item[\"transformation\"][\"informal\"])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T19:33:23.948388Z",
     "iopub.status.busy": "2024-02-10T19:33:23.947783Z",
     "iopub.status.idle": "2024-02-10T19:33:23.951737Z",
     "shell.execute_reply": "2024-02-10T19:33:23.950882Z",
     "shell.execute_reply.started": "2024-02-10T19:33:23.948354Z"
    },
    "id": "bn7qcSwQZpgn"
   },
   "outputs": [],
   "source": [
    "# preprocessed_data = preprocess_data(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T19:33:23.953612Z",
     "iopub.status.busy": "2024-02-10T19:33:23.952800Z",
     "iopub.status.idle": "2024-02-10T19:33:23.957989Z",
     "shell.execute_reply": "2024-02-10T19:33:23.957208Z",
     "shell.execute_reply.started": "2024-02-10T19:33:23.953573Z"
    },
    "id": "n1tNj1tfZpgn"
   },
   "outputs": [],
   "source": [
    "# tokenizing the whole dataset\n",
    "def tokenize_data(data):\n",
    "    for item in data:\n",
    "        item[\"tokenized_informal\"] = word_tokenize(item[\"transformation\"][\"informal\"])\n",
    "        item[\"tokenized_formal\"] = word_tokenize(item[\"transformation\"][\"formal.ref0\"])\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T19:33:23.959269Z",
     "iopub.status.busy": "2024-02-10T19:33:23.958997Z",
     "iopub.status.idle": "2024-02-10T19:33:23.964163Z",
     "shell.execute_reply": "2024-02-10T19:33:23.963345Z",
     "shell.execute_reply.started": "2024-02-10T19:33:23.959248Z"
    },
    "id": "Z_6RS2IgZpgn"
   },
   "outputs": [],
   "source": [
    "# assigning indices to each unique word and creating a vocabulary\n",
    "def build_vocab(data):\n",
    "    input_vocab = set()\n",
    "    output_vocab = set()\n",
    "\n",
    "    for item in data:\n",
    "        input_vocab.update(item[\"tokenized_informal\"])\n",
    "        output_vocab.update(item[\"tokenized_formal\"])\n",
    "\n",
    "    input_vocab = {word: idx for idx, word in enumerate(input_vocab)}\n",
    "    output_vocab = {word: idx for idx, word in enumerate(output_vocab)}\n",
    "\n",
    "    return input_vocab, output_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T19:33:23.966145Z",
     "iopub.status.busy": "2024-02-10T19:33:23.965291Z",
     "iopub.status.idle": "2024-02-10T19:33:23.972031Z",
     "shell.execute_reply": "2024-02-10T19:33:23.971316Z",
     "shell.execute_reply.started": "2024-02-10T19:33:23.966104Z"
    },
    "id": "RTgJBYycZpgn"
   },
   "outputs": [],
   "source": [
    "# function for padding the sequence with max length provided\n",
    "def pad_sequence(sequence, max_length, vocab):\n",
    "    padded_sequence = [vocab[word] for word in sequence if word in vocab]\n",
    "    padded_sequence += [0] * (max_length - len(padded_sequence))\n",
    "    return padded_sequence\n",
    "\n",
    "# function to convert tokenized informal and formal sentences in the dataset using padding of 65 tokens\n",
    "def numericalize_data(data, input_vocab, output_vocab):\n",
    "#     max_input_length = max(len(item[\"tokenized_informal\"]) for item in data)\n",
    "#     max_output_length = max(len(item[\"tokenized_formal\"]) for item in data)\n",
    "    max_input_length = 65\n",
    "    max_output_length = 65\n",
    "\n",
    "    for item in data:\n",
    "        item[\"input_seq\"] = pad_sequence(item[\"tokenized_informal\"], max_input_length, input_vocab)\n",
    "        item[\"output_seq\"] = pad_sequence(item[\"tokenized_formal\"], max_output_length, output_vocab)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMm8J8TmZpgn"
   },
   "source": [
    "## Defining a Bi-Directional RNN Model using LSTM from Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T19:33:23.973418Z",
     "iopub.status.busy": "2024-02-10T19:33:23.973103Z",
     "iopub.status.idle": "2024-02-10T19:33:23.979314Z",
     "shell.execute_reply": "2024-02-10T19:33:23.978558Z",
     "shell.execute_reply.started": "2024-02-10T19:33:23.973385Z"
    },
    "id": "3yI-IWO6Zpgn"
   },
   "outputs": [],
   "source": [
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(BiRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.birnn = nn.LSTM(hidden_size, hidden_size, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, len(output_vocab))\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, _ = self.birnn(embedded)\n",
    "        output = self.fc(output)\n",
    "        return torch.nn.functional.log_softmax(output, dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T19:33:23.980918Z",
     "iopub.status.busy": "2024-02-10T19:33:23.980350Z",
     "iopub.status.idle": "2024-02-10T19:33:23.987287Z",
     "shell.execute_reply": "2024-02-10T19:33:23.986517Z",
     "shell.execute_reply.started": "2024-02-10T19:33:23.980883Z"
    },
    "id": "5D6EF6jBZpgn"
   },
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "            inputs = batch[\"input_seq\"].to(device)\n",
    "            targets = batch[\"output_seq\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.view(-1, len(output_vocab)), targets.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDOy2r78Zpgn"
   },
   "source": [
    "### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T19:33:23.988973Z",
     "iopub.status.busy": "2024-02-10T19:33:23.988490Z",
     "iopub.status.idle": "2024-02-10T19:46:16.784389Z",
     "shell.execute_reply": "2024-02-10T19:46:16.783697Z",
     "shell.execute_reply.started": "2024-02-10T19:33:23.988949Z"
    },
    "id": "LEBBgy0GZpgn"
   },
   "outputs": [],
   "source": [
    "data = preprocess_data(dataset)\n",
    "data = tokenize_data(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T19:46:16.786095Z",
     "iopub.status.busy": "2024-02-10T19:46:16.785511Z",
     "iopub.status.idle": "2024-02-10T19:46:19.565168Z",
     "shell.execute_reply": "2024-02-10T19:46:19.564484Z",
     "shell.execute_reply.started": "2024-02-10T19:46:16.786057Z"
    },
    "id": "fYQptumaKc42"
   },
   "outputs": [],
   "source": [
    "with open(\"./data_train_preprocess_n_token.json\", 'w') as f:\n",
    "  json.dump(data, f, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-02-10T19:46:19.566411Z",
     "iopub.status.busy": "2024-02-10T19:46:19.566090Z",
     "iopub.status.idle": "2024-02-10T19:46:19.570230Z",
     "shell.execute_reply": "2024-02-10T19:46:19.569570Z",
     "shell.execute_reply.started": "2024-02-10T19:46:19.566387Z"
    },
    "id": "qB5Kpq14Zpgo",
    "outputId": "2df5bf7c-89ec-4b37-ffff-ce880b035414",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 1, 'topic': 'Family_Relationships', 'transformation': {'informal': 'Hmmm, I am a guy suffering from verbal abuse from my wife.', 'formal.ref0': 'I suffer through verbal abuse from my wife.', 'formal.ref1': '', 'formal.ref2': '', 'formal.ref3': ''}, 'tokenized_informal': ['Hmmm', ',', 'I', 'am', 'a', 'guy', 'suffering', 'from', 'verbal', 'abuse', 'from', 'my', 'wife', '.'], 'tokenized_formal': ['I', 'suffer', 'through', 'verbal', 'abuse', 'from', 'my', 'wife', '.']}\n"
     ]
    }
   ],
   "source": [
    "print(data[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T19:46:19.571654Z",
     "iopub.status.busy": "2024-02-10T19:46:19.571181Z",
     "iopub.status.idle": "2024-02-10T19:46:21.105372Z",
     "shell.execute_reply": "2024-02-10T19:46:21.104709Z",
     "shell.execute_reply.started": "2024-02-10T19:46:19.571620Z"
    },
    "id": "JKWbIHZCZpgo"
   },
   "outputs": [],
   "source": [
    "# numericalizing the data\n",
    "input_vocab, output_vocab = build_vocab(data)\n",
    "data = numericalize_data(data, input_vocab, output_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-02-10T19:46:21.106634Z",
     "iopub.status.busy": "2024-02-10T19:46:21.106265Z",
     "iopub.status.idle": "2024-02-10T19:46:21.111115Z",
     "shell.execute_reply": "2024-02-10T19:46:21.110490Z",
     "shell.execute_reply.started": "2024-02-10T19:46:21.106593Z"
    },
    "id": "TDes1lnUZpgo",
    "outputId": "152dfc3e-753f-4516-c181-02761fb15de3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104562\n",
      "input_seq: \n",
      "65\n",
      "[9153, 27463, 2674, 15503, 11541, 27463, 48901, 39206, 10198, 15283, 43127, 11309, 25505, 31303, 1096, 44518, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "output_seq: \n",
      "65\n",
      "[27855, 30566, 186, 30637, 8073, 18183, 22268, 771, 31664, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(len(data))\n",
    "print(\"input_seq: \")\n",
    "print(len(data[0]['input_seq']))\n",
    "print(data[0]['input_seq'])\n",
    "print(\"output_seq: \")\n",
    "print(len(data[0]['output_seq']))\n",
    "print(data[0]['output_seq'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T19:46:21.115077Z",
     "iopub.status.busy": "2024-02-10T19:46:21.114769Z",
     "iopub.status.idle": "2024-02-10T19:46:21.119024Z",
     "shell.execute_reply": "2024-02-10T19:46:21.118273Z",
     "shell.execute_reply.started": "2024-02-10T19:46:21.115055Z"
    },
    "id": "m9StD9p2Zpgo"
   },
   "outputs": [],
   "source": [
    "# training parameters\n",
    "train_data = data\n",
    "train_dataset = CustomDataset(train_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "num_epochs = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T19:46:21.120295Z",
     "iopub.status.busy": "2024-02-10T19:46:21.119767Z",
     "iopub.status.idle": "2024-02-10T19:46:22.299944Z",
     "shell.execute_reply": "2024-02-10T19:46:22.299313Z",
     "shell.execute_reply.started": "2024-02-10T19:46:21.120259Z"
    },
    "id": "TOnjNLy_Zpgo"
   },
   "outputs": [],
   "source": [
    "# getting everything together for the model\n",
    "model = BiRNN(len(input_vocab), 256, len(output_vocab)).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-02-10T19:46:22.301163Z",
     "iopub.status.busy": "2024-02-10T19:46:22.300847Z",
     "iopub.status.idle": "2024-02-10T20:35:32.517882Z",
     "shell.execute_reply": "2024-02-10T20:35:32.516879Z",
     "shell.execute_reply.started": "2024-02-10T19:46:22.301139Z"
    },
    "id": "x9-e4qVEZpgo",
    "outputId": "3528f7e1-5fdb-4653-aa0b-eb04795d464b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 3268/3268 [04:51<00:00, 11.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 5.64649626018076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 3268/3268 [04:54<00:00, 11.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 5.1782613426556345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 3268/3268 [04:55<00:00, 11.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 4.963087262547002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 3268/3268 [04:55<00:00, 11.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 4.84183089801702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 3268/3268 [04:55<00:00, 11.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 4.760151210948916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 3268/3268 [04:55<00:00, 11.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 4.697540205870293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 3268/3268 [04:55<00:00, 11.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 4.643745141995288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 3268/3268 [04:55<00:00, 11.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 4.600115753081028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 3268/3268 [04:55<00:00, 11.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 4.560939538289167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 3268/3268 [04:55<00:00, 11.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 4.527787081906379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "trained_model = train_model(model, train_loader, criterion, optimizer, num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T20:35:32.519767Z",
     "iopub.status.busy": "2024-02-10T20:35:32.519284Z",
     "iopub.status.idle": "2024-02-10T20:35:33.143529Z",
     "shell.execute_reply": "2024-02-10T20:35:33.142837Z",
     "shell.execute_reply.started": "2024-02-10T20:35:32.519729Z"
    },
    "id": "uJWNidupWMlc"
   },
   "outputs": [],
   "source": [
    "# saving the entire model\n",
    "torch.save(model.state_dict(), 'bi_rnn_model.pth')\n",
    "\n",
    "# saving the vocabularies as well for future reference\n",
    "vocabularies = {\n",
    "    'input_vocab': input_vocab,\n",
    "    'output_vocab': output_vocab\n",
    "}\n",
    "\n",
    "with open('vocabularies.json', 'w') as vocab_file:\n",
    "    json.dump(vocabularies, vocab_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T20:35:33.145263Z",
     "iopub.status.busy": "2024-02-10T20:35:33.144631Z",
     "iopub.status.idle": "2024-02-10T20:35:33.604073Z",
     "shell.execute_reply": "2024-02-10T20:35:33.603394Z",
     "shell.execute_reply.started": "2024-02-10T20:35:33.145227Z"
    },
    "id": "QknT2OlyWTEC"
   },
   "outputs": [],
   "source": [
    "max_input_length = 65\n",
    "# loading the vocabularies\n",
    "with open('./vocabularies.json', 'r') as vocab_file:\n",
    "    loaded_vocabularies = json.load(vocab_file)\n",
    "\n",
    "input_vocab = loaded_vocabularies['input_vocab']\n",
    "output_vocab = loaded_vocabularies['output_vocab']\n",
    "\n",
    "# creating a new instance of the BiRNN model\n",
    "loaded_model = BiRNN(len(input_vocab), 256, len(output_vocab)).to(device)\n",
    "\n",
    "# loading the trained model weights\n",
    "map_location=torch.device('cpu')\n",
    "loaded_model.load_state_dict(torch.load('./bi_rnn_model.pth'))\n",
    "loaded_model.eval()  # Setting the model to evaluation mode\n",
    "\n",
    "def predict(model, input_sequence, input_vocab, output_vocab):\n",
    "    # Preprocess and tokenize input_sequence\n",
    "    preprocessed_input = preprocess(input_sequence)\n",
    "    tokenized_input = word_tokenize(preprocessed_input)\n",
    "\n",
    "    # Numericalize input using the loaded vocabularies\n",
    "    numericalized_input = pad_sequence(tokenized_input, max_input_length, input_vocab)\n",
    "\n",
    "    # Convert numericalized input to PyTorch tensor\n",
    "    input_tensor = torch.tensor(numericalized_input).unsqueeze(0).to(device)\n",
    "\n",
    "    # Make prediction using the loaded model\n",
    "    with torch.no_grad():\n",
    "        model_output = model(input_tensor)\n",
    "\n",
    "    # Process the model output\n",
    "    _, predicted_indices = torch.max(model_output, dim=2 )\n",
    "\n",
    "    # Convert predicted indices to words using the output vocabulary\n",
    "    predicted_words = [word for index in predicted_indices.squeeze().tolist() for word, idx in output_vocab.items() if idx == index]\n",
    "\n",
    "    # Join the words into a string (or use as needed)\n",
    "    processed_output = ' '.join(predicted_words)\n",
    "\n",
    "    return processed_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-02-10T20:35:33.605364Z",
     "iopub.status.busy": "2024-02-10T20:35:33.605014Z",
     "iopub.status.idle": "2024-02-10T20:35:33.726067Z",
     "shell.execute_reply": "2024-02-10T20:35:33.725219Z",
     "shell.execute_reply.started": "2024-02-10T20:35:33.605336Z"
    },
    "id": "t_9B0JxoWxp_",
    "outputId": "9e57cc5a-f972-4592-e900-05ef8dc1fe55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted output: wish is not . that you , want to on depends . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n"
     ]
    }
   ],
   "source": [
    "# example usage:\n",
    "input_sentence = \"Luck isn't everything that you may wanna rely upon!!!\"\n",
    "predictions = predict(loaded_model, input_sentence, input_vocab, output_vocab)\n",
    "print(\"Predicted output:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T20:35:33.727596Z",
     "iopub.status.busy": "2024-02-10T20:35:33.727190Z",
     "iopub.status.idle": "2024-02-10T20:35:33.749459Z",
     "shell.execute_reply": "2024-02-10T20:35:33.748647Z",
     "shell.execute_reply.started": "2024-02-10T20:35:33.727567Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wish is not. That you, want to on depends......................................................\n"
     ]
    }
   ],
   "source": [
    "predictions = preprocess(predictions)\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "seMoRzzC7W7s"
   },
   "source": [
    "# Metrics Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing TERp and PINC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-02-10T20:35:33.750774Z",
     "iopub.status.busy": "2024-02-10T20:35:33.750438Z",
     "iopub.status.idle": "2024-02-10T20:35:41.548773Z",
     "shell.execute_reply": "2024-02-10T20:35:41.547840Z",
     "shell.execute_reply.started": "2024-02-10T20:35:33.750748Z"
    },
    "id": "1dVY4RavDmN6",
    "outputId": "d3d13484-9f26-4da6-ea85-ca22ca282d59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchmetrics\n",
      "  Using cached torchmetrics-1.3.0.post0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>1.20.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from torchmetrics) (1.26.3)\n",
      "Requirement already satisfied: packaging>17.1 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from torchmetrics) (23.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from torchmetrics) (2.1.0)\n",
      "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
      "  Using cached lightning_utilities-0.10.1-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: setuptools in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (69.0.3)\n",
      "Requirement already satisfied: typing-extensions in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.9.0)\n",
      "Requirement already satisfied: filelock in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.13.1)\n",
      "Requirement already satisfied: sympy in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
      "Using cached torchmetrics-1.3.0.post0-py3-none-any.whl (840 kB)\n",
      "Using cached lightning_utilities-0.10.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: lightning-utilities, torchmetrics\n",
      "Successfully installed lightning-utilities-0.10.1 torchmetrics-1.3.0.post0\n",
      "Requirement already satisfied: nltk in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (3.8.1)\n",
      "Collecting sacrebleu\n",
      "  Using cached sacrebleu-2.4.0-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: click in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from nltk) (4.66.1)\n",
      "Collecting portalocker (from sacrebleu)\n",
      "  Using cached portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting tabulate>=0.8.9 (from sacrebleu)\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from sacrebleu) (1.26.3)\n",
      "Requirement already satisfied: colorama in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\n",
      "Collecting lxml (from sacrebleu)\n",
      "  Using cached lxml-5.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.5 kB)\n",
      "Using cached sacrebleu-2.4.0-py3-none-any.whl (106 kB)\n",
      "Using cached lxml-5.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.0 MB)\n",
      "Using cached portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
      "Installing collected packages: tabulate, portalocker, lxml, sacrebleu\n",
      "Successfully installed lxml-5.1.0 portalocker-2.8.2 sacrebleu-2.4.0 tabulate-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchmetrics\n",
    "!pip install nltk sacrebleu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-02-10T20:35:41.551469Z",
     "iopub.status.busy": "2024-02-10T20:35:41.551066Z",
     "iopub.status.idle": "2024-02-10T20:35:46.060204Z",
     "shell.execute_reply": "2024-02-10T20:35:46.059465Z",
     "shell.execute_reply.started": "2024-02-10T20:35:41.551437Z"
    },
    "id": "eMFyxV7MDr5c",
    "outputId": "f6e5de8b-1795-4a9a-df6f-c1158d813f8e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-10 20:35:44.009887: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-10 20:35:44.055568: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-10 20:35:44.055600: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-10 20:35:44.056795: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-10 20:35:44.064516: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-10 20:35:45.421931: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import numpy as np\n",
    "import json\n",
    "from transformers import EncoderDecoderModel\n",
    "import torchmetrics\n",
    "from datasets import Dataset\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import gc\n",
    "import torch\n",
    "import math\n",
    "import sacrebleu\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk import ngrams\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T20:35:46.061922Z",
     "iopub.status.busy": "2024-02-10T20:35:46.061326Z",
     "iopub.status.idle": "2024-02-10T20:35:46.074060Z",
     "shell.execute_reply": "2024-02-10T20:35:46.073172Z",
     "shell.execute_reply.started": "2024-02-10T20:35:46.061895Z"
    },
    "id": "TfxVHg31D8sN"
   },
   "outputs": [],
   "source": [
    "#Higher is better\n",
    "def calculate_terp(hypothesis: str, reference: str, phrase_table=list(), edit_costs=list()):\n",
    "    hypothesis_tokens = hypothesis.split()\n",
    "    reference_tokens = reference.split()\n",
    "\n",
    "    # TERp by Stem Matches, Synonym Matches, and Phrase Substitutions\n",
    "    stem_matches = calculate_stem_matches(hypothesis_tokens, reference_tokens)\n",
    "    synonym_matches = calculate_synonym_matches(hypothesis_tokens, reference_tokens)\n",
    "    phrase_substitutions = calculate_phrase_substitutions(hypothesis_tokens, reference_tokens, phrase_table, edit_costs)\n",
    "\n",
    "    # Calculate TERp score\n",
    "    terp_score = (stem_matches + synonym_matches + phrase_substitutions) / len(reference_tokens)\n",
    "\n",
    "    return terp_score\n",
    "\n",
    "def calculate_stem_matches(hypothesis_tokens, reference_tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    stem_matches = sum(1 for hyp_token, ref_token in zip(hypothesis_tokens, reference_tokens)\n",
    "                      if stemmer.stem(hyp_token.lower()) == stemmer.stem(ref_token.lower()))\n",
    "    return stem_matches\n",
    "\n",
    "def calculate_synonym_matches(hypothesis_tokens, reference_tokens):\n",
    "    synonym_matches = sum(1 for hyp_token, ref_token in zip(hypothesis_tokens, reference_tokens)\n",
    "                          if are_synonyms(hyp_token.lower(), ref_token.lower()))\n",
    "    return synonym_matches\n",
    "\n",
    "def are_synonyms(word1, word2):\n",
    "    synsets1 = wordnet.synsets(word1)\n",
    "    synsets2 = wordnet.synsets(word2)\n",
    "\n",
    "    return any(set1.wup_similarity(set2) > 0.7 for set1 in synsets1 for set2 in synsets2)\n",
    "\n",
    "def calculate_phrase_substitutions(hypothesis_tokens, reference_tokens, phrase_table, edit_costs):\n",
    "    substitution_cost = 0\n",
    "\n",
    "    for i in range(len(hypothesis_tokens)):\n",
    "        for j in range(len(reference_tokens)):\n",
    "            if (hypothesis_tokens[i], reference_tokens[j]) in phrase_table:\n",
    "                # Retrieve paraphrase information from the phrase table\n",
    "                paraphrase_info = phrase_table[(hypothesis_tokens[i], reference_tokens[j])]\n",
    "\n",
    "                # Calculate the cost using the provided formula\n",
    "                cost = (\n",
    "                    edit_costs['w1'] +\n",
    "                    edit_costs['w2'] * paraphrase_info['edit'] * math.log(paraphrase_info['probability']) +\n",
    "                    edit_costs['w3'] * paraphrase_info['edit'] * paraphrase_info['probability'] +\n",
    "                    edit_costs['w4'] * paraphrase_info['edit']\n",
    "                )\n",
    "\n",
    "                # Ensure the substitution cost is not negative\n",
    "                substitution_cost += max(0, cost)\n",
    "\n",
    "    return substitution_cost\n",
    "\n",
    "def terp_alignment(hypothesis, reference, phrase_table=list(), edit_costs=list()):\n",
    "    alignment = []\n",
    "\n",
    "    for hyp_token, ref_token in zip(hypothesis.split(), reference.split()):\n",
    "        if hyp_token == ref_token:\n",
    "            alignment.append((hyp_token, ref_token, \"Exact Match\"))\n",
    "        else:\n",
    "            alignment.append((hyp_token, ref_token, \"Mismatch\"))\n",
    "\n",
    "    return alignment\n",
    "\n",
    "def calculate_pinc(hypothesis: str, reference: str, n: int):\n",
    "    hypothesis_split = hypothesis.split()\n",
    "    reference_split = reference.split()\n",
    "\n",
    "    hypothesis_ngrams = set(ngrams(hypothesis_split, n))\n",
    "    reference_ngrams = set(ngrams(reference_split, n))\n",
    "    new_ngrams = hypothesis_ngrams - reference_ngrams\n",
    "    pinc_score = len(new_ngrams) / len(hypothesis_ngrams)\n",
    "\n",
    "    return pinc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-02-10T20:35:46.075730Z",
     "iopub.status.busy": "2024-02-10T20:35:46.075305Z",
     "iopub.status.idle": "2024-02-10T20:35:47.695279Z",
     "shell.execute_reply": "2024-02-10T20:35:47.694313Z",
     "shell.execute_reply.started": "2024-02-10T20:35:46.075693Z"
    },
    "id": "DhIW79_LEprc",
    "outputId": "07bc90a6-3905-43cc-cd83-d758f1ab9b97",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TERp Score: 1.6\n",
      "Alignment: [('This', 'This', 'Exact Match'), ('is', 'is', 'Exact Match'), ('an', 'an', 'Exact Match'), ('example', 'example', 'Exact Match'), ('sentence.', 'sentence.', 'Exact Match')]\n"
     ]
    }
   ],
   "source": [
    "# example usage:\n",
    "hypothesis_sentence = \"This is an example sentence.\"\n",
    "reference_sentence = \"This is an example sentence.\"\n",
    "\n",
    "# calculating TERp score\n",
    "terp_score = calculate_terp(hypothesis_sentence, reference_sentence)\n",
    "print(f\"TERp Score: {terp_score}\")\n",
    "\n",
    "# generating alignment\n",
    "alignment = terp_alignment(hypothesis_sentence, reference_sentence)\n",
    "print(\"Alignment:\", alignment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-02-10T20:35:47.696722Z",
     "iopub.status.busy": "2024-02-10T20:35:47.696345Z",
     "iopub.status.idle": "2024-02-10T20:35:47.701050Z",
     "shell.execute_reply": "2024-02-10T20:35:47.700275Z",
     "shell.execute_reply.started": "2024-02-10T20:35:47.696675Z"
    },
    "id": "LADXQx2vE0GL",
    "outputId": "19688f47-9f41-4098-c6ed-221a324f5648"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PINC Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "pinc_score = calculate_pinc(hypothesis_sentence, reference_sentence, 2)\n",
    "print(f\"PINC Score: {pinc_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T20:35:47.702604Z",
     "iopub.status.busy": "2024-02-10T20:35:47.702226Z",
     "iopub.status.idle": "2024-02-10T20:35:47.708209Z",
     "shell.execute_reply": "2024-02-10T20:35:47.707470Z",
     "shell.execute_reply.started": "2024-02-10T20:35:47.702569Z"
    },
    "id": "643-PTYTE2vR",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def terp(preds, refs):\n",
    "  scores = np.zeros(len(preds), dtype=float)\n",
    "  for i in range(len(preds)):\n",
    "    pred = preds[i]\n",
    "    ref = refs[i]\n",
    "    score = np.min(np.array(list(map(lambda x: calculate_terp(pred, x), ref))))\n",
    "    scores[i] = score\n",
    "\n",
    "  return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T20:35:47.709992Z",
     "iopub.status.busy": "2024-02-10T20:35:47.709332Z",
     "iopub.status.idle": "2024-02-10T20:35:47.715472Z",
     "shell.execute_reply": "2024-02-10T20:35:47.714582Z",
     "shell.execute_reply.started": "2024-02-10T20:35:47.709959Z"
    },
    "id": "7KLK7J8hE5L7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pinc(preds, refs, n=2):\n",
    "  scores = np.zeros(len(preds), dtype=float)\n",
    "  for i in range(len(preds)):\n",
    "    pred = preds[i]\n",
    "    ref = refs[i]\n",
    "    score = np.min(np.array(list(map(lambda x: calculate_pinc(pred, x, n), ref))))\n",
    "    scores[i] = score\n",
    "\n",
    "  return np.mean(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T20:35:47.717272Z",
     "iopub.status.busy": "2024-02-10T20:35:47.716666Z",
     "iopub.status.idle": "2024-02-10T20:35:47.721146Z",
     "shell.execute_reply": "2024-02-10T20:35:47.720345Z",
     "shell.execute_reply.started": "2024-02-10T20:35:47.717237Z"
    },
    "id": "vVcvPL8-KiU4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_files={\n",
    "    \"val\":\"./GYAFC_Corpus/data_val.json\",\n",
    "    \"test\":\"./GYAFC_Corpus/data_test.json\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T20:35:47.722895Z",
     "iopub.status.busy": "2024-02-10T20:35:47.722294Z",
     "iopub.status.idle": "2024-02-10T20:35:47.763293Z",
     "shell.execute_reply": "2024-02-10T20:35:47.762635Z",
     "shell.execute_reply.started": "2024-02-10T20:35:47.722860Z"
    },
    "id": "V038BzKoKk9R",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data from the validation set\n",
    "with open(data_files[\"val\"], 'r') as file:\n",
    "    val_data = json.load(file)\n",
    "\n",
    "# Load data from the test set\n",
    "with open(data_files[\"test\"], 'r') as file:\n",
    "    test_data = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T20:35:47.764820Z",
     "iopub.status.busy": "2024-02-10T20:35:47.764363Z",
     "iopub.status.idle": "2024-02-10T20:35:47.783274Z",
     "shell.execute_reply": "2024-02-10T20:35:47.782424Z",
     "shell.execute_reply.started": "2024-02-10T20:35:47.764793Z"
    },
    "id": "ywmrCVsHNJTT",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Separate informal, formal references for evaluation\n",
    "val_informal = [item['transformation']['informal'] for item in val_data]\n",
    "val_formal_refs = [\n",
    "    [item['transformation'][f'formal.ref{i}'] for i in range(4)] for item in val_data\n",
    "]\n",
    "\n",
    "test_informal = [item['transformation']['informal'] for item in test_data]\n",
    "test_formal_refs = [\n",
    "    [item['transformation'][f'formal.ref{i}'] for i in range(4)] for item in test_data\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T20:35:47.784651Z",
     "iopub.status.busy": "2024-02-10T20:35:47.784341Z",
     "iopub.status.idle": "2024-02-10T20:35:47.788823Z",
     "shell.execute_reply": "2024-02-10T20:35:47.788048Z",
     "shell.execute_reply.started": "2024-02-10T20:35:47.784628Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2748\n",
      "5665\n"
     ]
    }
   ],
   "source": [
    "print(len(test_formal_refs))\n",
    "print(len(val_formal_refs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T21:42:59.006009Z",
     "iopub.status.busy": "2024-02-10T21:42:59.005608Z",
     "iopub.status.idle": "2024-02-10T22:00:13.952974Z",
     "shell.execute_reply": "2024-02-10T22:00:13.952049Z",
     "shell.execute_reply.started": "2024-02-10T21:42:59.005978Z"
    },
    "id": "mGYUY-NxNoqj",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you are under years you have a Big Problem.......................................................\n",
      "I So what if it is a the relationship for both of you?...................................................\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "val_preds = [preprocess(predict(model, input_seq, input_vocab, output_vocab)) for input_seq in val_informal]\n",
    "print(val_preds[0])\n",
    "test_preds = [preprocess(predict(model, input_seq, input_vocab, output_vocab)) for input_seq in test_informal]\n",
    "print(test_preds[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-02-10T22:00:13.955616Z",
     "iopub.status.busy": "2024-02-10T22:00:13.954887Z",
     "iopub.status.idle": "2024-02-10T22:05:43.521710Z",
     "shell.execute_reply": "2024-02-10T22:05:43.520901Z",
     "shell.execute_reply.started": "2024-02-10T22:00:13.955576Z"
    },
    "id": "nk73WLXpNuQ9",
    "outputId": "fc9510c0-b609-4b59-ac72-e784125bdde1",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average TERp Score on Validation Set: 0.12728081218710285\n",
      "Average PINC Score on Validation Set (n=2): 0.5578529250666088\n",
      "Average TERp Score on Test Set: 0.13842399521922077\n",
      "Average PINC Score on Test Set (n=2): 0.5375957588896298\n"
     ]
    }
   ],
   "source": [
    "# Evaluate TERp score on validation set\n",
    "val_terp_score = terp(val_preds, val_formal_refs)\n",
    "print(f\"Average TERp Score on Validation Set: {val_terp_score}\")\n",
    "# Evaluate PINC score on validation set\n",
    "val_pinc_score = pinc(val_preds, val_formal_refs, n=2)\n",
    "print(f\"Average PINC Score on Validation Set (n=2): {val_pinc_score}\")\n",
    "\n",
    "# Evaluate TERp score on test set\n",
    "test_terp_score = terp(test_preds, test_formal_refs)\n",
    "print(f\"Average TERp Score on Test Set: {test_terp_score}\")\n",
    "# Evaluate PINC score on validation set\n",
    "test_pinc_score = pinc(test_preds, test_formal_refs, n=2)\n",
    "print(f\"Average PINC Score on Test Set (n=2): {test_pinc_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing BLEU and TER from torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T22:05:43.523142Z",
     "iopub.status.busy": "2024-02-10T22:05:43.522692Z",
     "iopub.status.idle": "2024-02-10T22:05:46.811362Z",
     "shell.execute_reply": "2024-02-10T22:05:46.810474Z",
     "shell.execute_reply.started": "2024-02-10T22:05:43.523115Z"
    },
    "id": "2Ul0FVN2Peh7",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchmetrics in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (1.3.0.post0)\n",
      "Requirement already satisfied: numpy>1.20.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from torchmetrics) (1.26.3)\n",
      "Requirement already satisfied: packaging>17.1 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from torchmetrics) (23.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from torchmetrics) (2.1.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from torchmetrics) (0.10.1)\n",
      "Requirement already satisfied: setuptools in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (69.0.3)\n",
      "Requirement already satisfied: typing-extensions in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.9.0)\n",
      "Requirement already satisfied: filelock in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.13.1)\n",
      "Requirement already satisfied: sympy in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchmetrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T22:05:46.813580Z",
     "iopub.status.busy": "2024-02-10T22:05:46.813231Z",
     "iopub.status.idle": "2024-02-10T22:06:27.419781Z",
     "shell.execute_reply": "2024-02-10T22:06:27.418900Z",
     "shell.execute_reply.started": "2024-02-10T22:05:46.813552Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU Score on Validation Set: 0.41184934973716736\n",
      "Average TER Score on Validation Set: 0.436870813369751\n",
      "Average BLEU Score on Test Set: 0.42691048979759216\n",
      "Average TER Score on Test Set: 0.4093644320964813\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchmetrics\n",
    "from torchmetrics.text import BLEUScore\n",
    "from torchmetrics.text import TranslationEditRate\n",
    "\n",
    "def calculate_bleu(hypothesis, reference):\n",
    "    bleu_metric = torchmetrics.BLEUScore()\n",
    "    return bleu_metric(hypothesis, reference)\n",
    "\n",
    "def calculate_ter(hypothesis, reference):\n",
    "    ter_metric = torchmetrics.TranslationEditRate()\n",
    "    return ter_metric(hypothesis, reference)\n",
    "\n",
    "# Example usage:\n",
    "hypothesis_sentence = \"This is an example sentence.\"\n",
    "reference_sentence = \"This is an example sentence.\"\n",
    "\n",
    "# Calculate BLEU score on Validation Set\n",
    "bleu_score = calculate_bleu(val_preds, val_formal_refs)\n",
    "print(f\"Average BLEU Score on Validation Set: {bleu_score}\")\n",
    "# Calculate TER score on Validation Set\n",
    "ter_score = calculate_ter(val_preds, val_formal_refs)\n",
    "print(f\"Average TER Score on Validation Set: {ter_score}\")\n",
    "\n",
    "# Calculate BLEU score on Test Set\n",
    "bleu_score = calculate_bleu(test_preds, test_formal_refs)\n",
    "print(f\"Average BLEU Score on Test Set: {bleu_score}\")\n",
    "# Calculate TER score on Test Set\n",
    "ter_score = calculate_ter(test_preds, test_formal_refs)\n",
    "print(f\"Average TER Score on Test Set: {ter_score}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
