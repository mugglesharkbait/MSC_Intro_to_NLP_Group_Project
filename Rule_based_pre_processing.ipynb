{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "# import nltk\n",
    "import string\n",
    "from nltk import word_tokenize, pos_tag\n",
    "import contractions\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to capitalize the first letter of each sentence and proper nouns\n",
    "def capitalize_first_and_proper_nouns(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged = pos_tag(tokens)\n",
    "    result = []\n",
    "    for i in range(len(tagged)):\n",
    "        if i == 0:\n",
    "            result.append(tagged[i][0].capitalize())\n",
    "        elif tagged[i][1] == 'NNP' and tagged[i-1][0] == '.':\n",
    "            result.append(tagged[i][0].capitalize())\n",
    "        else:\n",
    "            result.append(tagged[i][0])\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove repeated punctuations\n",
    "def remove_repeated_punctuations(text):\n",
    "    pattern = r'([' + re.escape(string.punctuation) + r'])\\1+'\n",
    "    return re.sub(pattern, r'\\1', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to expand contractions\n",
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a tokenization function\n",
    "def tokenize_sentences(sentences):\n",
    "    return [word_tokenize(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_general_spacing(sentence):\n",
    "    # Fix space before punctuation (like ' ,' to ',')\n",
    "    sentence = re.sub(r'\\s([,.?!:;])', r'\\1', sentence)\n",
    "    # Fix space after punctuation (like ' . ' to '. ')\n",
    "    sentence = re.sub(r'([,.?!:;])\\s', r'\\1 ', sentence)\n",
    "    # Fix space in contractions (like \"don 't\" to \"don't\")\n",
    "    sentence = re.sub(r\"\\b(\\w+)\\s('t|'s|'m|'ll|'ve|'re|'d|n't)\\b\", r\"\\1\\2\", sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    # text = text.lower()\n",
    "    text = capitalize_first_and_proper_nouns(text)\n",
    "    text = expand_contractions(text)\n",
    "    text = remove_repeated_punctuations(text)\n",
    "    text = fix_general_spacing(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_pair_data (train_file_EM_informal, train_file_EM_formal):\n",
    "    # Read the informal and formal sentences from the provided text files\n",
    "    with open(train_file_EM_informal, 'r', encoding='utf-8') as file:\n",
    "        informal_sentences = file.readlines()\n",
    "\n",
    "    with open(train_file_EM_formal, 'r', encoding='utf-8') as file:\n",
    "        formal_sentences = file.readlines()\n",
    "\n",
    "    # Preprocess the data \n",
    "    informal_sentences = [preprocess(text) for text in informal_sentences]\n",
    "    formal_sentences = [preprocess(text) for text in formal_sentences]\n",
    "\n",
    "    # Create dataframes from the sentences lists\n",
    "    df_informal = pd.DataFrame({'informal': informal_sentences})\n",
    "    df_formal = pd.DataFrame({'formal': formal_sentences})\n",
    "\n",
    "    # Strip whitespace from the beginning and end of sentences\n",
    "    df_informal['informal'] = df_informal['informal'].str.strip()\n",
    "    df_formal['formal'] = df_formal['formal'].str.strip()\n",
    "\n",
    "    # Assuming that each line corresponds to a sentence pair, we can concatenate the dataframes\n",
    "    df_paired = pd.concat([df_informal, df_formal], axis=1)\n",
    "\n",
    "    return df_paired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to train data \n",
    "train_file_EM_informal = \"./GYAFC_Corpus/Entertainment_Music/train/informal\"\n",
    "train_file_EM_formal = \"./GYAFC_Corpus/Entertainment_Music/train/formal\"\n",
    "train_file_FR_informal = \"./GYAFC_Corpus/Family_Relationships/train/informal\"\n",
    "train_file_FR_formal = \"./GYAFC_Corpus/Family_Relationships/train/formal\"\n",
    "\n",
    "# Get preprocessed dataframes\n",
    "train_df_EM_paired = read_and_pair_data(train_file_EM_informal, train_file_EM_formal)\n",
    "train_df_FR_paired = read_and_pair_data(train_file_FR_informal, train_file_FR_formal)\n",
    "\n",
    "# Tokenize both informal and formal sentences from Entertainment Music\n",
    "train_df_EM_paired['informal_tokenized'] = tokenize_sentences(train_df_EM_paired['informal'])\n",
    "train_df_EM_paired['formal_tokenized'] = tokenize_sentences(train_df_EM_paired['formal'])\n",
    "\n",
    "# Tokenize both informal and formal sentences from Family Relationships\n",
    "train_df_FR_paired['informal_tokenized'] = tokenize_sentences(train_df_FR_paired['informal'])\n",
    "train_df_FR_paired['formal_tokenized'] = tokenize_sentences(train_df_FR_paired['formal'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df_EM_paired.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df_FR_paired.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
