{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk import word_tokenize, pos_tag\n",
    "import contractions\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy model outside of the function to avoid reloading it each time the function is called\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to capitalize the first letter of each sentence and proper nouns\n",
    "def capitalize_first_and_proper_nouns(text):\n",
    "    # Process the text using spaCy to create a Doc object\n",
    "    doc = nlp(text)\n",
    "\n",
    "    result = []\n",
    "\n",
    "    # Iterate over the sentences in the Doc\n",
    "    for sent in doc.sents:\n",
    "        # Iterate over the tokens in the sentence\n",
    "        for token in sent:\n",
    "            # Capitalize the first letter of each sentence and proper nouns\n",
    "            if token.is_sent_start or token.pos_ == 'PROPN':\n",
    "                result.append(token.text.capitalize())\n",
    "            else:\n",
    "                result.append(token.text)\n",
    "\n",
    "    # Rejoin the tokens into a single string\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sample sentence . John and Mary went to the park . The park was beautiful .\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_text = \"this is a sample sentence. john and mary went to the park. the park was beautiful.\"\n",
    "result_text = capitalize_first_and_proper_nouns(input_text)\n",
    "print(result_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How are you? I hope you're doing well.\n"
     ]
    }
   ],
   "source": [
    "# Function to remove repeated punctuations\n",
    "def remove_repeated_punctuations(sentence):\n",
    "    # Use regular expression to remove consecutive repeated punctuations\n",
    "    cleaned_sentence = re.sub(r'(\\W)\\1+', r'\\1', sentence)\n",
    "    return cleaned_sentence\n",
    "\n",
    "# Example usage:\n",
    "sentence1 = \"Hello!!! How are you?? I hope you''re doing well.....\"\n",
    "cleaned_sentence1 = remove_repeated_punctuations(sentence1)\n",
    "print(cleaned_sentence1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to expand contractions\n",
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a student and I have a test tomorrow.\n"
     ]
    }
   ],
   "source": [
    "text = \"I'm a student and I've a test tomorrow.\"\n",
    "expanded_text = expand_contractions(text)\n",
    "print(expanded_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a tokenization function\n",
    "def tokenize_sentences(sentences):\n",
    "    return [word_tokenize(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_general_spacing(sentence):\n",
    "    # Fix space before punctuation (like ' ,' to ',')\n",
    "    sentence = re.sub(r'\\s([,.?!:;])', r'\\1', sentence)\n",
    "    # Fix space after punctuation (like ' . ' to '. ')\n",
    "    sentence = re.sub(r'([,.?!:;])\\s', r'\\1 ', sentence)\n",
    "    # Fix space in contractions (like \"don 't\" to \"don't\")\n",
    "    sentence = re.sub(r\"\\b(\\w+)\\s('t|'s|'m|'ll|'ve|'re|'d|n't)\\b\", r\"\\1\\2\", sentence)\n",
    "    # Reduce multiple spaces between words to a single space\n",
    "    sentence = re.sub(r'\\s{2,}', ' ', sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    # text = text.lower()\n",
    "    text = capitalize_first_and_proper_nouns(text)\n",
    "    text = expand_contractions(text)\n",
    "    text = remove_repeated_punctuations(text)\n",
    "    text = fix_general_spacing(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_pair_data (train_file_EM_informal, train_file_EM_formal):\n",
    "    # Read the informal and formal sentences from the provided text files\n",
    "    with open(train_file_EM_informal, 'r', encoding='utf-8') as file:\n",
    "        informal_sentences = file.readlines()\n",
    "\n",
    "    with open(train_file_EM_formal, 'r', encoding='utf-8') as file:\n",
    "        formal_sentences = file.readlines()\n",
    "\n",
    "    # Preprocess the data \n",
    "    informal_sentences = [preprocess(text) for text in informal_sentences]\n",
    "    formal_sentences = [preprocess(text) for text in formal_sentences]\n",
    "\n",
    "    # Create dataframes from the sentences lists\n",
    "    df_informal = pd.DataFrame({'informal': informal_sentences})\n",
    "    df_formal = pd.DataFrame({'formal': formal_sentences})\n",
    "\n",
    "    # Strip whitespace from the beginning and end of sentences\n",
    "    df_informal['informal'] = df_informal['informal'].str.strip()\n",
    "    df_formal['formal'] = df_formal['formal'].str.strip()\n",
    "\n",
    "    # Assuming that each line corresponds to a sentence pair, we can concatenate the dataframes\n",
    "    df_paired = pd.concat([df_informal, df_formal], axis=1)\n",
    "\n",
    "    return df_paired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to train data \n",
    "train_file_EM_informal = \"./GYAFC_Corpus/Entertainment_Music/train/informal\"\n",
    "train_file_EM_formal = \"./GYAFC_Corpus/Entertainment_Music/train/formal\"\n",
    "train_file_FR_informal = \"./GYAFC_Corpus/Family_Relationships/train/informal\"\n",
    "train_file_FR_formal = \"./GYAFC_Corpus/Family_Relationships/train/formal\"\n",
    "\n",
    "# Get preprocessed dataframes\n",
    "train_df_EM_paired = read_and_pair_data(train_file_EM_informal, train_file_EM_formal)\n",
    "train_df_FR_paired = read_and_pair_data(train_file_FR_informal, train_file_FR_formal)\n",
    "\n",
    "# Tokenize both informal and formal sentences from Entertainment Music\n",
    "train_df_EM_paired['informal_tokenized'] = tokenize_sentences(train_df_EM_paired['informal'])\n",
    "train_df_EM_paired['formal_tokenized'] = tokenize_sentences(train_df_EM_paired['formal'])\n",
    "\n",
    "# Tokenize both informal and formal sentences from Family Relationships\n",
    "train_df_FR_paired['informal_tokenized'] = tokenize_sentences(train_df_FR_paired['informal'])\n",
    "train_df_FR_paired['formal_tokenized'] = tokenize_sentences(train_df_FR_paired['formal'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            informal  \\\n",
      "0  The movie The In - Laws not exactly a holiday ...   \n",
      "1        That page did not give me viroses(i think )   \n",
      "2  Of corse i be wachin It Evry day, my fav chara...   \n",
      "3  Runescape.com ( my kids love it ) & funbrain.c...   \n",
      "4  Is he Gay?he was on Late Night with Conan O'br...   \n",
      "\n",
      "                                              formal  \\\n",
      "0  The In - Laws movie isn't a holiday movie, but...   \n",
      "1           I don't think that page gave me viruses.   \n",
      "2  I watch it everyday, my favorite charachter is...   \n",
      "3  Funbrain.com and runescape.com are great for f...   \n",
      "4  He was on the Late Night show with Conan O'bri...   \n",
      "\n",
      "                                  informal_tokenized  \\\n",
      "0  [The, movie, The, In, -, Laws, not, exactly, a...   \n",
      "1  [That, page, did, not, give, me, viroses, (, i...   \n",
      "2  [Of, corse, i, be, wachin, It, Evry, day, ,, m...   \n",
      "3  [Runescape.com, (, my, kids, love, it, ), &, f...   \n",
      "4  [Is, he, Gay, ?, he, was, on, Late, Night, wit...   \n",
      "\n",
      "                                    formal_tokenized  \n",
      "0  [The, In, -, Laws, movie, is, n't, a, holiday,...  \n",
      "1  [I, do, n't, think, that, page, gave, me, viru...  \n",
      "2  [I, watch, it, everyday, ,, my, favorite, char...  \n",
      "3  [Funbrain.com, and, runescape.com, are, great,...  \n",
      "4  [He, was, on, the, Late, Night, show, with, Co...  \n"
     ]
    }
   ],
   "source": [
    "print(train_df_EM_paired.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            informal  \\\n",
      "0  Sure, it's ok, but I always have let the guy a...   \n",
      "1  Hmmm, I'm a guy suffering from verbal abuse fr...   \n",
      "2        You will have more friends that you want.;)   \n",
      "3  It's nice, you get to see pictures of who you ...   \n",
      "4                           I NEED TO KNOW WHAT 2 DO   \n",
      "\n",
      "                                              formal  \\\n",
      "0                    I prefer to let the guy ask me.   \n",
      "1        I suffer through verbal abuse from my wife.   \n",
      "2          You will have more friends than you want.   \n",
      "3  It's nice that you get to see pictures of who ...   \n",
      "4                         I need to know what to do.   \n",
      "\n",
      "                                  informal_tokenized  \\\n",
      "0  [Sure, ,, it, 's, ok, ,, but, I, always, have,...   \n",
      "1  [Hmmm, ,, I, 'm, a, guy, suffering, from, verb...   \n",
      "2  [You, will, have, more, friends, that, you, wa...   \n",
      "3  [It, 's, nice, ,, you, get, to, see, pictures,...   \n",
      "4                   [I, NEED, TO, KNOW, WHAT, 2, DO]   \n",
      "\n",
      "                                    formal_tokenized  \n",
      "0         [I, prefer, to, let, the, guy, ask, me, .]  \n",
      "1  [I, suffer, through, verbal, abuse, from, my, ...  \n",
      "2  [You, will, have, more, friends, than, you, wa...  \n",
      "3  [It, 's, nice, that, you, get, to, see, pictur...  \n",
      "4               [I, need, to, know, what, to, do, .]  \n"
     ]
    }
   ],
   "source": [
    "print(train_df_FR_paired.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
