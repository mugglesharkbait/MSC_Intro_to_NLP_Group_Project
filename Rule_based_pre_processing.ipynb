{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk import word_tokenize, pos_tag\n",
    "import contractions\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sg/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/sg/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to capitalize the first letter of each sentence and proper nouns\n",
    "def capitalize_first_and_proper_nouns(text):\n",
    "    # Tokenize the text using NLTK\n",
    "    tokens = word_tokenize(text)\n",
    "    # Part-of-speech tagging using NLTK\n",
    "    tagged = pos_tag(tokens)\n",
    "    \n",
    "    # Load spaCy for proper noun recognition\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "\n",
    "    result = []\n",
    "    \n",
    "    for i in range(len(tagged)):\n",
    "        # Capitalize the first letter of each sentence\n",
    "        if i == 0 or tagged[i-1][0] in ['.', '!', '?']:\n",
    "            result.append(tagged[i][0].capitalize())\n",
    "        # Capitalize the first letter of proper nouns\n",
    "        elif doc[i].pos_ == 'PROPN':\n",
    "            result.append(tagged[i][0].capitalize())\n",
    "        else:\n",
    "            result.append(tagged[i][0])\n",
    "\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sample sentence . John and Mary went to the park . The park was beautiful .\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_text = \"this is a sample sentence. john and mary went to the park. the park was beautiful.\"\n",
    "result_text = capitalize_first_and_proper_nouns(input_text)\n",
    "print(result_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.3)\n",
      "Requirement already satisfied: jinja2 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.14.6)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.11.17)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How are you? I hope you're doing well.\n"
     ]
    }
   ],
   "source": [
    "# Function to remove repeated punctuations\n",
    "def remove_repeated_punctuations(sentence):\n",
    "    # Use regular expression to remove consecutive repeated punctuations\n",
    "    cleaned_sentence = re.sub(r'(\\W)\\1+', r'\\1', sentence)\n",
    "    return cleaned_sentence\n",
    "\n",
    "# Example usage:\n",
    "sentence1 = \"Hello!!! How are you?? I hope you''re doing well.....\"\n",
    "cleaned_sentence1 = remove_repeated_punctuations(sentence1)\n",
    "print(cleaned_sentence1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to expand contractions\n",
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a student and I have a test tomorrow.\n"
     ]
    }
   ],
   "source": [
    "text = \"I'm a student and I've a test tomorrow.\"\n",
    "expanded_text = expand_contractions(text)\n",
    "print(expanded_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a tokenization function\n",
    "def tokenize_sentences(sentences):\n",
    "    return [word_tokenize(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_general_spacing(sentence):\n",
    "    # Fix space before punctuation (like ' ,' to ',')\n",
    "    sentence = re.sub(r'\\s([,.?!:;])', r'\\1', sentence)\n",
    "    # Fix space after punctuation (like ' . ' to '. ')\n",
    "    sentence = re.sub(r'([,.?!:;])\\s', r'\\1 ', sentence)\n",
    "    # Fix space in contractions (like \"don 't\" to \"don't\")\n",
    "    sentence = re.sub(r\"\\b(\\w+)\\s('t|'s|'m|'ll|'ve|'re|'d|n't)\\b\", r\"\\1\\2\", sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    # text = text.lower()\n",
    "    text = capitalize_first_and_proper_nouns(text)\n",
    "    text = expand_contractions(text)\n",
    "    text = remove_repeated_punctuations(text)\n",
    "    text = fix_general_spacing(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_pair_data (train_file_EM_informal, train_file_EM_formal):\n",
    "    # Read the informal and formal sentences from the provided text files\n",
    "    with open(train_file_EM_informal, 'r', encoding='utf-8') as file:\n",
    "        informal_sentences = file.readlines()\n",
    "\n",
    "    with open(train_file_EM_formal, 'r', encoding='utf-8') as file:\n",
    "        formal_sentences = file.readlines()\n",
    "\n",
    "    # Preprocess the data \n",
    "    informal_sentences = [preprocess(text) for text in informal_sentences]\n",
    "    formal_sentences = [preprocess(text) for text in formal_sentences]\n",
    "\n",
    "    # Create dataframes from the sentences lists\n",
    "    df_informal = pd.DataFrame({'informal': informal_sentences})\n",
    "    df_formal = pd.DataFrame({'formal': formal_sentences})\n",
    "\n",
    "    # Strip whitespace from the beginning and end of sentences\n",
    "    df_informal['informal'] = df_informal['informal'].str.strip()\n",
    "    df_formal['formal'] = df_formal['formal'].str.strip()\n",
    "\n",
    "    # Assuming that each line corresponds to a sentence pair, we can concatenate the dataframes\n",
    "    df_paired = pd.concat([df_informal, df_formal], axis=1)\n",
    "\n",
    "    return df_paired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "[E040] Attempt to access token at 10, max length 10.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m train_file_FR_formal \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./GYAFC_Corpus/Family_Relationships/train/formal\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Get preprocessed dataframes\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m train_df_EM_paired \u001b[38;5;241m=\u001b[39m \u001b[43mread_and_pair_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_file_EM_informal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_file_EM_formal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m train_df_FR_paired \u001b[38;5;241m=\u001b[39m read_and_pair_data(train_file_FR_informal, train_file_FR_formal)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Tokenize both informal and formal sentences from Entertainment Music\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[29], line 10\u001b[0m, in \u001b[0;36mread_and_pair_data\u001b[0;34m(train_file_EM_informal, train_file_EM_formal)\u001b[0m\n\u001b[1;32m      7\u001b[0m     formal_sentences \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Preprocess the data \u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m informal_sentences \u001b[38;5;241m=\u001b[39m [preprocess(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m informal_sentences]\n\u001b[1;32m     11\u001b[0m formal_sentences \u001b[38;5;241m=\u001b[39m [preprocess(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m formal_sentences]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Create dataframes from the sentences lists\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[29], line 10\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m     formal_sentences \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Preprocess the data \u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m informal_sentences \u001b[38;5;241m=\u001b[39m [\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m informal_sentences]\n\u001b[1;32m     11\u001b[0m formal_sentences \u001b[38;5;241m=\u001b[39m [preprocess(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m formal_sentences]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Create dataframes from the sentences lists\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[28], line 3\u001b[0m, in \u001b[0;36mpreprocess\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess\u001b[39m(text):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# text = text.lower()\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mcapitalize_first_and_proper_nouns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     text \u001b[38;5;241m=\u001b[39m expand_contractions(text)\n\u001b[1;32m      5\u001b[0m     text \u001b[38;5;241m=\u001b[39m remove_repeated_punctuations(text)\n",
      "Cell \u001b[0;32mIn[20], line 19\u001b[0m, in \u001b[0;36mcapitalize_first_and_proper_nouns\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     17\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(tagged[i][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcapitalize())\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Capitalize the first letter of proper nouns\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mdoc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mpos_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPROPN\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     20\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(tagged[i][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcapitalize())\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/spacy/tokens/doc.pyx:505\u001b[0m, in \u001b[0;36mspacy.tokens.doc.Doc.__getitem__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/spacy/tokens/token.pxd:25\u001b[0m, in \u001b[0;36mspacy.tokens.token.Token.cinit\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: [E040] Attempt to access token at 10, max length 10."
     ]
    }
   ],
   "source": [
    "# path to train data \n",
    "train_file_EM_informal = \"./GYAFC_Corpus/Entertainment_Music/train/informal\"\n",
    "train_file_EM_formal = \"./GYAFC_Corpus/Entertainment_Music/train/formal\"\n",
    "train_file_FR_informal = \"./GYAFC_Corpus/Family_Relationships/train/informal\"\n",
    "train_file_FR_formal = \"./GYAFC_Corpus/Family_Relationships/train/formal\"\n",
    "\n",
    "# Get preprocessed dataframes\n",
    "train_df_EM_paired = read_and_pair_data(train_file_EM_informal, train_file_EM_formal)\n",
    "train_df_FR_paired = read_and_pair_data(train_file_FR_informal, train_file_FR_formal)\n",
    "\n",
    "# Tokenize both informal and formal sentences from Entertainment Music\n",
    "train_df_EM_paired['informal_tokenized'] = tokenize_sentences(train_df_EM_paired['informal'])\n",
    "train_df_EM_paired['formal_tokenized'] = tokenize_sentences(train_df_EM_paired['formal'])\n",
    "\n",
    "# Tokenize both informal and formal sentences from Family Relationships\n",
    "train_df_FR_paired['informal_tokenized'] = tokenize_sentences(train_df_FR_paired['informal'])\n",
    "train_df_FR_paired['formal_tokenized'] = tokenize_sentences(train_df_FR_paired['formal'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            informal  \\\n",
      "0  The movie The In-Laws not exactly a holiday mo...   \n",
      "1      That page did not give me viroses ( i think )   \n",
      "2  Of corse i be wachin it evry day, my fav chara...   \n",
      "3  Runescape.com ( my kids love it ) & funbrain.c...   \n",
      "4  Is he gay? He was on Late Night with Conan O'B...   \n",
      "\n",
      "                                              formal  \\\n",
      "0  The In-Laws movie isn't a holiday movie, but i...   \n",
      "1           I don't think that page gave me viruses.   \n",
      "2  I watch it everyday, my favorite charachter is...   \n",
      "3  Funbrain.com and runescape.com are great for f...   \n",
      "4  He was on the Late Night show with Conan O'Bri...   \n",
      "\n",
      "                                  informal_tokenized  \\\n",
      "0  [The, movie, The, In-Laws, not, exactly, a, ho...   \n",
      "1  [That, page, did, not, give, me, viroses, (, i...   \n",
      "2  [Of, corse, i, be, wachin, it, evry, day, ,, m...   \n",
      "3  [Runescape.com, (, my, kids, love, it, ), &, f...   \n",
      "4  [Is, he, gay, ?, He, was, on, Late, Night, wit...   \n",
      "\n",
      "                                    formal_tokenized  \n",
      "0  [The, In-Laws, movie, is, n't, a, holiday, mov...  \n",
      "1  [I, do, n't, think, that, page, gave, me, viru...  \n",
      "2  [I, watch, it, everyday, ,, my, favorite, char...  \n",
      "3  [Funbrain.com, and, runescape.com, are, great,...  \n",
      "4  [He, was, on, the, Late, Night, show, with, Co...  \n"
     ]
    }
   ],
   "source": [
    "print(train_df_EM_paired.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            informal  \\\n",
      "0  Sure, it's ok, but I always have let the guy a...   \n",
      "1  Hmmm, I'm a guy suffering from verbal abuse fr...   \n",
      "2       You will have more friends that you want.; )   \n",
      "3  It's nice, you get to see pictures of who you ...   \n",
      "4                           I NEED TO KNOW WHAT 2 DO   \n",
      "\n",
      "                                              formal  \\\n",
      "0                    I prefer to let the guy ask me.   \n",
      "1        I suffer through verbal abuse from my wife.   \n",
      "2          You will have more friends than you want.   \n",
      "3  It's nice that you get to see pictures of who ...   \n",
      "4                         I need to know what to do.   \n",
      "\n",
      "                                  informal_tokenized  \\\n",
      "0  [Sure, ,, it, 's, ok, ,, but, I, always, have,...   \n",
      "1  [Hmmm, ,, I, 'm, a, guy, suffering, from, verb...   \n",
      "2  [You, will, have, more, friends, that, you, wa...   \n",
      "3  [It, 's, nice, ,, you, get, to, see, pictures,...   \n",
      "4                   [I, NEED, TO, KNOW, WHAT, 2, DO]   \n",
      "\n",
      "                                    formal_tokenized  \n",
      "0         [I, prefer, to, let, the, guy, ask, me, .]  \n",
      "1  [I, suffer, through, verbal, abuse, from, my, ...  \n",
      "2  [You, will, have, more, friends, than, you, wa...  \n",
      "3  [It, 's, nice, that, you, get, to, see, pictur...  \n",
      "4               [I, need, to, know, what, to, do, .]  \n"
     ]
    }
   ],
   "source": [
    "print(train_df_FR_paired.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
