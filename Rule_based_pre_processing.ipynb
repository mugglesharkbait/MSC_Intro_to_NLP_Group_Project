{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk import word_tokenize, pos_tag\n",
    "import contractions\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sg/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/sg/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.3)\n",
      "Requirement already satisfied: jinja2 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.14.6)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.11.17)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy model outside of the function to avoid reloading it each time the function is called\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to capitalize the first letter of each sentence and proper nouns\n",
    "def capitalize_first_and_proper_nouns(text):\n",
    "    # Process the text using spaCy to create a Doc object\n",
    "    doc = nlp(text)\n",
    "\n",
    "    result = []\n",
    "\n",
    "    # Iterate over the sentences in the Doc\n",
    "    for sent in doc.sents:\n",
    "        # Iterate over the tokens in the sentence\n",
    "        for token in sent:\n",
    "            # Capitalize the first letter of each sentence and proper nouns\n",
    "            if token.is_sent_start or token.pos_ == 'PROPN':\n",
    "                result.append(token.text.capitalize())\n",
    "            else:\n",
    "                result.append(token.text)\n",
    "\n",
    "    # Rejoin the tokens into a single string\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sample sentence . John and Mary went to the park . The park was beautiful .\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_text = \"this is a sample sentence. john and mary went to the park. the park was beautiful.\"\n",
    "result_text = capitalize_first_and_proper_nouns(input_text)\n",
    "print(result_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How are you? I hope you're doing well.\n"
     ]
    }
   ],
   "source": [
    "# Function to remove repeated punctuations\n",
    "def remove_repeated_punctuations(sentence):\n",
    "    # Use regular expression to remove consecutive repeated punctuations\n",
    "    cleaned_sentence = re.sub(r'(\\W)\\1+', r'\\1', sentence)\n",
    "    return cleaned_sentence\n",
    "\n",
    "# Example usage:\n",
    "sentence1 = \"Hello!!! How are you?? I hope you''re doing well.....\"\n",
    "cleaned_sentence1 = remove_repeated_punctuations(sentence1)\n",
    "print(cleaned_sentence1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to expand contractions\n",
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure,, I am a student and it is ok, but I always have let the guy ask me....\n"
     ]
    }
   ],
   "source": [
    "# text = \"I'm a student and I've a test tomorrow.\"\n",
    "text = \"Sure,, I'm a student and it's ok, but I always have let the guy ask me....\"\n",
    "expanded_text = expand_contractions(text)\n",
    "print(expanded_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a tokenization function\n",
    "def tokenize_sentences(sentences):\n",
    "    return [word_tokenize(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_general_spacing(sentence):\n",
    "    # Fix space before punctuation (like ' ,' to ',')\n",
    "    sentence = re.sub(r'\\s([,.?!:;])', r'\\1', sentence)\n",
    "    # Fix space after punctuation (like ' . ' to '. ')\n",
    "    sentence = re.sub(r'([,.?!:;])\\s', r'\\1 ', sentence)\n",
    "    # Fix space in contractions (like \"don 't\" to \"don't\")\n",
    "    sentence = re.sub(r\"\\b(\\w+)\\s('t|'s|'m|'ll|'ve|'re|'d|n't)\\b\", r\"\\1\\2\", sentence)\n",
    "    # Reduce multiple spaces between words to a single space\n",
    "    sentence = re.sub(r'\\s{2,}', ' ', sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    # text = text.lower()\n",
    "    text = expand_contractions(text)\n",
    "    text = remove_repeated_punctuations(text)\n",
    "    text = capitalize_first_and_proper_nouns(text)\n",
    "    text = fix_general_spacing(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, I am a student and it is ok, but I always have let the guy ask me.\n"
     ]
    }
   ],
   "source": [
    "# text = \"I'm a student and I've a test tomorrow.\"\n",
    "text = \"Sure,, I'm a student and it's ok, but I always have let the guy ask me....\"\n",
    "expanded_text = preprocess(text)\n",
    "print(expanded_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_pair_data (train_file_EM_informal, train_file_EM_formal):\n",
    "    # Read the informal and formal sentences from the provided text files\n",
    "    with open(train_file_EM_informal, 'r', encoding='utf-8') as file:\n",
    "        informal_sentences = file.readlines()\n",
    "\n",
    "    with open(train_file_EM_formal, 'r', encoding='utf-8') as file:\n",
    "        formal_sentences = file.readlines()\n",
    "\n",
    "    # Preprocess the data \n",
    "    informal_sentences = [preprocess(text) for text in informal_sentences]\n",
    "    formal_sentences = [preprocess(text) for text in formal_sentences]\n",
    "\n",
    "    # Create dataframes from the sentences lists\n",
    "    df_informal = pd.DataFrame({'informal': informal_sentences})\n",
    "    df_formal = pd.DataFrame({'formal': formal_sentences})\n",
    "\n",
    "    # Strip whitespace from the beginning and end of sentences\n",
    "    df_informal['informal'] = df_informal['informal'].str.strip()\n",
    "    df_formal['formal'] = df_formal['formal'].str.strip()\n",
    "\n",
    "    # Assuming that each line corresponds to a sentence pair, we can concatenate the dataframes\n",
    "    df_paired = pd.concat([df_informal, df_formal], axis=1)\n",
    "\n",
    "    return df_paired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to train data \n",
    "train_file_EM_informal = \"./GYAFC_Corpus/Entertainment_Music/train/informal\"\n",
    "train_file_EM_formal = \"./GYAFC_Corpus/Entertainment_Music/train/formal\"\n",
    "train_file_FR_informal = \"./GYAFC_Corpus/Family_Relationships/train/informal\"\n",
    "train_file_FR_formal = \"./GYAFC_Corpus/Family_Relationships/train/formal\"\n",
    "\n",
    "# Get preprocessed dataframes\n",
    "train_df_EM_paired = read_and_pair_data(train_file_EM_informal, train_file_EM_formal)\n",
    "train_df_FR_paired = read_and_pair_data(train_file_FR_informal, train_file_FR_formal)\n",
    "\n",
    "# Tokenize both informal and formal sentences from Entertainment Music\n",
    "train_df_EM_paired['informal_tokenized'] = tokenize_sentences(train_df_EM_paired['informal'])\n",
    "train_df_EM_paired['formal_tokenized'] = tokenize_sentences(train_df_EM_paired['formal'])\n",
    "\n",
    "# Tokenize both informal and formal sentences from Family Relationships\n",
    "train_df_FR_paired['informal_tokenized'] = tokenize_sentences(train_df_FR_paired['informal'])\n",
    "train_df_FR_paired['formal_tokenized'] = tokenize_sentences(train_df_FR_paired['formal'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            informal  \\\n",
      "0  The movie The In - Laws not exactly a holiday ...   \n",
      "1        That page did not give me viroses(i think )   \n",
      "2  Of corse i be wachin It Evry day, my fav chara...   \n",
      "3  Runescape.com ( my kids love it ) & funbrain.c...   \n",
      "4  Is he Gay?he was on Late Night with Conan O'br...   \n",
      "\n",
      "                                              formal  \\\n",
      "0  The In - Laws movie is not a holiday movie, bu...   \n",
      "1          I do not think that page gave me viruses.   \n",
      "2  I watch it everyday, my favorite charachter is...   \n",
      "3  Funbrain.com and runescape.com are great for f...   \n",
      "4  He was on the Late Night show with Conan O'bri...   \n",
      "\n",
      "                                  informal_tokenized  \\\n",
      "0  [The, movie, The, In, -, Laws, not, exactly, a...   \n",
      "1  [That, page, did, not, give, me, viroses, (, i...   \n",
      "2  [Of, corse, i, be, wachin, It, Evry, day, ,, m...   \n",
      "3  [Runescape.com, (, my, kids, love, it, ), &, f...   \n",
      "4  [Is, he, Gay, ?, he, was, on, Late, Night, wit...   \n",
      "\n",
      "                                    formal_tokenized  \n",
      "0  [The, In, -, Laws, movie, is, not, a, holiday,...  \n",
      "1  [I, do, not, think, that, page, gave, me, viru...  \n",
      "2  [I, watch, it, everyday, ,, my, favorite, char...  \n",
      "3  [Funbrain.com, and, runescape.com, are, great,...  \n",
      "4  [He, was, on, the, Late, Night, show, with, Co...  \n"
     ]
    }
   ],
   "source": [
    "print(train_df_EM_paired.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            informal  \\\n",
      "0  Sure, it is ok, but I always have let the guy ...   \n",
      "1  Hmmm, I am a guy suffering from verbal abuse f...   \n",
      "2        You will have more friends that you want.;)   \n",
      "3  It is nice, you get to see pictures of who you...   \n",
      "4                           I NEED TO KNOW WHAT 2 DO   \n",
      "\n",
      "                                              formal  \\\n",
      "0                    I prefer to let the guy ask me.   \n",
      "1        I suffer through verbal abuse from my wife.   \n",
      "2          You will have more friends than you want.   \n",
      "3  It is nice that you get to see pictures of who...   \n",
      "4                         I need to know what to do.   \n",
      "\n",
      "                                  informal_tokenized  \\\n",
      "0  [Sure, ,, it, is, ok, ,, but, I, always, have,...   \n",
      "1  [Hmmm, ,, I, am, a, guy, suffering, from, verb...   \n",
      "2  [You, will, have, more, friends, that, you, wa...   \n",
      "3  [It, is, nice, ,, you, get, to, see, pictures,...   \n",
      "4                   [I, NEED, TO, KNOW, WHAT, 2, DO]   \n",
      "\n",
      "                                    formal_tokenized  \n",
      "0         [I, prefer, to, let, the, guy, ask, me, .]  \n",
      "1  [I, suffer, through, verbal, abuse, from, my, ...  \n",
      "2  [You, will, have, more, friends, than, you, wa...  \n",
      "3  [It, is, nice, that, you, get, to, see, pictur...  \n",
      "4               [I, need, to, know, what, to, do, .]  \n"
     ]
    }
   ],
   "source": [
    "print(train_df_FR_paired.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'In', '-', 'Laws', 'movie', 'is', 'not', 'a', 'holiday', 'movie', ',', 'but', 'it', 'is', 'okay', '.']\n"
     ]
    }
   ],
   "source": [
    "print(train_df_EM_paired['formal_tokenized'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating JSON file for the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "train_ds = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = open(\"./GYAFC_Corpus/Family_Relationships/train/informal\", 'r')\n",
    "f2 = open(\"./GYAFC_Corpus/Family_Relationships/train/formal\", 'r')\n",
    "\n",
    "id = 0\n",
    "\n",
    "while True:\n",
    "  line1 = f1.readline().rstrip()\n",
    "  line2 = f2.readline().rstrip()\n",
    "  if not line1:\n",
    "    break\n",
    "\n",
    "  rule_based_preprocessed1 = preprocess(line1)\n",
    "\n",
    "  train_ds.append(\n",
    "      {\n",
    "          'id':id,\n",
    "          'topic':'Family_Relationships',\n",
    "          'transformation':{\n",
    "              'informal':line1,\n",
    "              'formal.ref0':line2,\n",
    "              'formal.ref1':\"\",\n",
    "              'formal.ref2':\"\",\n",
    "              'formal.ref3':\"\",\n",
    "              'rule_based_preprocessed':rule_based_preprocessed1,\n",
    "          }\n",
    "      }\n",
    "  )  # adding a row\n",
    "\n",
    "  id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = open(\"./GYAFC_Corpus/Entertainment_Music/train/informal\", 'r')\n",
    "f2 = open(\"./GYAFC_Corpus/Entertainment_Music/train/formal\", 'r')\n",
    "\n",
    "while True:\n",
    "  line1 = f1.readline().rstrip()\n",
    "  line2 = f2.readline().rstrip()\n",
    "  if not line1:\n",
    "    break\n",
    "\n",
    "  rule_based_preprocessed1 = preprocess(line1)\n",
    "\n",
    "  train_ds.append(\n",
    "      {\n",
    "          'id':id,\n",
    "          'topic':'Entertainment_Music',\n",
    "          'transformation':{\n",
    "              'informal':line1,\n",
    "              'formal.ref0':line2,\n",
    "              'formal.ref1':\"\",\n",
    "              'formal.ref2':\"\",\n",
    "              'formal.ref3':\"\",\n",
    "              'rule_based_preprocessed':rule_based_preprocessed1,\n",
    "          }\n",
    "      }\n",
    "  )\n",
    "\n",
    "  id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data_train_rule_based_preprocess.json\", 'w') as f:\n",
    "  json.dump(train_ds, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
