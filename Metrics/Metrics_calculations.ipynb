{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\nsadi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: sacrebleu in c:\\users\\nsadi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.4.0)\n",
      "Requirement already satisfied: click in c:\\users\\nsadi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\nsadi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\nsadi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nsadi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: portalocker in c:\\users\\nsadi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sacrebleu) (2.8.2)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in c:\\users\\nsadi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\nsadi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sacrebleu) (1.26.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\nsadi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in c:\\users\\nsadi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sacrebleu) (5.1.0)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\nsadi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from portalocker->sacrebleu) (306)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "%pip install nltk sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nsadi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import sacrebleu\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk import ngrams\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data\n",
    "reference_sentence = \"she was interested in world history because she read the book.\"\n",
    "hypothesis_sentence = \"she read the book because she was interested in world history.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Bilingual Evaluation Understudy (BLEU): BLEU score is calculated by measuring the overlap of n-grams between the candidate translation and the references. The calculation could be represented as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Higher is better\n",
    "def calculate_bleu(hypothesis: str, reference: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate BLEU score between a hypothesis and a reference.\n",
    "\n",
    "    Parameters:\n",
    "    - hypothesis (str): The generated hypothesis or translation.\n",
    "    - reference (str): The reference or ground truth translation.\n",
    "\n",
    "    Returns:\n",
    "    - float: The BLEU score, where higher values indicate better similarity to the reference - 0.0 (no similarity) and 1.0 (perfect similarity to the reference).\n",
    "    \"\"\"\n",
    "    hypothesis_split = hypothesis.split()\n",
    "    reference_split = reference.split()\n",
    "\n",
    "    # Using NLTK's sentence_bleu function to calculate BLEU score\n",
    "    bleu_score = sentence_bleu([reference_split], hypothesis_split)\n",
    "   \n",
    "    return bleu_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.502\n"
     ]
    }
   ],
   "source": [
    "bleu_score = calculate_bleu(hypothesis_sentence, reference_sentence)\n",
    "\n",
    "print(f\"BLEU Score: {bleu_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Translation Edit Rate Plus (TERp): TERp is calculated by measuring the matching flaw between machine-generated translations and human-created translation. The calculation could be represented as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Higher is better\n",
    "def calculate_terp(hypothesis: str, reference: str, phrase_table: dict = None, edit_costs: dict = None) -> float:\n",
    "    \"\"\"\n",
    "    Calculate TERp (Translation Edit Rate with partial credit) score between a hypothesis and a reference.\n",
    "\n",
    "    Parameters:\n",
    "    - hypothesis (str): The generated hypothesis or translation.\n",
    "    - reference (str): The reference or ground truth translation.\n",
    "    - phrase_table (dict, optional): A dictionary representing a phrase table for paraphrase information.\n",
    "    - edit_costs (dict, optional): A dictionary containing weights for edit operations in the TERp calculation.\n",
    "\n",
    "    Returns:\n",
    "    - float: The TERp score, a value indicating the similarity between the hypothesis and the reference,\n",
    "             where higher values are better.\n",
    "    \"\"\"\n",
    "    hypothesis_tokens = hypothesis.split()\n",
    "    reference_tokens = reference.split()\n",
    "\n",
    "    # TERp by Stem Matches, Synonym Matches, and Phrase Substitutions\n",
    "    stem_matches = calculate_stem_matches(hypothesis_tokens, reference_tokens)\n",
    "    synonym_matches = calculate_synonym_matches(hypothesis_tokens, reference_tokens)\n",
    "\n",
    "    phrase_substitutions = calculate_phrase_substitutions(hypothesis_tokens, reference_tokens, phrase_table, edit_costs) if phrase_table is not None and edit_costs is not None else 0\n",
    "\n",
    "\n",
    "    # Calculate TERp score\n",
    "    terp_score = (stem_matches + synonym_matches + phrase_substitutions) / (2*len(hypothesis_tokens))\n",
    "\n",
    "    return terp_score\n",
    "\n",
    "def calculate_stem_matches(hypothesis_tokens: list, reference_tokens: list) -> int:\n",
    "    \"\"\"\n",
    "    Calculate the number of stem matches between two tokenized sequences.\n",
    "\n",
    "    Parameters:\n",
    "    - hypothesis_tokens (list): List of tokens in the hypothesis.\n",
    "    - reference_tokens (list): List of tokens in the reference.\n",
    "\n",
    "    Returns:\n",
    "    - int: The number of stem matches.\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    stem_matches = sum(1 for hyp_token, ref_token in zip(hypothesis_tokens, reference_tokens)\n",
    "                      if stemmer.stem(hyp_token.lower()) == stemmer.stem(ref_token.lower()))\n",
    "    return stem_matches\n",
    "\n",
    "def calculate_synonym_matches(hypothesis_tokens: list, reference_tokens: list) -> int:\n",
    "    \"\"\"\n",
    "    Calculate the number of synonym matches between two tokenized sequences.\n",
    "\n",
    "    Parameters:\n",
    "    - hypothesis_tokens (list): List of tokens in the hypothesis.\n",
    "    - reference_tokens (list): List of tokens in the reference.\n",
    "\n",
    "    Returns:\n",
    "    - int: The number of synonym matches.\n",
    "    \"\"\"\n",
    "    synonym_matches = sum(1 for hyp_token, ref_token in zip(hypothesis_tokens, reference_tokens)\n",
    "                          if are_synonyms(hyp_token.lower(), ref_token.lower()))\n",
    "    return synonym_matches\n",
    "\n",
    "def are_synonyms(word1: str, word2: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if two words are synonyms.\n",
    "\n",
    "    Parameters:\n",
    "    - word1 (str): The first word.\n",
    "    - word2 (str): The second word.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if the words are synonyms, False otherwise.\n",
    "    \"\"\"\n",
    "    synsets1 = wordnet.synsets(word1)\n",
    "    synsets2 = wordnet.synsets(word2)\n",
    "    \n",
    "    return any(set1.wup_similarity(set2) > 0.7 for set1 in synsets1 for set2 in synsets2)\n",
    "\n",
    "def calculate_phrase_substitutions(hypothesis_tokens: list, reference_tokens: list, phrase_table: dict, edit_costs: dict) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the cost of phrase substitutions between two tokenized sequences.\n",
    "\n",
    "    Parameters:\n",
    "    - hypothesis_tokens (list): List of tokens in the hypothesis.\n",
    "    - reference_tokens (list): List of tokens in the reference.\n",
    "    - phrase_table (dict): A dictionary representing a phrase table for paraphrase information.\n",
    "    - edit_costs (dict): A dictionary containing weights for edit operations in the TERp calculation.\n",
    "\n",
    "    Returns:\n",
    "    - float: The total cost of phrase substitutions.\n",
    "    \"\"\"\n",
    "    substitution_cost = 0\n",
    "\n",
    "    for i in range(len(hypothesis_tokens)):\n",
    "        for j in range(len(reference_tokens)):\n",
    "            if (hypothesis_tokens[i], reference_tokens[j]) in phrase_table:\n",
    "                # Retrieve paraphrase information from the phrase table\n",
    "                paraphrase_info = phrase_table[(hypothesis_tokens[i], reference_tokens[j])]\n",
    "                \n",
    "                # Calculate the cost using the provided formula\n",
    "                cost = (\n",
    "                    edit_costs['w1'] +\n",
    "                    edit_costs['w2'] * paraphrase_info['edit'] * math.log(paraphrase_info['probability']) +\n",
    "                    edit_costs['w3'] * paraphrase_info['edit'] * paraphrase_info['probability'] +\n",
    "                    edit_costs['w4'] * paraphrase_info['edit']\n",
    "                )\n",
    "\n",
    "                # Ensure the substitution cost is not negative\n",
    "                substitution_cost += max(0, cost)\n",
    "\n",
    "    return substitution_cost\n",
    "\n",
    "def terp_alignment(hypothesis: str, reference: str, phrase_table: dict = None, edit_costs: dict = None) -> list:\n",
    "    \"\"\"\n",
    "    Generate a word-level alignment between a hypothesis and a reference.\n",
    "\n",
    "    Parameters:\n",
    "    - hypothesis (str): The generated hypothesis or translation.\n",
    "    - reference (str): The reference or ground truth translation.\n",
    "    - phrase_table (dict, optional): A dictionary representing a phrase table for paraphrase information.\n",
    "    - edit_costs (dict, optional): A dictionary containing weights for edit operations in the TERp calculation.\n",
    "\n",
    "    Returns:\n",
    "    - list of tuples: A list of tuples representing the word-level alignment, each tuple contains\n",
    "                      (hypothesis_token, reference_token, alignment_type).\n",
    "    \"\"\"\n",
    "    alignment = []\n",
    "\n",
    "    for hyp_token, ref_token in zip(hypothesis.split(), reference.split()):\n",
    "        if hyp_token == ref_token:\n",
    "            alignment.append((hyp_token, ref_token, \"Exact Match\"))\n",
    "        else:\n",
    "            alignment.append((hyp_token, ref_token, \"Mismatch\"))\n",
    "\n",
    "    return alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TERp Score: 0.8\n",
      "Alignment: [('This', 'This', 'Exact Match'), ('is', 'is', 'Exact Match'), ('an', 'an', 'Exact Match'), ('example', 'example', 'Exact Match'), ('sentence.', 'sentence.', 'Exact Match')]\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "hypothesis_sentence = \"This is an example sentence.\"\n",
    "reference_sentence = \"This is an example sentence.\"\n",
    "\n",
    "# Calculate TERp score\n",
    "terp_score = calculate_terp(hypothesis_sentence, reference_sentence)\n",
    "print(f\"TERp Score: {terp_score}\")\n",
    "\n",
    "# Generate alignment\n",
    "alignment = terp_alignment(hypothesis_sentence, reference_sentence)\n",
    "print(\"Alignment:\", alignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower is better\n",
    "def calculate_ter(hypothesis: str, reference: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate TER (Translation Edit Rate) score between a hypothesis and a reference.\n",
    "\n",
    "    Parameters:\n",
    "    - hypothesis (str): The generated hypothesis or translation.\n",
    "    - reference (str): The reference or ground truth translation.\n",
    "\n",
    "    Returns:\n",
    "    - float: The TER score, where lower values indicate better similarity to the reference.\n",
    "    \"\"\"\n",
    "    # Tokenize the input sentences into lists of words\n",
    "    hypothesis_tokens = hypothesis.split()\n",
    "    reference_tokens = reference.split()\n",
    "\n",
    "    # Compute Levenshtein distance between hypothesis and reference\n",
    "    distance = levenshtein_distance(hypothesis_tokens, reference_tokens)\n",
    "\n",
    "    # Compute TER score\n",
    "    ter_score = distance / len(reference_tokens)\n",
    "    \n",
    "    return ter_score\n",
    "\n",
    "def levenshtein_distance(s1: list, s2: list) -> int:\n",
    "    \"\"\"\n",
    "    Compute the Levenshtein distance between two sequences.\n",
    "\n",
    "    Parameters:\n",
    "    - s1 (list): List of tokens in the first sequence.\n",
    "    - s2 (list): List of tokens in the second sequence.\n",
    "\n",
    "    Returns:\n",
    "    - int: The Levenshtein distance between the two sequences.\n",
    "    \"\"\"\n",
    "    # Initialize a matrix to store the distances\n",
    "    matrix = [[0] * (len(s2) + 1) for _ in range(len(s1) + 1)]\n",
    "\n",
    "    # Initialize the first row and column\n",
    "    for i in range(len(s1) + 1):\n",
    "        matrix[i][0] = i\n",
    "    for j in range(len(s2) + 1):\n",
    "        matrix[0][j] = j\n",
    "\n",
    "    # Fill in the matrix\n",
    "    for i in range(1, len(s1) + 1):\n",
    "        for j in range(1, len(s2) + 1):\n",
    "            cost = 0 if s1[i - 1] == s2[j - 1] else 1\n",
    "            matrix[i][j] = min(\n",
    "                matrix[i - 1][j] + 1,  # Deletion\n",
    "                matrix[i][j - 1] + 1,  # Insertion\n",
    "                matrix[i - 1][j - 1] + cost  # Substitution\n",
    "            )\n",
    "\n",
    "    # Return the final edit distance\n",
    "    return matrix[len(s1)][len(s2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TER Score: 0.4\n"
     ]
    }
   ],
   "source": [
    "ter_score = calculate_ter(hypothesis_sentence, reference_sentence)\n",
    "\n",
    "print(f\"TER Score: {ter_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Paraphrase In N-gram Changes (PINC): PINC is calculated by computing the percentage of n-grams that appear in the candidate sentence but not in the source sentence. The calculation could be represented as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower is better\n",
    "def calculate_pinc(hypothesis: str, reference: str, n: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate PINC (Precision-based n-gram Inclusion Count) score between a hypothesis and a reference.\n",
    "\n",
    "    Parameters:\n",
    "    - hypothesis (str): The generated hypothesis or translation.\n",
    "    - reference (str): The reference or ground truth translation.\n",
    "    - n (int): The size of n-grams for which to calculate the PINC score.\n",
    "\n",
    "    Returns:\n",
    "    - float: The PINC score, where lower values indicate better similarity to the reference.\n",
    "    \"\"\"\n",
    "    hypothesis_split = hypothesis.split()\n",
    "    reference_split = reference.split()\n",
    "\n",
    "    hypothesis_ngrams = set(ngrams(hypothesis_split, n))\n",
    "    reference_ngrams = set(ngrams(reference_split, n))\n",
    "    new_ngrams = hypothesis_ngrams - reference_ngrams\n",
    "    hypo_ngram_len = len(hypothesis_ngrams)\n",
    "    pinc_score = len(new_ngrams) / hypo_ngram_len if hypo_ngram_len != 0 else 0\n",
    "\n",
    "    return pinc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PINC Score: 0.75\n"
     ]
    }
   ],
   "source": [
    "pinc_score = calculate_pinc(hypothesis_sentence, reference_sentence, 2)\n",
    "\n",
    "print(f\"PINC Score: {pinc_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
