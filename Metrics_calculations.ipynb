{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\nsadi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: sacrebleu in c:\\users\\nsadi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.4.0)\n",
      "Requirement already satisfied: click in c:\\users\\nsadi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\nsadi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\nsadi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nsadi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: portalocker in c:\\users\\nsadi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sacrebleu) (2.8.2)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in c:\\users\\nsadi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\nsadi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sacrebleu) (1.26.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\nsadi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in c:\\users\\nsadi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sacrebleu) (5.1.0)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\nsadi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from portalocker->sacrebleu) (306)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "%pip install nltk sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nsadi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import sacrebleu\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk import ngrams\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data\n",
    "reference_sentence = \"she was interested in world history because she read the book.\"\n",
    "hypothesis_sentence = \"she read the book because she was interested in world history.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Formality: This metric is calculated by evaluating a set of transformed texts against standard criteria for formality and calculating the percentage of texts that meet these criteria. The calculation could be represented as:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def calculate_formality(hypothesis: str, reference: str):\n",
    "# #     # formal_texts = [text for text in transformed_texts if meets_criteria(text, standard_criteria)]\n",
    "# #     # formality_score = len(formal_texts) / len(transformed_texts)\n",
    "\n",
    "# #     return formality_score\n",
    "\n",
    "# from transformers import BertTokenizer, BertForSequenceClassification\n",
    "# import torch\n",
    "\n",
    "# # Load pre-trained BERT model and tokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "# model.eval()\n",
    "\n",
    "# def your_formality_classifier(sentence: str) -> float:\n",
    "#     \"\"\"\n",
    "#     Advanced implementation of a formality classifier using BERT.\n",
    "#     Replace this with your actual implementation.\n",
    "\n",
    "#     Parameters:\n",
    "#     - sentence (str): The input sentence to classify.\n",
    "\n",
    "#     Returns:\n",
    "#     - formality_score (float): The formality score for the sentence.\n",
    "#     \"\"\"\n",
    "#     # Tokenize and convert to model input format\n",
    "#     inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    \n",
    "#     # Forward pass through the model\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "\n",
    "#     # Extract logits and apply softmax to get probabilities\n",
    "#     logits = outputs.logits\n",
    "#     probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
    "\n",
    "#     # Assuming the model is trained for binary classification (formal vs. informal)\n",
    "#     # You may need to adapt this based on your specific classification setup\n",
    "#     formal_probability = probabilities[0, 1].item()\n",
    "\n",
    "#     # You may need to fine-tune these thresholds based on your data\n",
    "#     formality_threshold_high = 0.6\n",
    "#     formality_threshold_low = 0.4\n",
    "\n",
    "#     print(formal_probability)\n",
    "\n",
    "#     # Classify based on probability thresholds\n",
    "#     # if formal_probability > formality_threshold_high:\n",
    "#     #     formality_score = 1.0  # More formal\n",
    "#     # elif formal_probability < formality_threshold_low:\n",
    "#     #     formality_score = -1.0  # Less formal\n",
    "#     # else:\n",
    "#     #     formality_score = 0.0  # Neutral or in-between\n",
    "\n",
    "#     return formal_probability\n",
    "\n",
    "# # Example usage:\n",
    "# hypothesis_sentence = \"you so suppa\"\n",
    "# reference_sentence = \"I appreciate the image\"\n",
    "\n",
    "# formality_score = your_formality_classifier(hypothesis_sentence)\n",
    "# print(f\"Formality Score: {formality_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Bilingual Evaluation Understudy (BLEU): BLEU score is calculated by measuring the overlap of n-grams between the candidate translation and the references. The calculation could be represented as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Higher is better\n",
    "def calculate_bleu(hypothesis: str, reference: str):\n",
    "    hypothesis_split = hypothesis.split()\n",
    "    reference_split = reference.split()\n",
    "\n",
    "    bleu_score = sentence_bleu([reference_split], hypothesis_split)\n",
    "   \n",
    "    return bleu_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.000\n"
     ]
    }
   ],
   "source": [
    "bleu_score = calculate_bleu(hypothesis_sentence, reference_sentence)\n",
    "\n",
    "print(f\"BLEU Score: {bleu_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Translation Edit Rate Plus (TERp): TERp is calculated by measuring the matching flaw between machine-generated translations and human-created translation. The calculation could be represented as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Higher is better\n",
    "def calculate_terp(hypothesis: str, reference: str, phrase_table, edit_costs):\n",
    "    hypothesis_tokens = hypothesis.split()\n",
    "    reference_tokens = reference.split()\n",
    "\n",
    "    # TERp by Stem Matches, Synonym Matches, and Phrase Substitutions\n",
    "    stem_matches = calculate_stem_matches(hypothesis_tokens, reference_tokens)\n",
    "    synonym_matches = calculate_synonym_matches(hypothesis_tokens, reference_tokens)\n",
    "    phrase_substitutions = calculate_phrase_substitutions(hypothesis_tokens, reference_tokens, phrase_table, edit_costs)\n",
    "\n",
    "    # Calculate TERp score\n",
    "    terp_score = (stem_matches + synonym_matches + phrase_substitutions) / len(reference_tokens)\n",
    "\n",
    "    return terp_score\n",
    "\n",
    "def calculate_stem_matches(hypothesis_tokens, reference_tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    stem_matches = sum(1 for hyp_token, ref_token in zip(hypothesis_tokens, reference_tokens)\n",
    "                      if stemmer.stem(hyp_token.lower()) == stemmer.stem(ref_token.lower()))\n",
    "    return stem_matches\n",
    "\n",
    "def calculate_synonym_matches(hypothesis_tokens, reference_tokens):\n",
    "    synonym_matches = sum(1 for hyp_token, ref_token in zip(hypothesis_tokens, reference_tokens)\n",
    "                          if are_synonyms(hyp_token.lower(), ref_token.lower()))\n",
    "    return synonym_matches\n",
    "\n",
    "def are_synonyms(word1, word2):\n",
    "    synsets1 = wordnet.synsets(word1)\n",
    "    synsets2 = wordnet.synsets(word2)\n",
    "    \n",
    "    return any(set1.wup_similarity(set2) > 0.7 for set1 in synsets1 for set2 in synsets2)\n",
    "\n",
    "def calculate_phrase_substitutions(hypothesis_tokens, reference_tokens, phrase_table, edit_costs):\n",
    "    substitution_cost = 0\n",
    "\n",
    "    for i in range(len(hypothesis_tokens)):\n",
    "        for j in range(len(reference_tokens)):\n",
    "            if (hypothesis_tokens[i], reference_tokens[j]) in phrase_table:\n",
    "                # Retrieve paraphrase information from the phrase table\n",
    "                paraphrase_info = phrase_table[(hypothesis_tokens[i], reference_tokens[j])]\n",
    "                \n",
    "                # Calculate the cost using the provided formula\n",
    "                cost = (\n",
    "                    edit_costs['w1'] +\n",
    "                    edit_costs['w2'] * paraphrase_info['edit'] * math.log(paraphrase_info['probability']) +\n",
    "                    edit_costs['w3'] * paraphrase_info['edit'] * paraphrase_info['probability'] +\n",
    "                    edit_costs['w4'] * paraphrase_info['edit']\n",
    "                )\n",
    "\n",
    "                # Ensure the substitution cost is not negative\n",
    "                substitution_cost += max(0, cost)\n",
    "\n",
    "    return substitution_cost\n",
    "\n",
    "def terp_alignment(hypothesis, reference, phrase_table, edit_costs):\n",
    "    alignment = []\n",
    "\n",
    "    for hyp_token, ref_token in zip(hypothesis.split(), reference.split()):\n",
    "        if hyp_token == ref_token:\n",
    "            alignment.append((hyp_token, ref_token, \"Exact Match\"))\n",
    "        else:\n",
    "            alignment.append((hyp_token, ref_token, \"Mismatch\"))\n",
    "\n",
    "    return alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TERp Score: 1.0\n",
      "Alignment: [('This', 'This', 'Exact Match'), ('is', 'is', 'Exact Match'), ('an', 'a', 'Mismatch'), ('example', 'instance', 'Mismatch'), ('sentence.', 'sentence.', 'Exact Match')]\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "hypothesis_sentence = \"This is an example sentence.\"\n",
    "reference_sentence = \"This is a instance sentence.\"\n",
    "\n",
    "# Calculate TERp score\n",
    "terp_score = calculate_terp(hypothesis_sentence, reference_sentence)\n",
    "print(f\"TERp Score: {terp_score}\")\n",
    "\n",
    "# Generate alignment\n",
    "alignment = terp_alignment(hypothesis_sentence, reference_sentence)\n",
    "print(\"Alignment:\", alignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower is better\n",
    "def calculate_ter(hypothesis, reference):\n",
    "    # Tokenize the input sentences into lists of words\n",
    "    hypothesis_tokens = hypothesis.split()\n",
    "    reference_tokens = reference.split()\n",
    "\n",
    "    # Compute Levenshtein distance between hypothesis and reference\n",
    "    distance = levenshtein_distance(hypothesis_tokens, reference_tokens)\n",
    "\n",
    "    # Compute TER score\n",
    "    ter_score = distance / len(reference_tokens)\n",
    "    \n",
    "    return ter_score\n",
    "\n",
    "def levenshtein_distance(s1, s2):\n",
    "    # Initialize a matrix to store the distances\n",
    "    matrix = [[0] * (len(s2) + 1) for _ in range(len(s1) + 1)]\n",
    "\n",
    "    # Initialize the first row and column\n",
    "    for i in range(len(s1) + 1):\n",
    "        matrix[i][0] = i\n",
    "    for j in range(len(s2) + 1):\n",
    "        matrix[0][j] = j\n",
    "\n",
    "    # Fill in the matrix\n",
    "    for i in range(1, len(s1) + 1):\n",
    "        for j in range(1, len(s2) + 1):\n",
    "            cost = 0 if s1[i - 1] == s2[j - 1] else 1\n",
    "            matrix[i][j] = min(\n",
    "                matrix[i - 1][j] + 1,  # Deletion\n",
    "                matrix[i][j - 1] + 1,  # Insertion\n",
    "                matrix[i - 1][j - 1] + cost  # Substitution\n",
    "            )\n",
    "\n",
    "    # Return the final edit distance\n",
    "    return matrix[len(s1)][len(s2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TER Score: 0.25\n"
     ]
    }
   ],
   "source": [
    "ter_score = calculate_ter(hypothesis_sentence, reference_sentence)\n",
    "\n",
    "print(f\"TER Score: {ter_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Paraphrase In N-gram Changes (PINC): PINC is calculated by computing the percentage of n-grams that appear in the candidate sentence but not in the source sentence. The calculation could be represented as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PINC Score: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# Lower is better\n",
    "def calculate_pinc(hypothesis: str, reference: str, n: int):\n",
    "    hypothesis_split = hypothesis.split()\n",
    "    reference_split = reference.split()\n",
    "\n",
    "    hypothesis_ngrams = set(ngrams(hypothesis_split, n))\n",
    "    reference_ngrams = set(ngrams(reference_split, n))\n",
    "    new_ngrams = hypothesis_ngrams - reference_ngrams\n",
    "    pinc_score = len(new_ngrams) / len(hypothesis_ngrams)\n",
    "\n",
    "    return pinc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PINC Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "pinc_score = calculate_pinc(hypothesis_sentence, reference_sentence, 2)\n",
    "\n",
    "print(f\"PINC Score: {pinc_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
