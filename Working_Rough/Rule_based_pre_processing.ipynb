{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk import word_tokenize, pos_tag\n",
    "import contractions\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sg/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/sg/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.3)\n",
      "Requirement already satisfied: jinja2 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.14.6)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.11.17)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/sg/anaconda3/envs/myenv/lib/python3.9/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy model outside of the function to avoid reloading it each time the function is called\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to capitalize the first letter of each sentence and proper nouns\n",
    "def capitalize_first_and_proper_nouns(text):\n",
    "    # Process the text using spaCy to create a Doc object\n",
    "    doc = nlp(text)\n",
    "\n",
    "    result = []\n",
    "\n",
    "    # Iterate over the sentences in the Doc\n",
    "    for sent in doc.sents:\n",
    "        # Iterate over the tokens in the sentence\n",
    "        for token in sent:\n",
    "            # Capitalize the first letter of each sentence and proper nouns\n",
    "            if token.is_sent_start or token.pos_ == 'PROPN':\n",
    "                result.append(token.text.capitalize())\n",
    "            else:\n",
    "                result.append(token.text)\n",
    "\n",
    "    # Rejoin the tokens into a single string\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sample sentence . John and Mary went to the park . The park was beautiful .\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_text = \"this is a sample sentence. john and mary went to the park. the park was beautiful.\"\n",
    "result_text = capitalize_first_and_proper_nouns(input_text)\n",
    "print(result_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How are you? I hope you're doing well.\n"
     ]
    }
   ],
   "source": [
    "# Function to remove repeated punctuations\n",
    "def remove_repeated_punctuations(sentence):\n",
    "    # Use regular expression to remove consecutive repeated punctuations\n",
    "    cleaned_sentence = re.sub(r'(\\W)\\1+', r'\\1', sentence)\n",
    "    return cleaned_sentence\n",
    "\n",
    "# Example usage:\n",
    "sentence1 = \"Hello!!! How are you?? I hope you''re doing well.....\"\n",
    "cleaned_sentence1 = remove_repeated_punctuations(sentence1)\n",
    "print(cleaned_sentence1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to expand contractions\n",
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure,, I am a student and it is ok, but I always have let the guy ask me....\n"
     ]
    }
   ],
   "source": [
    "# text = \"I'm a student and I've a test tomorrow.\"\n",
    "text = \"Sure,, I'm a student and it's ok, but I always have let the guy ask me....\"\n",
    "expanded_text = expand_contractions(text)\n",
    "print(expanded_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a tokenization function\n",
    "def tokenize_sentences(sentences):\n",
    "    return [word_tokenize(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_general_spacing(sentence):\n",
    "    # Fix space before punctuation (like ' ,' to ',')\n",
    "    sentence = re.sub(r'\\s([,.?!:;])', r'\\1', sentence)\n",
    "    # Fix space after punctuation (like ' . ' to '. ')\n",
    "    sentence = re.sub(r'([,.?!:;])\\s', r'\\1 ', sentence)\n",
    "    # Fix space in contractions (like \"don 't\" to \"don't\")\n",
    "    sentence = re.sub(r\"\\b(\\w+)\\s('t|'s|'m|'ll|'ve|'re|'d|n't)\\b\", r\"\\1\\2\", sentence)\n",
    "    # Reduce multiple spaces between words to a single space\n",
    "    sentence = re.sub(r'\\s{2,}', ' ', sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    # text = text.lower()\n",
    "    text = expand_contractions(text)\n",
    "    text = remove_repeated_punctuations(text)\n",
    "    text = capitalize_first_and_proper_nouns(text)\n",
    "    text = fix_general_spacing(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, I am a student and it is ok, but I always have let the guy ask me.\n"
     ]
    }
   ],
   "source": [
    "# text = \"I'm a student and I've a test tomorrow.\"\n",
    "text = \"Sure,, I'm a student and it's ok, but I always have let the guy ask me....\"\n",
    "expanded_text = preprocess(text)\n",
    "print(expanded_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_pair_data (train_file_EM_informal, train_file_EM_formal):\n",
    "    # Read the informal and formal sentences from the provided text files\n",
    "    with open(train_file_EM_informal, 'r', encoding='utf-8') as file:\n",
    "        informal_sentences = file.readlines()\n",
    "\n",
    "    with open(train_file_EM_formal, 'r', encoding='utf-8') as file:\n",
    "        formal_sentences = file.readlines()\n",
    "\n",
    "    # Preprocess the data \n",
    "    informal_sentences = [preprocess(text) for text in informal_sentences]\n",
    "    formal_sentences = [preprocess(text) for text in formal_sentences]\n",
    "\n",
    "    # Create dataframes from the sentences lists\n",
    "    df_informal = pd.DataFrame({'informal': informal_sentences})\n",
    "    df_formal = pd.DataFrame({'formal': formal_sentences})\n",
    "\n",
    "    # Strip whitespace from the beginning and end of sentences\n",
    "    df_informal['informal'] = df_informal['informal'].str.strip()\n",
    "    df_formal['formal'] = df_formal['formal'].str.strip()\n",
    "\n",
    "    # Assuming that each line corresponds to a sentence pair, we can concatenate the dataframes\n",
    "    df_paired = pd.concat([df_informal, df_formal], axis=1)\n",
    "\n",
    "    return df_paired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m train_file_FR_formal \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./GYAFC_Corpus/Family_Relationships/train/formal\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Get preprocessed dataframes\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m train_df_EM_paired \u001b[38;5;241m=\u001b[39m \u001b[43mread_and_pair_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_file_EM_informal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_file_EM_formal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m train_df_FR_paired \u001b[38;5;241m=\u001b[39m read_and_pair_data(train_file_FR_informal, train_file_FR_formal)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Tokenize both informal and formal sentences from Entertainment Music\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 10\u001b[0m, in \u001b[0;36mread_and_pair_data\u001b[0;34m(train_file_EM_informal, train_file_EM_formal)\u001b[0m\n\u001b[1;32m      7\u001b[0m     formal_sentences \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Preprocess the data \u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m informal_sentences \u001b[38;5;241m=\u001b[39m [preprocess(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m informal_sentences]\n\u001b[1;32m     11\u001b[0m formal_sentences \u001b[38;5;241m=\u001b[39m [preprocess(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m formal_sentences]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Create dataframes from the sentences lists\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 10\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m     formal_sentences \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Preprocess the data \u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m informal_sentences \u001b[38;5;241m=\u001b[39m [\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m informal_sentences]\n\u001b[1;32m     11\u001b[0m formal_sentences \u001b[38;5;241m=\u001b[39m [preprocess(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m formal_sentences]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Create dataframes from the sentences lists\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m, in \u001b[0;36mpreprocess\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      3\u001b[0m text \u001b[38;5;241m=\u001b[39m expand_contractions(text)\n\u001b[1;32m      4\u001b[0m text \u001b[38;5;241m=\u001b[39m remove_repeated_punctuations(text)\n\u001b[0;32m----> 5\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[43mcapitalize_first_and_proper_nouns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m text \u001b[38;5;241m=\u001b[39m fix_general_spacing(text)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m, in \u001b[0;36mcapitalize_first_and_proper_nouns\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcapitalize_first_and_proper_nouns\u001b[39m(text):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Process the text using spaCy to create a Doc object\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     result \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Iterate over the sentences in the Doc\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/spacy/language.py:1049\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     error_handler \u001b[38;5;241m=\u001b[39m proc\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1049\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcomponent_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1051\u001b[0m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/spacy/pipeline/attributeruler.py:142\u001b[0m, in \u001b[0;36mAttributeRuler.__call__\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    140\u001b[0m error_handler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 142\u001b[0m     matches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_annotations(doc, matches)\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/spacy/pipeline/attributeruler.py:149\u001b[0m, in \u001b[0;36mAttributeRuler.match\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmatch\u001b[39m(\u001b[38;5;28mself\u001b[39m, doc: Doc):\n\u001b[0;32m--> 149\u001b[0m     matches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_spans\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;66;03m# Sort by the attribute ID, so that later rules have precedence\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     matches \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    152\u001b[0m         (\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab\u001b[38;5;241m.\u001b[39mstrings[m_id]), m_id, s, e) \u001b[38;5;28;01mfor\u001b[39;00m m_id, s, e \u001b[38;5;129;01min\u001b[39;00m matches  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    153\u001b[0m     ]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# path to train data \n",
    "train_file_EM_informal = \"./GYAFC_Corpus/Entertainment_Music/train/informal\"\n",
    "train_file_EM_formal = \"./GYAFC_Corpus/Entertainment_Music/train/formal\"\n",
    "train_file_FR_informal = \"./GYAFC_Corpus/Family_Relationships/train/informal\"\n",
    "train_file_FR_formal = \"./GYAFC_Corpus/Family_Relationships/train/formal\"\n",
    "\n",
    "# Get preprocessed dataframes\n",
    "train_df_EM_paired = read_and_pair_data(train_file_EM_informal, train_file_EM_formal)\n",
    "train_df_FR_paired = read_and_pair_data(train_file_FR_informal, train_file_FR_formal)\n",
    "\n",
    "# Tokenize both informal and formal sentences from Entertainment Music\n",
    "train_df_EM_paired['informal_tokenized'] = tokenize_sentences(train_df_EM_paired['informal'])\n",
    "train_df_EM_paired['formal_tokenized'] = tokenize_sentences(train_df_EM_paired['formal'])\n",
    "\n",
    "# Tokenize both informal and formal sentences from Family Relationships\n",
    "train_df_FR_paired['informal_tokenized'] = tokenize_sentences(train_df_FR_paired['informal'])\n",
    "train_df_FR_paired['formal_tokenized'] = tokenize_sentences(train_df_FR_paired['formal'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            informal  \\\n",
      "0  The movie The In - Laws not exactly a holiday ...   \n",
      "1        That page did not give me viroses(i think )   \n",
      "2  Of corse i be wachin It Evry day, my fav chara...   \n",
      "3  Runescape.com ( my kids love it ) & funbrain.c...   \n",
      "4  Is he Gay?he was on Late Night with Conan O'br...   \n",
      "\n",
      "                                              formal  \\\n",
      "0  The In - Laws movie is not a holiday movie, bu...   \n",
      "1          I do not think that page gave me viruses.   \n",
      "2  I watch it everyday, my favorite charachter is...   \n",
      "3  Funbrain.com and runescape.com are great for f...   \n",
      "4  He was on the Late Night show with Conan O'bri...   \n",
      "\n",
      "                                  informal_tokenized  \\\n",
      "0  [The, movie, The, In, -, Laws, not, exactly, a...   \n",
      "1  [That, page, did, not, give, me, viroses, (, i...   \n",
      "2  [Of, corse, i, be, wachin, It, Evry, day, ,, m...   \n",
      "3  [Runescape.com, (, my, kids, love, it, ), &, f...   \n",
      "4  [Is, he, Gay, ?, he, was, on, Late, Night, wit...   \n",
      "\n",
      "                                    formal_tokenized  \n",
      "0  [The, In, -, Laws, movie, is, not, a, holiday,...  \n",
      "1  [I, do, not, think, that, page, gave, me, viru...  \n",
      "2  [I, watch, it, everyday, ,, my, favorite, char...  \n",
      "3  [Funbrain.com, and, runescape.com, are, great,...  \n",
      "4  [He, was, on, the, Late, Night, show, with, Co...  \n"
     ]
    }
   ],
   "source": [
    "print(train_df_EM_paired.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            informal  \\\n",
      "0  Sure, it is ok, but I always have let the guy ...   \n",
      "1  Hmmm, I am a guy suffering from verbal abuse f...   \n",
      "2        You will have more friends that you want.;)   \n",
      "3  It is nice, you get to see pictures of who you...   \n",
      "4                           I NEED TO KNOW WHAT 2 DO   \n",
      "\n",
      "                                              formal  \\\n",
      "0                    I prefer to let the guy ask me.   \n",
      "1        I suffer through verbal abuse from my wife.   \n",
      "2          You will have more friends than you want.   \n",
      "3  It is nice that you get to see pictures of who...   \n",
      "4                         I need to know what to do.   \n",
      "\n",
      "                                  informal_tokenized  \\\n",
      "0  [Sure, ,, it, is, ok, ,, but, I, always, have,...   \n",
      "1  [Hmmm, ,, I, am, a, guy, suffering, from, verb...   \n",
      "2  [You, will, have, more, friends, that, you, wa...   \n",
      "3  [It, is, nice, ,, you, get, to, see, pictures,...   \n",
      "4                   [I, NEED, TO, KNOW, WHAT, 2, DO]   \n",
      "\n",
      "                                    formal_tokenized  \n",
      "0         [I, prefer, to, let, the, guy, ask, me, .]  \n",
      "1  [I, suffer, through, verbal, abuse, from, my, ...  \n",
      "2  [You, will, have, more, friends, than, you, wa...  \n",
      "3  [It, is, nice, that, you, get, to, see, pictur...  \n",
      "4               [I, need, to, know, what, to, do, .]  \n"
     ]
    }
   ],
   "source": [
    "print(train_df_FR_paired.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'In', '-', 'Laws', 'movie', 'is', 'not', 'a', 'holiday', 'movie', ',', 'but', 'it', 'is', 'okay', '.']\n"
     ]
    }
   ],
   "source": [
    "print(train_df_EM_paired['formal_tokenized'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating JSON file for the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "train_ds = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line1:\n\u001b[1;32m     10\u001b[0m   \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m rule_based_preprocessed1 \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m train_ds\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     15\u001b[0m     {\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;28mid\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     }\n\u001b[1;32m     26\u001b[0m )  \u001b[38;5;66;03m# adding a row\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m, in \u001b[0;36mpreprocess\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      3\u001b[0m text \u001b[38;5;241m=\u001b[39m expand_contractions(text)\n\u001b[1;32m      4\u001b[0m text \u001b[38;5;241m=\u001b[39m remove_repeated_punctuations(text)\n\u001b[0;32m----> 5\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[43mcapitalize_first_and_proper_nouns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m text \u001b[38;5;241m=\u001b[39m fix_general_spacing(text)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m, in \u001b[0;36mcapitalize_first_and_proper_nouns\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcapitalize_first_and_proper_nouns\u001b[39m(text):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Process the text using spaCy to create a Doc object\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     result \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Iterate over the sentences in the Doc\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/spacy/language.py:1049\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     error_handler \u001b[38;5;241m=\u001b[39m proc\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1049\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcomponent_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1051\u001b[0m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/spacy/pipeline/trainable_pipe.pyx:52\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/spacy/pipeline/transition_parser.pyx:264\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser.predict\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/spacy/pipeline/transition_parser.pyx:285\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser.greedy_parse\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/thinc/model.py:334\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OutT:\n\u001b[1;32m    331\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;124;03m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/spacy/ml/tb_framework.py:34\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(model, X, is_train):\n\u001b[0;32m---> 34\u001b[0m     step_model \u001b[38;5;241m=\u001b[39m \u001b[43mParserStepModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43munseen_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munseen_classes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_upper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhas_upper\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m step_model, step_model\u001b[38;5;241m.\u001b[39mfinish_steps\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/spacy/ml/parser_model.pyx:257\u001b[0m, in \u001b[0;36mspacy.ml.parser_model.ParserStepModel.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/spacy/ml/parser_model.pyx:398\u001b[0m, in \u001b[0;36mspacy.ml.parser_model.precompute_hiddens.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/spacy/ml/_precomputable_affine.py:27\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Preallocate array for layer output, including padding.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m Yf \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39malloc2f(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, nF \u001b[38;5;241m*\u001b[39m nO \u001b[38;5;241m*\u001b[39m nP, zeros\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 27\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgemm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnF\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnO\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnI\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrans2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mYf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m Yf \u001b[38;5;241m=\u001b[39m Yf\u001b[38;5;241m.\u001b[39mreshape((Yf\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], nF, nO, nP))\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Set padding. Padding has shape (1, nF, nO, nP). Unfortunately, we cannot\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# change its shape to (nF, nO, nP) without breaking existing models. So\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# we'll squeeze the first dimension here.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "f1 = open(\"./GYAFC_Corpus/Family_Relationships/train/informal\", 'r')\n",
    "f2 = open(\"./GYAFC_Corpus/Family_Relationships/train/formal\", 'r')\n",
    "\n",
    "id = 0\n",
    "\n",
    "while True:\n",
    "  line1 = f1.readline().rstrip()\n",
    "  line2 = f2.readline().rstrip()\n",
    "  if not line1:\n",
    "    break\n",
    "\n",
    "  rule_based_preprocessed1 = preprocess(line1)\n",
    "\n",
    "  train_ds.append(\n",
    "      {\n",
    "          'id':id,\n",
    "          'topic':'Family_Relationships',\n",
    "          'transformation':{\n",
    "              'informal':rule_based_preprocessed1,\n",
    "              'formal.ref0':line2,\n",
    "              'formal.ref1':\"\",\n",
    "              'formal.ref2':\"\",\n",
    "              'formal.ref3':\"\",\n",
    "          }\n",
    "      }\n",
    "  )  # adding a row\n",
    "\n",
    "  id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = open(\"./GYAFC_Corpus/Entertainment_Music/train/informal\", 'r')\n",
    "f2 = open(\"./GYAFC_Corpus/Entertainment_Music/train/formal\", 'r')\n",
    "\n",
    "while True:\n",
    "  line1 = f1.readline().rstrip()\n",
    "  line2 = f2.readline().rstrip()\n",
    "  if not line1:\n",
    "    break\n",
    "\n",
    "  rule_based_preprocessed1 = preprocess(line1)\n",
    "\n",
    "  train_ds.append(\n",
    "      {\n",
    "          'id':id,\n",
    "          'topic':'Entertainment_Music',\n",
    "          'transformation':{\n",
    "              'informal':rule_based_preprocessed1,\n",
    "              'formal.ref0':line2,\n",
    "              'formal.ref1':\"\",\n",
    "              'formal.ref2':\"\",\n",
    "              'formal.ref3':\"\",\n",
    "          }\n",
    "      }\n",
    "  )\n",
    "\n",
    "  id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = open(\"./GYAFC_Corpus/Family_Relationships/tune/informal\", 'r')\n",
    "f2 = open(\"./GYAFC_Corpus/Family_Relationships/tune/formal.ref0\", 'r')\n",
    "f3 = open(\"./GYAFC_Corpus/Family_Relationships/tune/formal.ref1\", 'r')\n",
    "f4 = open(\"./GYAFC_Corpus/Family_Relationships/tune/formal.ref2\", 'r')\n",
    "f5 = open(\"./GYAFC_Corpus/Family_Relationships/tune/formal.ref3\", 'r')\n",
    "\n",
    "id = 0\n",
    "\n",
    "while True:\n",
    "  line1 = f1.readline().rstrip()\n",
    "  line2 = f2.readline().rstrip()\n",
    "  line3 = f3.readline().rstrip()\n",
    "  line4 = f4.readline().rstrip()\n",
    "  line5 = f5.readline().rstrip()\n",
    "  if not line1:\n",
    "    break\n",
    "  \n",
    "  rule_based_preprocessed_val = preprocess(line1)\n",
    "\n",
    "  val_ds.append(\n",
    "      {\n",
    "          'id':id,\n",
    "          'topic':'Family_Relationships',\n",
    "          'transformation':{\n",
    "              'informal':rule_based_preprocessed_val,\n",
    "              'formal.ref0':line2,\n",
    "              'formal.ref1':line3,\n",
    "              'formal.ref2':line4,\n",
    "              'formal.ref3':line5,\n",
    "          }\n",
    "      })  # adding a row\n",
    "  id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = open(\"./GYAFC_Corpus/Entertainment_Music/tune/informal\", 'r')\n",
    "f2 = open(\"./GYAFC_Corpus/Entertainment_Music/tune/formal.ref0\", 'r')\n",
    "f3 = open(\"./GYAFC_Corpus/Entertainment_Music/tune/formal.ref1\", 'r')\n",
    "f4 = open(\"./GYAFC_Corpus/Entertainment_Music/tune/formal.ref2\", 'r')\n",
    "f5 = open(\"./GYAFC_Corpus/Entertainment_Music/tune/formal.ref3\", 'r')\n",
    "\n",
    "while True:\n",
    "  line1 = f1.readline().rstrip()\n",
    "  line2 = f2.readline().rstrip()\n",
    "  line3 = f3.readline().rstrip()\n",
    "  line4 = f4.readline().rstrip()\n",
    "  line5 = f5.readline().rstrip()\n",
    "  if not line1:\n",
    "    break\n",
    "  \n",
    "  rule_based_preprocessed_val = preprocess(line1)\n",
    "\n",
    "  val_ds.append(\n",
    "      {\n",
    "          'id':id,\n",
    "          'topic':'Entertainment_Music',\n",
    "          'transformation':{\n",
    "              'informal':rule_based_preprocessed_val,\n",
    "              'formal.ref0':line2,\n",
    "              'formal.ref1':line3,\n",
    "              'formal.ref2':line4,\n",
    "              'formal.ref3':line5,\n",
    "          }\n",
    "      }) # adding a row\n",
    "  id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data_val_preprocessed.json\", 'w') as f:\n",
    "  json.dump(val_ds, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = open(\"./GYAFC_Corpus/Family_Relationships/test/informal\", 'r')\n",
    "f2 = open(\"./GYAFC_Corpus/Family_Relationships/test/formal.ref0\", 'r')\n",
    "f3 = open(\"./GYAFC_Corpus/Family_Relationships/test/formal.ref1\", 'r')\n",
    "f4 = open(\"./GYAFC_Corpus/Family_Relationships/test/formal.ref2\", 'r')\n",
    "f5 = open(\"./GYAFC_Corpus/Family_Relationships/test/formal.ref3\", 'r')\n",
    "\n",
    "id = 0\n",
    "\n",
    "while True:\n",
    "  line1 = f1.readline().rstrip()\n",
    "  line2 = f2.readline().rstrip()\n",
    "  line3 = f3.readline().rstrip()\n",
    "  line4 = f4.readline().rstrip()\n",
    "  line5 = f5.readline().rstrip()\n",
    "  if not line1:\n",
    "    break\n",
    "  rule_based_preprocessed_test = preprocess(line1)\n",
    "  \n",
    "  test_ds.append(\n",
    "      {\n",
    "          'id':id,\n",
    "          'topic':'Family_Relationships',\n",
    "          'transformation':{\n",
    "              'informal':rule_based_preprocessed_test,\n",
    "              'formal.ref0':line2,\n",
    "              'formal.ref1':line3,\n",
    "              'formal.ref2':line4,\n",
    "              'formal.ref3':line5,\n",
    "          }\n",
    "      })  # adding a row\n",
    "  id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = open(\"./GYAFC_Corpus/Entertainment_Music/test/informal\", 'r')\n",
    "f2 = open(\"./GYAFC_Corpus/Entertainment_Music/test/formal.ref0\", 'r')\n",
    "f3 = open(\"./GYAFC_Corpus/Entertainment_Music/test/formal.ref1\", 'r')\n",
    "f4 = open(\"./GYAFC_Corpus/Entertainment_Music/test/formal.ref2\", 'r')\n",
    "f5 = open(\"./GYAFC_Corpus/Entertainment_Music/test/formal.ref3\", 'r')\n",
    "\n",
    "while True:\n",
    "  line1 = f1.readline().rstrip()\n",
    "  line2 = f2.readline().rstrip()\n",
    "  line3 = f3.readline().rstrip()\n",
    "  line4 = f4.readline().rstrip()\n",
    "  line5 = f5.readline().rstrip()\n",
    "  if not line1:\n",
    "    break\n",
    "  \n",
    "  rule_based_preprocessed_test = preprocess(line1)\n",
    "\n",
    "  test_ds.append(\n",
    "      {\n",
    "          'id':id,\n",
    "          'topic':'Entertainment_Music',\n",
    "          'transformation':{\n",
    "              'informal':line1,\n",
    "              'formal.ref0':line2,\n",
    "              'formal.ref1':line3,\n",
    "              'formal.ref2':line4,\n",
    "              'formal.ref3':line5,\n",
    "          }\n",
    "      })  # adding a row\n",
    "  id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data_test_preprocessed.json\", 'w') as f:\n",
    "  json.dump(test_ds, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
